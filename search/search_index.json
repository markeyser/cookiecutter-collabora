{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Fast ML/AI Project Scaffolding with Built-In Collaboration and Reproducibility <p>       Kickstart your AI projects in minutes with our Cookiecutter template. As an AI expert, you can dive straight into coding without the hassle of setting up scaffolding or configuring tools.       Everything has been pre-configured to follow best practices and standards in AI software development, with tools included to ensure adherence, guaranteeing high-quality, collaborative, and reproducible development from the start.     </p> Get started Learn more Everything you would expect Standard Scaffolding for ML/AI Projects <p>We provide a standardized scaffolding customized for all types of ML projects, from traditional ML to Generative AI and even ML Python packages.</p> Comprehensive Tools Included <p>Includes built-in VS Code settings, GitHub workflows, formatters, linters, Docker configuration, etc., to ensure seamless collaboration, reproducibility, and efficiency from the start.</p> Best Practices Guidance <p>We also provide extensive explanations of best practices for using these tools and configurations, enabling collaborative and reproducible workflows.</p> Cross-Platform and Cloud Compatibility <p>Our template works seamlessly on all major operating systems, including Mac, Windows, and Linux, and is compatible with all major cloud providers, including Google Cloud, Azure, and AWS, ensuring flexibility and convenience for all users.</p>"},{"location":"api-reference/toc-api-reference/","title":"API Reference","text":"<p>About This Section</p> <p>The API Reference section provides detailed information and documentation for various components and functionalities of the Cookiecutter Collabora project. This reference is a valuable resource for developers and contributors who want in-depth insights into the project's APIs and modules.</p>"},{"location":"api-reference/toc-api-reference/#purpose","title":"Purpose","text":"<ul> <li> Comprehensive Documentation: The API Reference offers extensive documentation for the project's APIs, modules, and components.</li> <li> Developer-Friendly: It is designed to be developer-friendly, with clear explanations and examples.</li> <li> Reference for Contributions: Developers and contributors can use this section as a reference while working on the project.</li> </ul>"},{"location":"api-reference/toc-api-reference/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Module A</li> <li>Module B</li> <li>Module C</li> <li>Module D</li> <li>Module E</li> <li>Module F</li> <li>Module G</li> <li>Module H</li> <li>Module I</li> <li>Module J</li> <li>Module K</li> <li>Module L</li> <li>Module M</li> <li>Module N</li> <li>Module O</li> <li>Module P</li> <li>Module Q</li> <li>Module R</li> <li>Module S</li> <li>Module T</li> <li>Module U</li> <li>Module V</li> <li>Module W</li> <li>Module X</li> <li>Module Y</li> <li>Module Z</li> <li>[More API documentation coming soon...]</li> </ul> <p>Navigating the API Reference</p> <p>The API Reference is organized alphabetically by modules and components, making it easy to find the information you need. Each module provides detailed explanations, usage examples, and reference material for the associated component.</p>"},{"location":"explanation/dvc-git-branches/","title":"Explanation: Managing Data Versions with Git Branches and Integrating DVC with Collaborative Tools","text":""},{"location":"explanation/dvc-git-branches/#introduction","title":"Introduction","text":"<p>Overview</p> <p>This guide focuses on understanding the use of Git branches for data versioning and the integration of DVC with collaborative tools like Slack or GitHub. It's crucial for enhancing team collaboration and efficiency in data-centric projects.</p>"},{"location":"explanation/dvc-git-branches/#using-git-branches-to-manage-data-versions","title":"Using Git Branches to Manage Data Versions","text":""},{"location":"explanation/dvc-git-branches/#the-concept-of-branching-in-git","title":"The Concept of Branching in Git","text":"<ul> <li> Branching in Git is about creating separate lines of development. Each branch acts as an isolated environment for specific features or versions.</li> <li> Isolation provided by branches allows for simultaneous and independent development streams.</li> </ul>"},{"location":"explanation/dvc-git-branches/#managing-data-versions-with-branches","title":"Managing Data Versions with Branches","text":"<p>Branch-Specific Workflows</p> <ul> <li>Version Control: Using branches for different data versions facilitates controlled experimentation and changes.</li> <li>Naming Conventions: Descriptive branch names like <code>feature/new-data-set-2.1</code> aid in identifying and managing various data versions.</li> </ul>"},{"location":"explanation/dvc-git-branches/#integrating-dvc-with-collaborative-tools","title":"Integrating DVC with Collaborative Tools","text":""},{"location":"explanation/dvc-git-branches/#enhancing-team-collaboration","title":"Enhancing Team Collaboration","text":"<ul> <li> DVC's Role: DVC extends Git's capabilities for handling large datasets, crucial in data-heavy projects.</li> <li> Collaborative Tools: Integrations with tools like Slack or GitHub provide automated updates and notifications, increasing transparency and team communication.</li> </ul>"},{"location":"explanation/dvc-git-branches/#communication-and-transparency","title":"Communication and Transparency","text":"<p>Effective Communication</p> <ul> <li>Automated Notifications: Receive alerts about data updates through collaborative tool integrations.</li> <li>Code Reviews and Pull Requests: These should include scrutiny of <code>.dvc</code> files, ensuring data changes are reviewed along with code.</li> </ul>"},{"location":"explanation/dvc-git-branches/#the-importance-of-this-integration","title":"The Importance of this Integration","text":"<ul> <li> Efficient Workflows: Streamlined and transparent workflows are achieved through integration, improving project efficiency.</li> <li> Reproducibility and Consistency: Ensures all team members are aligned with the same data version, crucial for consistent results in data projects.</li> </ul>"},{"location":"explanation/dvc-git-branches/#conclusion","title":"Conclusion","text":"<p>Key Takeaways</p> <p>Understanding the management of data versions with Git branches and the integration of DVC with collaborative tools is essential in modern data management. These practices bolster teamwork, enhance clarity, and are indispensable in data science and machine learning environments.</p>"},{"location":"explanation/dvc-understanding-dvs/","title":"Understanding Data Version Control with DVC","text":""},{"location":"explanation/dvc-understanding-dvs/#introduction-to-data-version-control-dvc","title":"Introduction to Data Version Control (DVC)","text":"<p>What is DVC?</p> <p>Data Version Control, or DVC, is a tool designed to provide robust mechanisms for the versioning of large datasets. It is specifically tailored for the needs of data scientists and machine learning engineers, offering functionalities that go beyond traditional code version control systems.</p>"},{"location":"explanation/dvc-understanding-dvs/#the-importance-of-data-version-control-in-machine-learning","title":"The Importance of Data Version Control in Machine Learning","text":"<p>Machine learning projects often involve working with large, complex datasets. Managing these datasets effectively is vital for several reasons:</p> Key Reasons for Data Version Control <ol> <li>Reproducibility: Ensuring that experiments can be replicated using the exact data versions used initially.</li> <li>Collaboration: Facilitating synchronized data usage across teams, crucial for consistency in results and methodologies.</li> <li>Efficiency: Efficiently handling large data files, making the process of tracking changes and managing different dataset versions more manageable.</li> </ol>"},{"location":"explanation/dvc-understanding-dvs/#how-dvc-complements-code-version-control","title":"How DVC Complements Code Version Control","text":"<p>While traditional code version control systems are excellent at handling code, they are not optimized for large data files. DVC fills this gap by:</p> <ul> <li>Providing a Git-like interface specifically for data files.</li> <li>Allowing teams to track changes in data, similar to how code changes are tracked.</li> <li>Linking data versions to specific stages or experiments in a project.</li> </ul>"},{"location":"explanation/dvc-understanding-dvs/#key-features-and-benefits-of-dvc","title":"Key Features and Benefits of DVC","text":"<p>DVC stands out due to several key features that make it particularly suitable for data versioning in machine learning:</p> DVC's Features <ul> <li>Versioning Large Datasets: DVC can handle and version large data files effectively, offering a history of modifications just like Git does for source code.</li> <li>Enhanced Collaboration: It simplifies sharing and synchronizing data changes within a team, ensuring everyone works with the same data version.</li> <li>Reproducibility and Traceability: By binding specific data versions to project stages, DVC enhances the reproducibility of results and provides traceability of data changes.</li> </ul>"},{"location":"explanation/dvc-understanding-dvs/#conclusion","title":"Conclusion","text":"<p>In summary, DVC is an essential tool in the realm of data science and machine learning. By integrating it into workflows, teams gain significant control over their data assets, ensuring that datasets are managed consistently, efficiently, and transparently.</p> Understanding the Role of DVC <p>Understanding the role and functionality of DVC is the first step towards harnessing its full potential in managing complex data-driven projects.</p>"},{"location":"explanation/effective-data-documentation/","title":"Metadata and Documentation in Data Science Projects","text":""},{"location":"explanation/effective-data-documentation/#importance-of-metadata-and-documentation","title":"Importance of Metadata and Documentation","text":"<p>Contextual Understanding</p> <p>Metadata provides essential context, detailing data source, collection methods, format, and any alterations.</p> <p>Reproducibility and Traceability</p> <p>Ensures data processing is replicable and understandable, crucial for credibility in data science.</p> <p>Data Quality Insights</p> <p>Offers insights into data quality (accuracy, completeness, consistency), aiding in assessing reliability and understanding limitations.</p> <p>Compliance and Auditing</p> <p>Facilitates adherence to regulatory requirements in data management, simplifying audits.</p>"},{"location":"explanation/effective-data-documentation/#recommendations-for-effective-metadata-and-documentation","title":"Recommendations for Effective Metadata and Documentation","text":""},{"location":"explanation/effective-data-documentation/#standardized-format","title":"Standardized Format","text":"<p>Adopt consistent formats like JSON/XML for metadata, and markdown/structured text for documentation.</p>"},{"location":"explanation/effective-data-documentation/#automated-generation","title":"Automated Generation","text":"<p>Automate metadata creation during data import or processing.</p>"},{"location":"explanation/effective-data-documentation/#version-control","title":"Version Control","text":"<p>Implement version control for metadata, reflecting data evolution.</p>"},{"location":"explanation/effective-data-documentation/#key-elements-inclusion","title":"Key Elements Inclusion","text":"<p>Ensure metadata contains vital details like data source, acquisition date, format, preprocessing steps, data schema, and confidentiality information.</p>"},{"location":"explanation/effective-data-documentation/#accessibility","title":"Accessibility","text":"<p>Store metadata and documentation in accessible locations, linked to corresponding data.</p>"},{"location":"explanation/effective-data-documentation/#training-and-guidelines","title":"Training and Guidelines","text":"<p>Offer team training on creating and maintaining proper documentation and metadata.</p>"},{"location":"explanation/effective-data-documentation/#regular-updates","title":"Regular Updates","text":"<p>Update documentation to mirror data or procedure changes.</p>"},{"location":"explanation/effective-data-documentation/#tools-utilization","title":"Tools Utilization","text":"<p>Leverage metadata management and documentation tools, especially in larger organizations.</p>"},{"location":"explanation/effective-data-documentation/#collaboration-and-reviews","title":"Collaboration and Reviews","text":"<p>Encourage team reviews for improved quality and comprehension.</p>"},{"location":"explanation/effective-data-documentation/#data-pipeline-integration","title":"Data Pipeline Integration","text":"<p>Integrate metadata and documentation updates into data processing pipelines.</p> <p>Summary</p> <p>Comprehensive, current metadata and documentation are vital for effective data management and understanding in data science projects, enhancing collaboration, compliance, and data integrity.</p>"},{"location":"explanation/effective-data-documentation/#example-of-metadata-creation","title":"Example of Metadata Creation","text":"<p>Suppose you have a CSV file named <code>sales_data.csv</code>. The metadata for this file could include information such as the source of the data, the date of creation, the number of rows and columns, column names, and any preprocessing steps applied.</p> <p>Here's an example of what the JSON metadata might look like:</p> JSON Metadata for a CSV File<pre><code>{\n  \"file_name\": \"sales_data.csv\",\n  \"creation_date\": \"2024-01-22\",\n  \"source\": \"Internal Sales System\",\n  \"number_of_rows\": 1200,\n  \"number_of_columns\": 5,\n  \"columns\": [\n    {\"name\": \"Date\", \"type\": \"Date\", \"description\": \"Date of sale\"},\n    {\"name\": \"Product_ID\", \"type\": \"String\", \"description\": \"Unique identifier for the product\"},\n    {\"name\": \"Quantity\", \"type\": \"Integer\", \"description\": \"Number of products sold\"},\n    {\"name\": \"Price\", \"type\": \"Float\", \"description\": \"Sale price per unit\"},\n    {\"name\": \"Total_Sales\", \"type\": \"Float\", \"description\": \"Total sales amount\"}\n  ],\n  \"preprocessing\": [\n    {\"step\": \"Data Cleaning\", \"description\": \"Removed null values and corrected data formats\"},\n    {\"step\": \"Normalization\", \"description\": \"Normalized the Price column using min-max scaling\"}\n  ],\n  \"notes\": \"Data updated monthly. Last update included Q4 2023 sales data.\"\n}\n</code></pre>"},{"location":"explanation/effective-data-documentation/#automating-the-metadata-generation","title":"Automating the Metadata Generation","text":"<p>To automate the process of generating this metadata, you can use a script in Python. This script will:</p> <ol> <li>Read the CSV file.</li> <li>Extract relevant information such as the number of rows and columns,    column names, etc.</li> <li>Generate and save the metadata in a JSON file.</li> </ol> <p>Here's a simple Python script to achieve this:</p> Automating the Metadata Generation<pre><code>import json\nimport pandas as pd\nfrom datetime import datetime\n\ndef generate_metadata(csv_file_path):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Extracting information\n    file_name = csv_file_path.split('/')[-1]\n    creation_date = datetime.now().strftime(\"%Y-%m-%d\")\n    number_of_rows = df.shape[0]\n    number_of_columns = df.shape[1]\n    columns = [{\"name\": col, \"type\": str(df[col].dtype)} for col in df.columns]\n\n    # Metadata dictionary\n    metadata = {\n        \"file_name\": file_name,\n        \"creation_date\": creation_date,\n        \"source\": \"Specify the data source\",\n        \"number_of_rows\": number_of_rows,\n        \"number_of_columns\": number_of_columns,\n        \"columns\": columns,\n        \"preprocessing\": [],  # Add any preprocessing steps manually or through code\n        \"notes\": \"Add any additional notes here\"\n    }\n\n    # Saving metadata to a JSON file\n    with open(file_name.replace('.csv', '_metadata.json'), 'w') as json_file:\n        json.dump(metadata, json_file, indent=4)\n\n# Example usage\ngenerate_metadata('path/to/your/sales_data.csv')\n</code></pre> <p>Note</p> <p>Customize the script's <code>preprocessing</code> section as needed. Adjust the script for specific data contexts. Requires the <code>pandas</code> library.</p>"},{"location":"explanation/explanation/","title":"Explanation","text":""},{"location":"explanation/explanation/#python-code-formatting-guide-for-vs-code-with-jupyter-integration","title":"Python Code Formatting Guide for VS Code with Jupyter Integration","text":"<p>Welcome to the deep dive into our Python Code Formatting strategy. This guide delves into our rationale, the considerations we made, and the broader context that prompted us to tailor a formatting environment suitable for VS Code with Jupyter notebooks.</p>"},{"location":"explanation/explanation/#why-a-dedicated-formatting-strategy-matters","title":"Why a Dedicated Formatting Strategy Matters","text":"<p>In today's fast-paced development landscape, the importance of code readability and maintainability can't be overstated. Well-structured code not only enhances the development experience but also makes collaborative efforts more straightforward. As we traverse through this guide, you'll understand our commitment to these principles and how we have materialized them through our chosen tools and practices.</p>"},{"location":"explanation/explanation/#1-foundations-of-our-code-formatting-strategy","title":"1. Foundations of Our Code Formatting Strategy","text":"<p>At the heart of our strategy lies the ambition to maintain high-quality software development practices. A significant portion of this commitment revolves around the clarity of code. Here's a deeper dive into our approach:</p>"},{"location":"explanation/explanation/#local-remote-server-code-formatting","title":"Local (Remote Server) Code Formatting:","text":"<p>Manual intervention offers precision. By giving developers the tools to manually format using Black in VS Code, especially within Jupyter notebooks, we recognize the significance of context and narrative flow. The dedicated keyboard shortcut for the \"Format Document\" command in VS Code becomes more than just a tool\u2014it embodies this philosophy of control and precision.</p>"},{"location":"explanation/explanation/#unified-code-standard-enforcement","title":"Unified Code Standard Enforcement:","text":"<p>We believe in reinforcing our standards at multiple levels. Pre-commit hooks are the first checkpoint, providing immediate feedback to developers. GitHub Actions, on the other hand, act as custodians of our repository's sanctity. By leveraging both, we not only streamline the development process but also ensure that our codebase remains pristine.</p>"},{"location":"explanation/explanation/#configuration-specifics","title":"Configuration Specifics:","text":"<p>These configurations are not mere technical details but decisions that reflect our understanding of the development environment:</p> <ul> <li> <p>Python Version Consistency: Uniformity is key. By synchronizing our Python versions, we eliminate discrepancies that might arise due to version differences.</p> </li> <li> <p>Jupyter Notebook Formatting: Jupyter notebooks are unique, and so is our approach towards them. By configuring Black in a way that respects their structure, we show our commitment to readability without compromising on consistency.</p> </li> <li> <p>Folder Specific Formatting: A clear focus on specific paths like <code>src/</code> and <code>notebooks/</code> indicates our emphasis on precision and our profound understanding of our repository's anatomy.</p> </li> </ul>"},{"location":"explanation/explanation/#scalability-and-onboarding","title":"Scalability and Onboarding:","text":"<p>A growth-oriented strategy is future-proof. By considering the influx of new developers and potential scalability challenges, our comprehensive documentation serves as a beacon, ensuring uniformity in the development experience.</p>"},{"location":"explanation/explanation/#2-a-glimpse-at-black","title":"2. A Glimpse at Black","text":"<p>The Black formatter is a testament to our belief in uniformity. By conforming to the PEP 8 style guide automatically, Black becomes an ally in our quest for code consistency, helping us keep stylistic debates at bay and maintain focus on core functionalities.</p>"},{"location":"explanation/explanation/#the-way-forward","title":"The Way Forward","text":"<p>We believe that our strategy's success lies in its adaptability and foresight. Uniform code formatting, when coupled with automated checks within VS Code, sets the stage for an intuitive development experience. The harmony achieved through consistent code styling paves the way for more focused collaborative undertakings and ensures that our code reviews remain substantive rather than stylistic.</p> <p>Embrace clarity. Prioritize consistency. And above all, cherish the joy of coding!</p>"},{"location":"explanation/github-naming-conventions/","title":"GitHub Repository Naming Conventions for Data Science Projects","text":"<p>Overview</p> <p>Choosing the right naming convention for GitHub repositories in data science projects is crucial for clarity, organization, and ease of navigation. A well-defined naming convention helps team members and stakeholders to quickly understand the scope and purpose of a repository at a glance. This section outlines the guidelines for naming GitHub repositories related to data science projects.</p> <p>Naming Convention Structure</p> <p>Repositories should be named following this format:</p> <pre><code>&lt;prefix&gt;-&lt;descriptive-name&gt;[-&lt;optional-version&gt;]\n</code></pre> Components <ul> <li>Prefix: A concise identifier related to the project's domain or main technology.</li> <li>Descriptive Name: A clear and specific description of the repository's content or purpose.</li> <li>Optional Version: A version number, if applicable, to distinguish between different iterations or stages of the project.</li> </ul> <p>Guidelines</p> <ol> <li>Choose an Appropriate Prefix</li> <li>The prefix should represent the key area or technology of the project, like <code>ml</code> for machine learning, <code>nlp</code> for natural language processing, <code>cv</code> for computer vision, etc.</li> <li> <p>This helps in categorizing and quickly identifying the project's domain.</p> </li> <li> <p>Be Clear and Specific</p> </li> <li>Use descriptive and meaningful terms that accurately reflect the primary focus or functionality of the repository.</li> <li> <p>Avoid vague or overly broad terms that do not convey the specific purpose of the repository.</p> </li> <li> <p>Include Versioning Where Necessary</p> </li> <li>For projects that have multiple versions or stages, include a version number at the end of the repository name.</li> <li> <p>This is useful for tracking development progress and differentiating between major project phases.</p> </li> <li> <p>Maintain Consistency</p> </li> <li>Keep all repository names in lowercase and use hyphens (<code>-</code>) to separate words. This enhances readability and avoids issues with URL encoding.</li> </ol> <p>Examples</p> <ul> <li><code>ml-predictive-modeling</code></li> <li><code>nlp-chatbot-interface</code></li> <li><code>cv-facial-recognition-v1</code></li> <li><code>ds-data-cleaning-tools</code></li> </ul> <p>Conclusion</p> <p>Adopting these naming conventions for GitHub repositories in data science projects promotes a structured and systematic approach to repository management. It ensures that the repository names are informative, organized, and aligned with the project's objectives and technical domain.</p>"},{"location":"explanation/ml-naming-conventions/","title":"Notebook and Script Naming Conventions in ML Projects","text":""},{"location":"explanation/ml-naming-conventions/#overview","title":"Overview","text":"<p>Properly naming Jupyter notebooks and scripts is essential for quick identification, efficient management, and collaborative ease in machine learning projects. A systematic naming convention helps in understanding the file's purpose at a glance and tracking its evolution over time.</p> <p>Importance of Naming</p> <p>A well-defined naming convention is crucial for organizing and managing files in any ML project.</p>"},{"location":"explanation/ml-naming-conventions/#naming-convention-structure","title":"Naming Convention Structure","text":"<p>Use the following format for naming notebooks and scripts:</p> <pre><code>&lt;type&gt;_&lt;topic&gt;_&lt;version&gt;_&lt;YYYYMMDD&gt;.&lt;extension&gt;\n</code></pre>"},{"location":"explanation/ml-naming-conventions/#components","title":"Components:","text":"<ul> <li>Type: A short identifier indicating the nature of the work (e.g., <code>eda</code> for exploratory data analysis, <code>preprocess</code> for data preprocessing, <code>model</code> for model training).</li> <li>Topic: A concise descriptor of the notebook's or script's main focus.</li> <li>Version: An optional version number or identifier, especially useful if the notebook or script undergoes significant iterative updates.</li> <li>Date: The creation or last modified date in <code>YYYYMMDD</code> format.</li> <li>Extension: The file extension, like <code>.ipynb</code> for Jupyter notebooks, <code>.py</code> for Python scripts.</li> </ul> <p>Components Breakdown</p> <p>Understanding each component of the naming convention helps in creating more informative and easily recognizable file names.</p>"},{"location":"explanation/ml-naming-conventions/#guidelines","title":"Guidelines:","text":"<ol> <li>Descriptive and Purposeful:</li> <li>Start with a type that categorizes the file based on its primary purpose in the ML workflow.</li> <li> <p>The topic should be sufficiently descriptive to convey the specific focus or task of the notebook/script.</p> </li> <li> <p>Versioning:</p> </li> <li> <p>Include a version number if the file is part of an iterative process, such as <code>v1</code>, <code>v2</code>, or more detailed semantic versioning like <code>1.0</code>, <code>1.1</code>.</p> </li> <li> <p>Date Stamp:</p> </li> <li> <p>Adding the date (in <code>YYYYMMDD</code> format) helps in identifying the most recent version or understanding the timeline of development.</p> </li> <li> <p>Consistency:</p> </li> <li> <p>Maintain a consistent naming convention across all notebooks and scripts for ease of organization and retrieval.</p> </li> <li> <p>Clarity and Brevity:</p> </li> <li>Ensure the name is clear yet concise. Avoid overly long names but provide enough information to understand the file's content and purpose.</li> </ol>"},{"location":"explanation/ml-naming-conventions/#examples","title":"Examples:","text":"<ul> <li><code>eda_customer_segmentation_v1_20240101.ipynb</code></li> <li><code>preprocess_data_cleaning_v2_20240215.py</code></li> <li><code>model_train_regression_20240310.ipynb</code></li> </ul> <p>Naming Examples</p> <p>These examples illustrate how the naming convention is applied in practice.</p>"},{"location":"explanation/ml-naming-conventions/#conclusion","title":"Conclusion","text":"<p>This naming convention for Jupyter notebooks and scripts will foster a more organized and manageable ML project environment. It aids in quickly locating specific files, understanding their purpose, and tracking their evolution over time.</p>"},{"location":"explanation/model-persistence-naming-conventions/","title":"Model Persistence Naming Convention","text":"<p>Model Persistence File Naming Conventions</p> <p>Adopting a consistent naming convention for persisting machine learning models in binary format is essential for easy identification and management across various projects. This guide outlines the recommended format for naming these files.</p>"},{"location":"explanation/model-persistence-naming-conventions/#recommended-format","title":"Recommended Format","text":"<ul> <li>Format: <code>&lt;project_name&gt;_&lt;model_version&gt;_&lt;model_type&gt;_&lt;timestamp&gt;.pkl</code></li> <li>The format includes the project name, model version, model type, and a timestamp.</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#field-definitions","title":"Field Definitions","text":"Field Definition <code>project_name</code> The name of the project the model is associated with <code>model_version</code> The version of the model, following semantic versioning (MAJOR.MINOR.PATCH) <code>model_type</code> Type or name of the model (e.g., linearReg, neuralNet) <code>timestamp</code> Date when the model was persisted (YYYYMMDD format)"},{"location":"explanation/model-persistence-naming-conventions/#naming-examples","title":"Naming Examples","text":"<ul> <li><code>service_sage_v1.2.0_linearReg_20240123.pkl</code></li> <li>Linear regression model from the Service Sage project, version 1.2.0, updated on January 23, 2024.</li> <li><code>one_assist_v3.0.1_neuralNet_20240215.pkl</code></li> <li>Neural network model for One Assist, version 3.0.1, updated on February 15, 2024.</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#versioning-scheme","title":"Versioning Scheme","text":"<ul> <li>MAJOR: Incremented for incompatible API changes.</li> <li>MINOR: Incremented for adding functionality in a backward-compatible manner.</li> <li>PATCH: Incremented for backward-compatible bug fixes.</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#metadata-storage","title":"Metadata Storage","text":"<ul> <li>Store a JSON file containing model metadata alongside each model file (e.g., <code>service_sage_v1.2.0_linearReg_20240123_metadata.json</code>).</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#documentation-and-registry","title":"Documentation and Registry","text":"<ul> <li>Maintain a Makefile to automate the documentation or registry generation process.</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#automating-metadata-creation","title":"Automating Metadata Creation","text":"<ul> <li>Example Python script provided for creating a linear regression model with metadata using Scikit-Learn and saving it as a <code>.pkl</code> file and corresponding metadata as a <code>.json</code> file.</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#markdown-documentation-for-models","title":"Markdown Documentation for Models","text":"<ul> <li>Example Markdown documentation (<code>service_sage_v1.2.0_linearReg_20240123_documentation.md</code>) provided, explaining the training process, model parameters, performance metrics, and metadata structure.</li> </ul>"},{"location":"explanation/model-persistence-naming-conventions/#directory-structure","title":"Directory Structure","text":"<ul> <li>Store related files in the same directory, using subdirectories for better organization in projects with multiple models. ```</li> </ul>"},{"location":"explanation/mypy-configuration-guide/","title":"Mypy Configuration Guide","text":""},{"location":"explanation/mypy-configuration-guide/#introduction","title":"Introduction","text":"<p>Mypy is a static type checker for Python. This guide explains how to configure Mypy in your project using a <code>pyproject.toml</code> file and Visual Studio Code settings.</p> <p>Default Mypy Configuration</p> <p>The Mypy configuration in this guide is the default setup included in the \u201cCookiecutter Collabora\u201d project. This configuration aims to provide a solid foundation for static type checking in Python, enhancing code quality and robustness.</p> <p>Feel free to tailor and customize this configuration to fit your  specific project requirements. Adjust the settings as needed to  improve your type-checking process and achieve the best results for your development workflow.</p>"},{"location":"explanation/mypy-configuration-guide/#configuration","title":"Configuration","text":""},{"location":"explanation/mypy-configuration-guide/#visual-studio-code-settings","title":"Visual Studio Code Settings","text":"<p>Add the following configuration to your <code>{{cookiecutter.project_slug}}/.vscode/settings.json</code> file:</p> <pre><code>{\n    \"mypy-type-checker.args\": [\n        \"--config-file=pyproject.toml\"\n    ],\n    \"mypy-type-checker.cwd\": \"${workspaceFolder}\",\n    \"mypy-type-checker.importStrategy\": \"fromEnvironment\",\n    \"mypy-type-checker.preferDaemon\": true,\n    \"mypy-type-checker.reportingScope\": \"workspace\"\n}\n</code></pre>"},{"location":"explanation/mypy-configuration-guide/#pyproject-configuration","title":"PyProject Configuration","text":"<p>Add the following Mypy configuration to your <code>{{cookiecutter.project_slug}}/pyproject.toml</code> file:</p> <pre><code>[tool.mypy]\npython_version = \"3.10\"\ndisallow_untyped_defs = true\ndisallow_untyped_calls = true\nignore_missing_imports = true\n</code></pre>"},{"location":"explanation/mypy-configuration-guide/#explanation","title":"Explanation","text":"<ul> <li><code>python_version</code>: Specifies the Python version.</li> <li><code>disallow_untyped_defs</code>: Disallows functions without type annotations.</li> <li><code>disallow_untyped_calls</code>: Disallows calling functions without type annotations.</li> <li><code>ignore_missing_imports</code>: Ignores imports that cannot be resolved.</li> </ul>"},{"location":"explanation/mypy-configuration-guide/#running-mypy","title":"Running Mypy","text":"<p>To run Mypy, execute the following command in your terminal:</p> <pre><code>mypy .\n</code></pre> <p>This will check your code for type errors based on the configuration provided.</p> Why are Mypy extensions under [tool.poetry.dev-dependencies]? <p>Mypy is primarily used during the development phase to ensure type checking and code quality. Including Mypy and its extensions under <code>[tool.poetry.dev-dependencies]</code> ensures they are available during development but not included in the production environment. This helps maintain performance and security in production while allowing thorough type checking during development.</p> <p>Running Mypy in production is unnecessary and can introduce performance overhead. Instead, type checks are typically executed in CI/CD pipelines to verify code quality before deployment.</p> Why Configure Both .vscode/settings.json and pyproject.toml? <p>Configuring both <code>.vscode/settings.json</code> and <code>pyproject.toml</code> ensures consistent and integrated type checking in your development environment.</p> <ul> <li>.vscode/settings.json: Customizes Mypy behavior in Visual Studio Code. This allows for immediate feedback and integration with the editor's features, enhancing the development experience.</li> <li>pyproject.toml: Centralizes Mypy configurations and project dependencies, making it easier to maintain consistency across different environments and tools.</li> </ul>"},{"location":"explanation/mypy-configuration-guide/#conclusion","title":"Conclusion","text":"<p>By following this guide, you will have Mypy configured in your project, ensuring your Python code is type-checked and more robust.</p>"},{"location":"explanation/naming-conventions/","title":"Introduction to Naming Conventions for AI Project Assets","text":"<p>The Imperative of Naming Conventions</p> <p>In the realms of Artificial Intelligence (AI) and Machine Learning (ML), where projects are often complex and involve many moving parts, standardized naming conventions are not just a formality but a critical element of project infrastructure. These conventions act as the fundamental underpinnings of clear communication, ensuring that developers, both current and future, as well as automated tools, can navigate, understand, and manage the myriad components of a project with efficiency and precision. Such standards are the cornerstones of maintaining project integrity, enhancing collaborative efforts, and guaranteeing the reproducibility of results across various teams and environments.</p>"},{"location":"explanation/naming-conventions/#the-cornerstones-of-effective-collaboration","title":"The Cornerstones of Effective Collaboration","text":"<p>Robust naming conventions are essential for project organization in the nuanced fields of Data Science and ML. This is particularly true in an era where interdisciplinary teams and open-source collaborations are the norm. A well-defined naming protocol establishes a clear, unambiguous framework that enables every contributor, irrespective of their role or location, to comprehend the structure, purpose, and current state of project components quickly. It's a common language that transcends individual preferences and unites teams under a single standard, making collaborative efforts more streamlined and less prone to miscommunication or error.</p>"},{"location":"explanation/naming-conventions/#navigating-complexity-with-clarity","title":"Navigating Complexity with Clarity","text":"<p>Within the dense ecosystem of AI and ML projects, where datasets, models, scripts, and documentation are in constant evolution, the importance of a systematic approach to naming cannot be overstated. The right naming convention can illuminate the path through this complexity, providing immediate context and saving countless hours otherwise spent in deciphering project structures. This clarity is not just about convenience; it's about building a framework that supports rigorous scientific inquiry and experimentation, where each component, each file, each model is a well-documented piece of a larger puzzle.</p>"},{"location":"explanation/naming-conventions/#solidifying-the-foundation-for-future-work","title":"Solidifying the Foundation for Future Work","text":"<p>The adoption of standardized naming conventions lays the groundwork for current project success and sets the stage for future scalability. It's about foresight\u2014anticipating the needs of future analyses, the onboarding of new team members, and the potential for shared research. In this light, naming conventions are much more than a best practice; they are a pivotal strategy for ensuring that the work done today can stand the test of time and serve as a reliable foundation for the innovations of tomorrow.</p>"},{"location":"explanation/naming-conventions/#conclusion","title":"Conclusion","text":"<p>Adopting these naming conventions will not only streamline current workflows but will also facilitate future project scalability and knowledge transfer. They are the unsung heroes that ensure a project's structure remains intuitive and accessible to new and existing developers alike.</p>"},{"location":"explanation/pytest-configuration-guide/","title":"Pytest Configuration in VS Code","text":""},{"location":"explanation/pytest-configuration-guide/#introduction","title":"Introduction","text":"<p>This document explains the Pytest configuration for a Python project using VS Code and Poetry. The setup aims to streamline testing, ensure code quality, and enhance developer productivity by leveraging various Pytest plugins and settings.</p> <p>Default Pytest Configuration</p> <p>The Pytest configuration in this guide is the default configuration included in the \u201cCookiecutter Collabora\u201d project. This setup is designed to provide a robust starting point for running tests efficiently and effectively.</p> <p>We invite you to adapt and customize this configuration to suit your specific project needs and preferences. Modify the settings as needed to optimize your testing workflow and achieve the best results.</p>"},{"location":"explanation/pytest-configuration-guide/#configuration-overview","title":"Configuration Overview","text":"<p>The Pytest configuration is defined in two key files:</p> <ol> <li><code>.vscode/settings.json</code></li> <li><code>pyproject.toml</code></li> </ol>"},{"location":"explanation/pytest-configuration-guide/#vscodesettingsjson","title":"<code>.vscode/settings.json</code>","text":"<p>This file configures Pytest to work seamlessly within VS Code. Below is the configuration used:</p> <pre><code>{\n    \"python.testing.pytestArgs\": [\n        \"tests\",\n        \"--cov=src\",\n        \"--cov-report=term-missing\",\n        \"-v\",\n        \"-n\",\n        \"2\",\n        \"--timeout=5\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true\n}\n</code></pre>"},{"location":"explanation/pytest-configuration-guide/#explanation","title":"Explanation","text":"<ul> <li><code>python.testing.pytestArgs</code>: An array of arguments passed to Pytest when running tests in VS Code.</li> <li><code>\"tests\"</code>: Specifies the directory where test files are located.</li> <li><code>\"--cov=src\"</code>: Enables code coverage measurement for the <code>src</code> directory.</li> <li><code>\"--cov-report=term-missing\"</code>: Displays a coverage report in the terminal, highlighting missing lines.</li> <li><code>\"-v\"</code>: Enables verbose mode, providing detailed test output.</li> <li><code>\"-n\", \"2\"</code>: Utilizes <code>pytest-xdist</code> to run tests in parallel using 2 CPU cores.</li> <li> <p><code>\"--timeout=5\"</code>: Sets a 5-second timeout for each test to prevent hanging tests.</p> </li> <li> <p><code>python.testing.unittestEnabled</code>: Disables the unittest framework.</p> </li> <li><code>python.testing.pytestEnabled</code>: Enables the Pytest framework.</li> </ul>"},{"location":"explanation/pytest-configuration-guide/#pyprojecttoml","title":"<code>pyproject.toml</code>","text":"<p>This file manages the project dependencies and Pytest settings through Poetry.</p> <pre><code>[tool.poetry.dev-dependencies]\npytest = \"8.2.2\"\npytest-benchmark = \"4.0.0\"\npytest-xdist = \"3.6.1\"\npytest-cov = \"5.0.0\"\npytest-mock = \"^3.14.0\"\npytest-sugar = \"1.0.0\"\npytest-timeout = \"2.3.1\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\naddopts = \"-v --cov=src/fpservicesagemodel --cov-report=term-missing -n 2 --timeout=5\"\npython_files = \"test_*.py\"\npython_classes = \"Test*\"\npython_functions = \"test_*\"\nlog_cli = true\nlog_cli_level = \"INFO\"\nlog_format = \"%(asctime)s %(levelname)s %(message)s\"\nlog_date_format = \"%Y-%m-%d %H:%M:%S\"\n</code></pre>"},{"location":"explanation/pytest-configuration-guide/#explanation_1","title":"Explanation","text":"<ul> <li><code>[tool.poetry.dev-dependencies]</code>: Specifies the development dependencies required for the project.</li> <li><code>pytest</code>: The core testing framework.</li> <li><code>pytest-benchmark</code>: For benchmarking test performance.</li> <li><code>pytest-xdist</code>: For parallel test execution.</li> <li><code>pytest-cov</code>: For measuring code coverage.</li> <li><code>pytest-mock</code>: For mocking objects in tests.</li> <li><code>pytest-sugar</code>: For a more readable test output.</li> <li> <p><code>pytest-timeout</code>: For setting time limits on tests.</p> </li> <li> <p><code>[tool.pytest.ini_options]</code>: Configures Pytest options.</p> </li> <li><code>testpaths = [\"tests\"]</code>: Specifies the directory where test files are located.</li> <li><code>addopts = \"-v --cov=src/fpservicesagemodel --cov-report=term-missing -n 2 --timeout=5\"</code>: Additional options for running tests.<ul> <li><code>\"-v\"</code>: Enables verbose mode.</li> <li><code>\"--cov=src/fpservicesagemodel\"</code>: Measures coverage for the specified directory.</li> <li><code>\"--cov-report=term-missing\"</code>: Displays missing coverage in the terminal.</li> <li><code>\"-n 2\"</code>: Runs tests in parallel using 2 CPU cores.</li> <li><code>\"--timeout=5\"</code>: Sets a 5-second timeout for each test.</li> </ul> </li> <li><code>python_files = \"test_*.py\"</code>: Pattern for identifying test files.</li> <li><code>python_classes = \"Test*\"</code>: Pattern for identifying test classes.</li> <li><code>python_functions = \"test_*\"</code>: Pattern for identifying test functions.</li> <li><code>log_cli = true</code>: Enables logging to the console during test runs.</li> <li><code>log_cli_level = \"INFO\"</code>: Sets the log level to INFO.</li> <li><code>log_format = \"%(asctime)s %(levelname)s %(message)s\"</code>: Specifies the log message format.</li> <li><code>log_date_format = \"%Y-%m-%d %H:%M:%S\"</code>: Specifies the date format for log messages.</li> </ul>"},{"location":"explanation/pytest-configuration-guide/#pre-commit-configuration","title":"Pre-commit Configuration","text":"<p>To ensure that tests are run automatically before committing or pushing code, you can use the pre-commit framework with the following configuration in your <code>.pre-commit-config.yaml</code> file:</p> <pre><code>repos:\n  - repo: local\n    hooks:\n      - id: pytest\n        name: Run PyTest\n        entry: pytest\n        language: system\n        types: [python]\n        stages: [pre-commit, pre-push]\n        args: [\"tests/\"]\n</code></pre>"},{"location":"explanation/pytest-configuration-guide/#explanation_2","title":"Explanation","text":"<ul> <li><code>repo: local</code>: Specifies that this hook is defined locally rather than fetched from a remote repository.</li> <li><code>id: pytest</code>: The unique identifier for this hook.</li> <li><code>name: Run PyTest</code>: A descriptive name for the hook.</li> <li><code>entry: pytest</code>: The command to run Pytest.</li> <li><code>language: system</code>: Indicates that the hook uses system-installed tools.</li> <li><code>types: [python]</code>: Specifies that this hook should run for Python files.</li> <li><code>stages: [pre-commit, pre-push]</code>: Defines when the hook should run, here both before commit and before push.</li> <li><code>args: [\"tests/\"]</code>: Arguments passed to Pytest, specifying the directory to run tests from.</li> </ul> Why are Pytest extensions under [tool.poetry.dev-dependencies]? <p>Pytest is primarily used during the development phase of a project and is not intended for use in production or runtime environments. Unit tests, written using Pytest, help developers catch bugs early, facilitate code refactoring, and ensure code quality before deployment.</p> <p>Running Pytest in production can introduce unnecessary performance overhead and potential security risks. Instead, unit tests are typically executed in CI/CD pipelines to verify code functionality and reliability before the code is deployed to production. This approach ensures that production environments remain optimized for performance and stability.</p> <p>This is why we include Pytest and its extensions under <code>[tool.poetry.dev-dependencies]</code> in our configuration, ensuring they are available during development but not included in the production environment.</p> Why Configure Both .vscode/settings.json and pyproject.toml? <p>Configuring both <code>.vscode/settings.json</code> and <code>pyproject.toml</code> ensures consistent and integrated testing in your development environment.</p> <ul> <li>.vscode/settings.json: Customizes Pytest behavior in Visual Studio Code. With the Python extension installed, the beaker icon (flask) appears in the Activity bar, opening the Test Explorer for running and debugging tests directly in VS Code.</li> <li>pyproject.toml: Manages project dependencies and centralizes Pytest configurations for use across different environments.</li> </ul>"},{"location":"explanation/pytest-configuration-guide/#conclusion","title":"Conclusion","text":"<p>This configuration ensures that tests are run efficiently and effectively within VS Code, leveraging parallel execution, code coverage, and timeouts to maintain code quality and performance. By following these settings, developers can ensure a consistent and productive testing environment.</p>"},{"location":"explanation/pytest-configuration-guide/#see-also","title":"See also","text":"<ul> <li>Visual Studio Code Testing Documentation</li> <li>Visual Studio Code Python Testing Documentation</li> </ul>"},{"location":"explanation/toc-explanation/","title":"Explanation","text":"<p>About This Section</p> <p>The Explanation section delves deep into the theoretical underpinnings and practical applications within the Cookiecutter Collabora project. It serves as a pivotal educational hub for users and developers seeking to grasp the foundational concepts and sophisticated methodologies that are integral to the project.</p>"},{"location":"explanation/toc-explanation/#purpose","title":"Purpose","text":"<ul> <li> Comprehensive Insights: Aimed at offering a thorough comprehension of the project's essential concepts and operative principles.</li> <li> Clarity and Accessibility: Delivers lucid explanations complemented by illustrative examples, demystifying complex subjects.</li> <li> Educational Bedrock: Fosters a robust knowledge base, empowering users to proficiently navigate and contribute to the project.</li> </ul>"},{"location":"explanation/toc-explanation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding Data Version Control (DVC)</li> <li>Git Branches in DVC: A Collaborative Perspective</li> <li>GitHub Naming Conventions: A Key to Organized Collaboration</li> <li>Model Persistence Naming Conventions: Ensuring Consistency and Efficiency</li> <li>Introduction to Naming Conventions for AI Project Assets</li> <li>Pytest Configuration Guide</li> <li>Mypy Configuration Guide</li> </ul> <p>Enhancing Conceptual Clarity</p> <p>Arranged in an intuitive sequence, the Explanation section encourages exploratory learning and incremental acquisition of knowledge. Each entry is designed to augment the reader's understanding, providing comprehensive explanations and tangible examples that connect theory with practice.</p>"},{"location":"getting-started/config_files_guidelines/","title":"Configuration File and Folder Modification Guidelines","text":"<p>This table outlines the modification permissions and guidelines for various configuration files and folders within the project. Adhering to these guidelines ensures consistency, maintainability, and adherence to best practices across the project.</p> Configuration File/Section Modification of Existing Content Allowed Additional Content Allowed Explanation .vscode/settings.json Ensures that all team members adhere to the same standards for the project. .vscode/extensions.json , in User settings only Developers can add extensions as part of their User settings but should not modify the remote or workspace settings. .github/black.yaml The GitHub Actions workflow for the Black formatter ensures consistent code formatting across the project. .github/ruff.yaml The GitHub Actions workflow for the Ruff linter enforces coding standards and detects issues in the codebase. .gitignore Developers can add entries to specify files and directories that should be ignored by Git. poetry.lock This file ensures reproducibility by locking dependencies to specific versions and should not be modified manually. mkdocs.yml  (except <code>nav:</code> section)  (<code>nav:</code> section only) Configuration file for MkDocs documentation generation. Developers can add lines to the <code>nav:</code> section for navigation structure. Makefile The Makefile contains predefined commands for common tasks. Developers can add new targets but should not modify existing ones. Dockerfile The Dockerfile contains instructions for containerizing the application and should not be modified to ensure consistent build environments. site folder The <code>site</code> folder is generated by MkDocs and should not be modified manually to ensure the documentation build process remains consistent. pyproject.toml [tool.poetry.dependencies] Packages necessary for the application to run in both development and production environments. pyproject.toml [tool.poetry.dev-dependencies] Packages only necessary for development and should not be included in the production environment. pyproject.toml [tool.black] This section configures the Black formatter to maintain consistent code style across the project. pyproject.toml [tool.ruff] This section configures the Ruff linter to enforce coding standards and detect issues in the codebase. pyproject.toml [tool.pymarkdown] This section configures the PyMarkdown tool to ensure markdown files adhere to project standards. pyproject.toml [tool.coverage.report] This section configures the coverage report tool to monitor and enforce code coverage standards."},{"location":"getting-started/purpose_and_vision/","title":"Welcome to Cookiecutter Collabora Documentation","text":"<p>Cookiecutter Collabora is a specialized Cookiecutter template designed to facilitate collaborative artificial intelligence, machine learning, and generative AI projects. This template serves as an extensive framework, bringing together a comprehensive set of tools and practices that meet the evolving demands of modern AI development.</p>"},{"location":"getting-started/purpose_and_vision/#purpose-and-vision","title":"Purpose and Vision","text":"<p>Our goal is to establish a standardized foundation for your AI projects, akin to Collabora moving forward in harmony, to guarantee both diversity and uniformity from the beginning. The Cookiecutter Two Lanes template integrates best practices for collaboration and reproducibility, as advocated in \"The Turing Way,\" promoting an open culture of data science and research within the AI community.</p> <p>The Cookiecutter Collabora template is crafted for ML professionals who aspire to engage in collaborative and reproducible research methodologies. It lays the groundwork for projects to be conducted with efficiency and precision, providing a robust starting point for innovation and discovery in the field of AI.</p>"},{"location":"getting-started/resource_compilation/","title":"Resource Compilation","text":"<ul> <li> <p> Configure the working environment   Configure your data science project's working environment according to the Explanation documentation.</p> </li> <li> <p> Learn how to use tools How-To Guides teach you how to produce professional code and documentation using the tools available in your working environment.</p> </li> <li> <p> Step-by-step guide Tutorials demonstrate how to use the tools in your working environment to complete an end-to-end toy data science project.</p> </li> <li> <p> Reference   Reference material, such as naming conventions for Data Science items, can be found in the API Reference section.</p> </li> </ul>"},{"location":"getting-started/resource_compilation/#resource-compendium","title":"Resource Compendium","text":"Conventions <ul> <li> Project Scaffolding Standards - A comprehensive guide on establishing a standardized folder and file structure for AI/ML projects, enhancing organization and collaboration within the AI team.</li> <li> Introduction to Naming Conventions for AI Project Assets - An essential guide that underscores the significance of systematic naming conventions in fostering clear communication, efficient project management, and reproducibility in AI and ML collaborations.</li> <li> File Naming Conventions - A comprehensive guide on using the snake_case naming convention for all file types in your project to maintain consistency and readability.</li> <li> Column Naming Conventions for ML/AI Projects - A comprehensive guide on using the snake_case naming convention for column names across different types of data files in your ML/AI projects to maintain consistency and readability.</li> <li> Python Docstrings Conventions - A detailed guide on how to write and automate Google-style docstrings in Python projects, ensuring clear and maintainable code documentation.</li> <li> Code and Comment Length Standards in Python Projects - A detailed guide on maintaining consistent line lengths for code and comments in Python projects, including configuration and usage of VS Code tools to enforce these standards.</li> <li> Effective GitHub Naming Conventions - Essential guidelines for naming GitHub repositories in data science, enhancing clarity and organization.</li> <li> Git Branch Naming Standards - A comprehensive guide on standardized branch naming for ML projects, enhancing repository clarity and collaboration.</li> <li> Commit Message Standards in ML Projects - A guide on structuring and standardizing commit messages to improve team collaboration in ML projects.</li> <li> Best Practices for Using Git and Pushing to GitHub - A guide outlining best practices for effective use of Git and GitHub, including commit frequency, message standards, branch management, and collaboration techniques.</li> <li> ML Data Folder Naming Guide - A comprehensive guide to naming data folders in ML projects for enhanced organization and efficiency.</li> <li> Model Persistence File Naming Conventions - Guide on naming conventions for persisting machine learning models, covering format, versioning, metadata storage, and documentation.</li> <li> Automating Metadata Creation - A practical guide for automating the creation and saving of machine learning model metadata using a Python script.</li> <li> Markdown Documentation for ML Models - Step-by-step tutorial on documenting machine learning models using Markdown, covering the training process, parameters, performance metrics, and metadata.</li> <li> Notebook and Script Naming Conventions in ML Projects - Essential guidelines for naming Jupyter notebooks and scripts in ML projects, emphasizing clarity, consistency, and efficient file management.</li> <li> Enforcing Naming Conventions with GitHub Actions - A practical guide to set up GitHub Actions for enforcing naming conventions in machine learning projects, ensuring consistency and organization.</li> <li> Templates for pull requests, issues/stories, feature and readme files</li> <li> Branching Strategies for ML Projects - Branching strategies for AI-based projects.</li> <li> Python Scripting for Data Conversion - Detailed Python functions for converting data between XLSX and CSV formats, tailored for machine learning experts working on NLP projects.</li> <li> Project Data Management Practices - Best practices for organizing and managing project data, including folder structure, naming conventions, and centralized data repository usage.</li> <li> Using Code Tags - A practical guide on why and how to use code tags in ML/AI projects, leveraging VS Code extensions to enhance code readability and collaboration.</li> <li> Using TODO Tree with Code Tags - A guide on how to effectively use the TODO Tree VS Code extension in combination with code tags to manage tasks and comments in your codebase.</li> <li> AI/ML Project Lifecycle with Git and GitHub - A comprehensive guide outlining the lifecycle of using Git and GitHub for AI/ML projects, detailing steps from opening a JIRA issue to managing successful and unsuccessful experiments.</li> <li> Introduction to Doctest - A beginner-friendly guide to using doctest for unit testing and documentation in Python, tailored for data scientists in machine learning projects.</li> <li> Python OOP for Machine Learning Projects - A comprehensive guide on using Object-Oriented Programming (OOP) in Python to create modular, collaborative, and reproducible code for machine learning projects.</li> <li> Code Review Best Practices - A comprehensive guide on conducting effective code reviews, covering the importance, best practices, and workflows to ensure code quality and collaboration in ML/AI projects.</li> <li> Best Practices for Creating JIRA Stories for ML/AI Projects - A comprehensive guide to creating well-defined JIRA stories for ML/AI projects, ensuring clarity, actionable tasks, and alignment with project goals.</li> <li> Moving from Jupyter Notebooks to Production Python Code - A comprehensive guide on transitioning your ML/AI prototype code from Jupyter notebooks to production-ready Python scripts, covering class design, formatting, testing, and more.</li> <li> Using Pre-Commit Hooks to Enforce Coding Standards - A detailed guide on setting up and using pre-commit hooks to maintain code quality and adherence to coding standards, including descriptions and examples of each hook.</li> <li> Using Configuration Files to Avoid Hardcoding Values - Guidelines on best practices for creating and managing YAML configuration files to externalize hardcoded values in Python scripts, enhancing maintainability and flexibility.</li> </ul> Python Code Quality Tools <ul> <li> Formatting Your Code with Black - A comprehensive guide on using Black to format Python code, ensuring consistency and readability across your codebase.</li> <li> Accelerating Linting with Ruff - Learn how to integrate Ruff for fast and efficient linting to maintain high-quality Python code.</li> <li> Static Type Checking with Mypy - An in-depth guide on using Mypy to enhance your Python code with static type checking, including type inference, union types, optionals, and advanced features like overloads and generics.</li> <li> Pytest Introduction Guide - A beginner's guide to unit testing with Pytest, covering the basics of unit testing, its importance, and how to write and run tests using Pytest.</li> <li> Pytest Configuration Guide - Detailed explanation and setup for Pytest configuration within VS Code, ensuring efficient and effective test execution.</li> <li> Mypy Configuration Guide - Detailed explanation and setup for Mypy configuration within VS Code and <code>pyproject.toml</code>, ensuring efficient and effective type checking.</li> </ul> Data Version Control with DVC <ul> <li> Understanding DVC - Essentials of DVC for large datasets management in ML, covering integration, versioning, and reproducibility.</li> <li> Setting Up DVC - Comprehensive setup guide for DVC, from initialization to managing data in the cloud.</li> <li> Local Data Updates - Step-by-step tutorial on managing and tracking local dataset updates with DVC.</li> <li> Cloud Data Updates - Tutorial on syncing cloud data updates in services like AWS S3 or Azure Blob Storage using DVC.</li> <li> Data Version Communication - Guidelines for effective communication of data versions in collaborative settings using DVC and GitHub.</li> <li> Collaborative Data Workflow - Workflow for collaborative data updates, tracking changes, and team syncing with DVC and GitHub.</li> <li> Branches for Data Versions - Understanding the use of Git branches for data version management and DVC integration with collaboration tools.</li> <li> DVC in VS Code - How to use the DVC extension in Visual Studio Code for efficient data version control and management.</li> </ul> Data Management <ul> <li> Effective Data Documentation - A comprehensive guide on creating and managing metadata and documentation for data science projects, emphasizing importance, standard practices, and automation.</li> <li> Metadata Integration in Data Analysis - A practical guide on integrating metadata with data analysis tools, detailing steps, strategies, and an example Python script for effective collaboration. </li> <li> Creating and Managing Metadata and Documentation - An in-depth tutorial on crafting effective metadata and maintaining thorough data documentation, complete with steps, examples, and best practices.</li> </ul> Weights &amp; Biases Experiment Tracking <ul> <li> ML Experiments Life-Cycle with Weights &amp; Biases - This guide provides a comprehensive overview of managing the life-cycle of machine learning (ML) experiments using Weights &amp; Biases (W&amp;B). It offers a structured approach for documenting experiments, establishing coding practices, configuring and tracking experiments with W&amp;B, and efficiently managing outcomes.</li> <li> Automating Backups for Weights &amp; Biases - A comprehensive guide dedicated to establishing robust backup mechanisms for Weights &amp; Biases experiments on your MacBook Pro</li> </ul> VS Code Configuration <ul> <li> Sharing VS Code Settings for Python Projects - A detailed guide on how to maintain consistent VS Code settings across your Python projects, ensuring uniform coding standards and best practices.</li> </ul>"},{"location":"getting-started/software-development-best-practices/","title":"Software Development Best Practices Guide","text":""},{"location":"getting-started/software-development-best-practices/#github-repository-management","title":"GitHub Repository Management","text":"<p>One Project, One GitHub Repo</p> <p>It is best practice to create one GitHub repository per project. This helps to maintain clarity, organization, and manageability. Each project should have a dedicated repository to keep all relevant code, documentation, and resources in one place.</p> <p>Standard Repo Naming Convention: Use <code>kebab-case</code> for naming your repositories.</p> <p>Example: <code>my-github-repo</code></p> <p>REad more</p> <p>Repository Scaffolding</p> <p>Standardizing the project structure ensures consistency, ease of navigation, and efficient collaboration across projects. When code is always in the <code>src</code> folder, team members can quickly locate files. This also facilitates onboarding of new team members, improves maintainability, and enhances the overall productivity by reducing the time spent searching for files and understanding project layout.</p> <p>Read more about repository scaffolding best practices</p> <p>Standardize the README.md File</p> <p>The <code>README.md</code> file is crucial for providing an overview of the project, instructions for setup, usage, and contribution guidelines. Standardizing this file ensures that every project has a clear and informative introduction, making it easier for new developers and users to understand and contribute to the project. A well-maintained <code>README.md</code> can significantly enhance the accessibility and usability of the project.</p> <p>Read more about README.md best practices</p> <p>One Branch per JIRA Story/Bug</p> <p>Creating a separate branch for each JIRA story or bug ensures that changes are isolated and can be reviewed and tested independently. This practice improves code quality and makes it easier to track changes related to specific issues.</p> <p>Read more about branching best practices</p> <p>Multiple Commits per Branch</p> <p>Making multiple, small commits helps to save progress and reduces the risk of losing work. It also makes it easier to review changes and identify issues. Each commit should represent a logical unit of work and include a meaningful commit message.</p> <p>Commit Best Practices</p> <p>Committing frequently helps ensure progress is saved and reduces the risk of losing work. Each commit should represent a logical unit of work and include a meaningful commit message. This practice improves code quality and makes it easier to track changes.</p> <p>Read more about commit best practices</p>"},{"location":"getting-started/software-development-best-practices/#environment-management","title":"Environment Management","text":"<p>Python Virtual Environments</p> <p>Using a dedicated Python virtual environment for each project helps avoid conflicts between dependencies and ensures that the project\u2019s dependencies are isolated. This practice prevents the base environment from being inadvertently modified and makes it easier to manage and replicate the project environment.</p> <p>Read more about Python virtual environments</p>"},{"location":"getting-started/software-development-best-practices/#coding-standards","title":"Coding Standards","text":"<p>Code and Text Length Standards</p> <p>Adhering to code and text length standards ensures that your code is readable and maintainable. Limiting line lengths helps prevent horizontal scrolling and makes it easier for team members to review code and understand comments.</p> <ul> <li>Comments: 72 characters</li> <li>Code: 79 characters</li> </ul> <p>Read more about Python Line Length Standards</p> <p>Python Docstring Style</p> <p>Using a consistent docstring style, such as the Google style, ensures that your documentation is clear and easy to follow. Well-written docstrings help team members understand the purpose and usage of your code, which is crucial for collaboration and maintenance.</p> <p>Read more about Python Docstring Conventions</p>"},{"location":"getting-started/software-development-best-practices/#data-management","title":"Data Management","text":"<p>Data Folder Structure</p> <p>Organizing your data folder by data processing stage helps maintain clarity and order in your projects. This approach makes it easier to locate and manage data files relevant to different stages of data processing.</p> <p>Read more about ML Data Folder Naming Guide</p> <p>Field/Column Naming</p> <p>Using snake_case for field and column names ensures consistency and readability across your datasets. This practice facilitates easier data manipulation and querying.</p> <p>Read more about Column Naming Conventions</p> <p>File Naming Convention</p> <p>Adhering to a consistent file naming convention like snake_case helps maintain organization and makes it easier to find and manage files. This practice is crucial for collaborative work and long-term project maintenance.</p> <p>Read more about File Naming Conventions</p>"},{"location":"getting-started/software-development-best-practices/#experiment-tracking","title":"Experiment Tracking","text":"<p>Experiment Tracking Practices</p> <p>Using a consistent experiment tracking practice is crucial for reproducibility and effective collaboration in machine learning projects. Tools like Weights &amp; Biases (W&amp;B) help you keep track of your experiments, visualize results, and share findings with your team.</p> <p>Read more about W&amp;B Experiment Tracking for RAG</p>"},{"location":"getting-started/software-development-best-practices/#object-oriented-programming-oop","title":"Object-Oriented Programming (OOP)","text":"<p>OOP Best Practices</p> <p>Designing classes and methods properly is essential for creating maintainable and scalable code in machine learning projects. Following OOP principles ensures that your code is organized, reusable, and easy to understand.</p> <p>Read more about Python OOP for ML Projects</p>"},{"location":"getting-started/software-development-lifecycle/","title":"Software Development Life Cycle","text":""},{"location":"getting-started/software-development-lifecycle/#introduction","title":"Introduction","text":"<p>This flowchart provides a visual representation of the key stages involved in the software development life cycle. Each step is accompanied by detailed documentation to guide you through best practices and standardized processes.</p>"},{"location":"getting-started/software-development-lifecycle/#flowchart","title":"Flowchart","text":"<pre><code>---\ntitle: Software Development Cycle\n---\nflowchart TB\n  A([1. Open the JIRA story &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira'&gt;link&lt;/a&gt;]) ==&gt; B([2. Create a branch from main \ud83e\ude9d &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;link&lt;/a&gt;])\n  B ==&gt; C([3. Develop Python code])\n  C ==&gt; D([4. Debug Python code])\n  D ==&gt; E([5. Unit test with PyTest \ud83e\ude9d &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/pytest-inroduction-guide/'&gt;link&lt;/a&gt;])\n  E ==&gt; F([6. Type check with Mypy &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/type-checking-mypy/?h=type+che'&gt;link&lt;/a&gt;])\n  F ==&gt; G([7. Refactor code])\n  G ==&gt; H([8. Format code with Black \ud83e\ude9d &lt;a href='https://markeyser.github.io/cookiecutter-collabora/tutorials/black-formatter/?h=black'&gt;link&lt;/a&gt;])\n  H ==&gt; I([9. Lint code with Ruff \ud83e\ude9d &lt;a href='https://markeyser.github.io/cookiecutter-collabora/tutorials/ruff-linter/?h=lint'&gt;link&lt;/a&gt;])\n  I ==&gt; J([10. Pass pre-commit hooks \ud83e\ude9d &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/pre-commit-hooks-guide/'&gt;link&lt;/a&gt;])\n  J ==&gt; K([11. Update documentation &lt;a href='https://markeyser.github.io/cookiecutter-collabora/tutorials/mkdocs-docs/?h=mkd'&gt;link&lt;/a&gt;])\n  K ==&gt; L([12. Commit changes \ud83e\ude9d &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/commit-message-standards-ml/?h=commit'&gt;link&lt;/a&gt;])\n  L ==&gt; M([13. Push to remote repo])\n  M --&gt; |Repeat as necessary for multiple commits|C\n  M ==&gt; N([14. Open a Pull Request &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/templates/?h=pull'&gt;link&lt;/a&gt;])\n  N ==&gt; O([15. Code Review &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/code-review-best-practices/?h=review'&gt;link&lt;/a&gt;])\n  O ==&gt; P([16. Address review feedback])\n  P ==&gt; Q([17. Merge into main])\n  Q ==&gt; R([18. Continuous Integration/Deployment])\n  R ==&gt; S([19. Integration Testing])\n  S ==&gt; T([20. Deploy to Staging])\n  T ==&gt; U([21. Monitoring and Logging])\n  U ==&gt; |Start again with a new issue|A\n\n  classDef highlighted fill:#d9534f,stroke:#333,stroke-width:2px,color:#fff;\n  classDef blue fill:#5bc0de,stroke:#333,stroke-width:2px,color:#fff;\n  class A,B,C,F,H,I,J,L,M,N highlighted;\n  class R,S,T,U blue;</code></pre> <pre><code>---\ntitle: Software Development Cycle\n---\nflowchart TB\n  A(\"&lt;div style='text-align: left;'&gt;&lt;b&gt;1. Initiation and Planning:&lt;/b&gt; &lt;br&gt;\u2022 Scope and Requirements &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Open Jira Story&lt;/a&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Create a branch&lt;/a&gt;&lt;/div&gt;\")\n  B(\"&lt;div style='text-align: left;'&gt;&lt;b&gt;2. Development:&lt;/b&gt; &lt;br&gt;\u2022 Develop Python Code&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Error handling and logging&lt;/a&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Inline comments and docstrings&lt;/a&gt;&lt;/div&gt;\")\n  A ==&gt; B\n  C(\"&lt;div style='text-align: left;'&gt;&lt;b&gt;3. Refactoring:&lt;/b&gt; &lt;br&gt;\u2022 Type hints&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/cofig-files/' style='text-decoration: underline; color: #ff00ff;'&gt;Config files&lt;/a&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/config-files/'&gt;Document&lt;/a&gt;&lt;/div&gt;\")\n  B ==&gt; C\n  D(\"&lt;div style='text-align: left;'&gt;&lt;b&gt;4. Quality:&lt;/b&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Run formatter&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Run linter&lt;/a&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Run Static typing checker&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Perform unit tests&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Conduct integration testing&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Optimize performance&lt;/a&gt;&lt;/div&gt;\")\n  C ==&gt; D\n  E(\"&lt;div style='text-align: left;'&gt;&lt;b&gt;4. Comprehensive Testing and Review:&lt;/b&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Perform end-to-end testing&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Conduct code coverage analysis&lt;/a&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Perform additional unit tests&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Perform unit tests&lt;/a&gt;&lt;/div&gt;\")\n  D ==&gt; E\n  F(\"&lt;div style='text-align: left;'&gt;&lt;b&gt;5. Quality Assurance and Compliance:&lt;/b&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Pass pre-commit hooks&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/jira-story-best-practices/?h=jira' style='text-decoration: underline; color: #ff00ff;'&gt;Update documentation&lt;/a&gt; &lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Perform additional unit tests&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Perform unit tests&lt;/a&gt;&lt;br&gt;\u2022 &lt;a href='https://markeyser.github.io/cookiecutter-collabora/how-to-guides/git-branch-naming-standards/'&gt;Prepare deployment procedures and update CI/CD pipelines&lt;/a&gt;&lt;/div&gt;\")\n  E ==&gt; F\n  classDef highlighted fill:#d9534f,stroke:#333,stroke-width:2px,color:#fff;\n  classDef blue fill:#5bc0de,stroke:#333,stroke-width:2px,color:#fff;\n  class H,I,J,L,M,N highlighted;\n  class R,S,T,U blue;</code></pre>"},{"location":"getting-started/standards_summary/","title":"Standards Summary","text":"Category Standard Resources File Naming snake_case File Naming Conventions Field/Column Naming snake_case Column Naming Conventions Python Docstring Style Google Style Python Docstring Conventions Length Standards Comments: 72 chars, Code: 79 chars Python Line Length Standards Formatter Black using GitHub Actions Black Formatter Linter Ruff using GitHub Actions - all <code>.py</code> and <code>.ipynb</code> Ruff Linter Branching Strategy GitHub Flow Branching Strategy Commit Message Conventional Commits Commit Message Standards in ML Projects Code Review Peer Review Code Review Best Practices Branch Names <code>&lt;category&gt;/&lt;description&gt;-&lt;JIRA_number&gt;</code> Git Branch Naming Standards Technical Docs MkDocs with Material Theme MkDocs Documentation Testing Doctest and/or PyTest Introduction to Doctest Data Folder Structure Organized by data processing stage ML Data Folder Naming Guide Project Structure Standardized Scaffolding Project Scaffolding Standards Exploratory Analysis Jupyter Notebooks Steps to Move a Prototype from Jupyter Notebook to Production-Ready Python Class Object-Oriented Programming Design classes and methods properly Python OOP for ML Projects JIRA Stories Track work effectively Best Practices for Creating JIRA Stories for ML/AI Projects Experiment Tracking W&amp;B Experiment Tracking for RAG W&amp;B Experiment Tracking for RAG Lifecycle Management Comprehensive Git and GitHub usage Lifecycle Using Git and GitHub Pushing to GitHub Follow best practices Pushing to GitHub Best Practices <p>Branch Protection with Ruff and Black</p> <p>To ensure code quality, the <code>dev</code> branch is protected with branch protection rules. This setup requires that all code must pass Ruff linting checks before being merged into the <code>dev</code> branch. While you can push code to your feature branches even if it fails linting and/or formatting, merging into <code>dev</code> is blocked until all linting and formatting errors are resolved. This approach mimics pre-commit hooks, maintaining a clean and compliant <code>dev</code> branch. Developers should run Ruff and Black locally and fix issues before pushing code for review.</p>"},{"location":"getting-started/usage/","title":"Usage","text":""},{"location":"getting-started/usage/#installing-cookiecutter","title":"Installing Cookiecutter","text":"<p>Before starting, ensure Cookiecutter is installed on your system:</p> <pre><code>pip install cookiecutter\n</code></pre>"},{"location":"getting-started/usage/#generate-a-new-project","title":"Generate a New Project","text":"<p>Create a new project by running:</p> <pre><code>cookiecutter gh:markeyser/cookiecutter-collabora.git\n</code></pre> <p>You will be prompted to enter details for your project, such as project name, OS type, author name, and more. For example:</p> <pre><code>[1/17] project_name (Project Name): My AI Project\n[2/17] project_slug (Project Slug): my-ai-project\n[3/17] package_name (Package Name): myaiproject\n[4/17] env_name (Environment Name): my-ai-project-env\n[5/17] author (Author Name): Jane Doe\n[6/17] email (Author Email): jane.doe@example.com\n[7/17] description (Project Description): AI project development\n[8/17] reviewer (Independent Reviewer GitHub): \n[9/17] site_name (Documentation Site Name): My AI Project Docs\n[10/17] os (Operating System) [Linux, Windows, Mac]: Linux\n[11/17] cloud (Cloud Provider) [Google Cloud, AWS, Azure]: AWS\n[12/17] ml_type (ML Project Type) [Package, Supervised, Unsupervised, Generative AI]: Generative AI\n[13/17] data_type (Data Type) [Structured, Unstructured, Semi-structured]: Unstructured\n[14/17] packaging (Packaging Tool) [Poetry, Conda]: Poetry\n[15/17] venv (Create Virtual Environment) [Yes, No]: Yes\n[16/17] formatter (Code Formatter) [Black, Ruff]: Black\n[17/17] docker (Use Docker with VS Code Dev Containers) [Yes, No]: Yes\nINFO:root:Current working directory: /Users/username/Projects/my-ai-project\n</code></pre> <p>This process will generate a new project with your specific configurations.</p> <p>This will create a new project based on the <code>cookiecutter-collabora</code> template with your specified details.</p> <pre><code>cd my-ai-project\n</code></pre> <p>Once you're in your project's directory, you can open the entire directory in Visual Studio Code:</p> <pre><code>code .\n</code></pre>"},{"location":"getting-started/usage/#generate-a-project-from-a-downloaded-template","title":"Generate a Project from a Downloaded Template","text":"<p>If you have already downloaded the Cookiecutter template:</p>"},{"location":"getting-started/usage/#listing-installed-cookiecutter-templates","title":"Listing Installed Cookiecutter Templates","text":"<p>To see installed Cookiecutter templates, use:</p> <pre><code>cookiecutter --list-installed\n1 installed templates:\n * cookiecutter-collabora\n</code></pre>"},{"location":"getting-started/usage/#locating-the-cookiecutters-directory-on-a-mac","title":"Locating the <code>.cookiecutters</code> Directory on a Mac","text":"<p>Your cloned cookiecutters are usually stored in <code>~/.cookiecutters/</code>.</p> <ol> <li> <p>Open Terminal: Find Terminal in Applications &gt; Utilities, or use    Spotlight.</p> </li> <li> <p>Navigate to Home Directory: Type <code>cd ~</code> and press Enter.</p> </li> <li> <p>List Hidden Directories: Use <code>ls -a</code> to view all files and    directories, including hidden ones like <code>.cookiecutters</code>.</p> </li> <li> <p>Access the <code>.cookiecutters</code> Directory: Enter <code>cd .cookiecutters</code>.</p> </li> <li> <p>View Template Contents (Optional): Use <code>ls</code> to view the templates    in the <code>.cookiecutters</code> directory.</p> </li> </ol> <pre><code>/Users/username/.cookiecutters/cookiecutter-collabora\n</code></pre>"},{"location":"getting-started/usage/#creating-a-new-project-with-a-template","title":"Creating a New Project with a Template","text":"<p>To create a project using the <code>cookiecutter-collabora</code> template:</p> <pre><code>cookiecutter /Users/username/.cookiecutters/cookiecutter-collabora\n</code></pre> <p>Enter the details for your project when prompted. For instance:</p> <pre><code>[1/17] project_name (Project Name): My AI Project\n[2/17] project_slug (Project Slug): my-ai-project\n[3/17] package_name (Package Name): myaiproject\n[4/17] env_name (Environment Name): my-ai-project-env\n[5/17] author (Author Name): Jane Doe\n[6/17] email (Author Email): jane.doe@example.com\n[7/17] description (Project Description): AI project development\n[8/17] reviewer (Independent Reviewer GitHub): \n[9/17] site_name (Documentation Site Name): My AI Project Docs\n[10/17] os (Operating System) [Linux, Windows, Mac]: Linux\n[11/17] cloud (Cloud Provider) [Google Cloud, AWS, Azure]: AWS\n[12/17] ml_type (ML Project Type) [Package, Supervised, Unsupervised, Generative AI]: Generative AI\n[13/17] data_type (Data Type) [Structured, Unstructured, Semi-structured]: Unstructured\n[14/17] packaging (Packaging Tool) [Poetry, Conda]: Poetry\n[15/17] venv (Create Virtual Environment) [Yes, No]: Yes\n[16/17] formatter (Code Formatter) [Black, Ruff]: Black\n[17/17] docker (Use Docker with VS Code Dev Containers) [Yes, No]: Yes\nINFO:root:Current working directory: /Users/username/Projects/my-ai-project\n</code></pre> <p>This will create a new project based on the <code>cookiecutter-collabora</code> template with your specified details.</p> <pre><code>cd my-ai-project\n</code></pre> <p>Once you're in your project's directory, you can open the entire directory in Visual Studio Code:</p> <pre><code>code .\n</code></pre>"},{"location":"getting-started/usage/#resulting-directory-structure","title":"Resulting Directory Structure","text":"<p>Below is an example directory structure for a Generative AI project that fine-tunes a Named Entity Recognition (NER) model using SpaCy Transformers to create a Python package:</p> <pre><code>.\n\u251c\u2500\u2500 .benchmarks                     # Benchmark files for performance testing.\n\u251c\u2500\u2500 .devcontainer                   # Configuration files for development containers.\n\u251c\u2500\u2500 .github                         # GitHub-specific files.\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md             # Guidelines for contributing to the project.\n\u2502   \u251c\u2500\u2500 PULL_REQUEST_TEMPLATE       # Template for pull request descriptions.\n\u2502   \u2514\u2500\u2500 workflows                   # GitHub Actions workflows.\n\u2502       \u251c\u2500\u2500 black.yaml              # Workflow for Black formatter.\n\u2502       \u2514\u2500\u2500 ruff.yaml               # Workflow for Ruff linter.\n\u251c\u2500\u2500 .vscode                         # VS Code configuration.\n\u2502   \u251c\u2500\u2500 cspell.json                 # Spell checker configuration.\n\u2502   \u251c\u2500\u2500 dictionaries                # Custom dictionaries for spell checker.\n\u2502   \u2502   \u2514\u2500\u2500 data-science-en.txt\n\u2502   \u251c\u2500\u2500 extensions.json             # Recommended VS Code extensions.\n\u2502   \u2514\u2500\u2500 settings.json               # VS Code workspace settings.\n\u251c\u2500\u2500 .env                            # Environment variables configuration file.\n\u251c\u2500\u2500 .gitignore                      # Specifies intentionally untracked files to ignore.\n\u251c\u2500\u2500 .coverage                       # Code coverage report.\n\u251c\u2500\u2500 CHANGELOG.md                    # Changelog for the project.\n\u251c\u2500\u2500 Makefile                        # Makefile with commands like `make data` or `make train`.\n\u251c\u2500\u2500 README.md                       # The top-level README for developers using this project.\n\u251c\u2500\u2500 config                          # Configuration files for the project.\n\u2502   \u251c\u2500\u2500 base_config.yaml\n\u251c\u2500\u2500 dist                            # Distribution files\n\u251c\u2500\u2500 docs                            # Project documentation.\n\u2502   \u251c\u2500\u2500 api-reference.md\n\u2502   \u251c\u2500\u2500 assets\n\u2502   \u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 custom.css\n\u2502   \u2502   \u2514\u2500\u2500 logo.png\n\u2502   \u251c\u2500\u2500 explanation.md\n\u2502   \u251c\u2500\u2500 how-to-guides.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u2514\u2500\u2500 tutorials.md\n\u251c\u2500\u2500 mkdocs.yml                      # MkDocs configuration for generating documentation.\n\u251c\u2500\u2500 notebooks                       # Jupyter notebooks for exploration and analysis.\n\u251c\u2500\u2500 poetry.lock                     # Poetry lock file for dependencies.\n\u251c\u2500\u2500 pyproject.toml                  # Project configuration file for dependencies and tools.\n\u251c\u2500\u2500 src                             # Source code for the project.\n\u2502   \u251c\u2500\u2500 package_name                # Main package directory.\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py             # Initialization file for the package.\n\u2502   \u2502   \u251c\u2500\u2500 module1.py              # Generic module 1.\n\u2502   \u2502   \u251c\u2500\u2500 module2.py              # Generic module 2.\n\u2502   \u2502   \u251c\u2500\u2500 module3.py              # Generic module 3.\n\u2502   \u2502   \u251c\u2500\u2500 data                    # Data handling modules.\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 external\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 features\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 interim\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 processed\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 raw\n\u2502   \u2502   \u251c\u2500\u2500 logs\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 application.log\n\u2502   \u2502   \u2514\u2500\u2500 models                  # Model files\n\u2502   \u251c\u2500\u2500 common\n\u2502   \u2502   \u2514\u2500\u2500 utils.py                # Common utility functions.\n\u2514\u2500\u2500 tests                           # Unit and integration tests.\n    \u2514\u2500\u2500 __pycache__\n        \u251c\u2500\u2500 test_module1.py\n        \u2514\u2500\u2500 test_module2.py\n</code></pre>"},{"location":"how-to-guides/automating-wandb-backups/","title":"How to Back Up Weights &amp; Biases Experiments Locally on MacBook Pro","text":"<p>This guide provides step-by-step instructions on how to back up your Weights &amp; Biases (W&amp;B) experiments when running locally on a MacBook Pro. Following the Detaxis approach for structured technical documentation, this guide ensures that you can securely and efficiently safeguard your data.</p>"},{"location":"how-to-guides/automating-wandb-backups/#define-the-task","title":"Define the Task","text":"<p>Backing up W&amp;B experiments involves saving experiment data, metadata, and artifacts to prevent data loss and ensure reproducibility of results.</p>"},{"location":"how-to-guides/automating-wandb-backups/#assumptions","title":"Assumptions","text":"<ul> <li>You have W&amp;B set up and running locally on your MacBook Pro.</li> <li>Basic familiarity with terminal commands.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#tools-and-requirements","title":"Tools and Requirements","text":"<ul> <li>Terminal access on MacBook Pro.</li> <li>Optional: Access to cloud storage services or an external hard drive   for remote backups.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#task-instructions","title":"Task Instructions","text":""},{"location":"how-to-guides/automating-wandb-backups/#1-export-wb-data","title":"1. Export W&amp;B Data","text":"<ul> <li>Action: Use the W&amp;B CLI to export project data.</li> <li>Commands:     <pre><code>wandb export USERNAME/PROJECT_NAME --output file.json\n</code></pre>     Replace <code>USERNAME</code> with your W&amp;B username and <code>PROJECT_NAME</code> with     the name of your project.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#2-database-backup","title":"2. Database Backup","text":"<ul> <li>Action: Locate and copy the W&amp;B local SQLite database for backup.</li> <li>Location: The default W&amp;B directory is <code>~/wandb</code>. Find the   <code>local.db</code> file within this directory.</li> <li>Backup Command:     <pre><code>cp ~/wandb/local.db /path/to/your/backup/location\n</code></pre></li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#3-artifact-storage","title":"3. Artifact Storage","text":"<ul> <li>Action: Manually back up the artifacts directory.</li> <li>Command:     <pre><code>cp -R ~/wandb/artifacts /path/to/your/backup/location\n</code></pre></li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#4-automate-backups","title":"4. Automate Backups","text":"<ul> <li>Action: Create a script to automate the backup process.</li> <li>Details: Use the provided example backup script, modifying paths   and destinations as needed. Schedule this script using <code>cron</code> for   periodic backups.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#5-version-control-integration","title":"5. Version Control Integration","text":"<ul> <li>Action: Ensure that your project code is under version control   (e.g., Git).</li> <li>Purpose: Facilitates matching experiment results with code   versions, enhancing reproducibility.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#verification-steps","title":"Verification Steps","text":"<ol> <li>Check Exported Data: Ensure that the JSON or CSV file contains    all relevant experiment data.</li> <li>Validate Database Copy: Verify that the <code>local.db</code> file is    correctly copied to your backup location and is not corrupted.</li> <li>Artifact Backup: Confirm that all files in the <code>artifacts</code>    directory are present in your backup location.</li> <li>Test Automation Script: Execute your backup script manually to    ensure it works as expected. Check the backup destination for the new    backup folder.</li> <li>Review Backup Frequency: Ensure that your backup script is    scheduled appropriately and runs at the desired frequency.</li> </ol>"},{"location":"how-to-guides/automating-wandb-backups/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Issue: Export command fails.</li> <li>Solution: Check your W&amp;B username and project name for typos.     Ensure you're connected to the internet if using W&amp;B cloud.</li> <li>Issue: Backup script does not run automatically.</li> <li>Solution: Verify the <code>cron</code> job setup. Check for syntax errors     in your cron configuration.</li> <li>Issue: Insufficient backup space.</li> <li>Solution: Regularly monitor your backup location's available     space. Consider using cloud storage for additional capacity.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#automating-backups-with-a-script","title":"Automating Backups with a Script","text":"<p>To simplify and automate the backup process for Weights &amp; Biases experiments on your MacBook Pro, you can use a shell script. This section provides an example script and instructions on how to set it up for automated execution using <code>cron</code>.</p>"},{"location":"how-to-guides/automating-wandb-backups/#backup-script-example","title":"Backup Script Example","text":"<p>Below is an example of a backup script named <code>wandb_backup.sh</code>. This script backs up the W&amp;B local database and artifacts to a specified backup directory.</p> <pre><code>#!/bin/bash\n\n# Define the source and destination directories\nWANDB_DIR=\"$HOME/wandb\"\nBACKUP_DIR=\"/path/to/your/backup/location/$(date +%Y%m%d_%H%M%S)\"\n\n# Create a new backup directory with a timestamp\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup the local database\ncp \"$WANDB_DIR/local.db\" \"$BACKUP_DIR\"\n\n# Backup artifacts\ncp -R \"$WANDB_DIR/artifacts\" \"$BACKUP_DIR/artifacts\"\n\necho \"Backup completed successfully.\"\n</code></pre> <p>Instructions:</p> <ol> <li>Create the Script:</li> <li>Open your terminal.</li> <li>Use a text editor (e.g., <code>nano</code> or <code>vim</code>) to create a new file      named <code>wandb_backup.sh</code>.</li> <li>Copy and paste the script above into the file.</li> <li> <p>Replace <code>/path/to/your/backup/location</code> with the actual path where      you want to store your backups.</p> </li> <li> <p>Make the Script Executable:</p> </li> <li> <p>Run the following command in your terminal:      <pre><code>chmod +x wandb_backup.sh\n</code></pre></p> </li> <li> <p>Automating Execution with <code>cron</code>:</p> </li> <li>Open your crontab for editing by running:      <pre><code>crontab -e\n</code></pre></li> <li>Add a line to schedule the script execution. For example, to run      the backup daily at 2 AM, add:      <pre><code>0 2 * * * /path/to/your/wandb_backup.sh\n</code></pre></li> <li>Save and close the editor. <code>cron</code> will now execute the script      according to the schedule.</li> </ol>"},{"location":"how-to-guides/automating-wandb-backups/#verification-and-monitoring","title":"Verification and Monitoring","text":"<ul> <li>After setting up the script and scheduling it with <code>cron</code>, verify that   it runs as expected by checking the backup directory for new backups   after the scheduled time.</li> <li>Monitor your system logs and the backup directory to ensure ongoing   successful executions. You might want to set up email notifications   for <code>cron</code> jobs to stay informed about the backup script's execution   status.</li> </ul>"},{"location":"how-to-guides/automating-wandb-backups/#conclusion","title":"Conclusion","text":"<p>Automating the backup process ensures that your Weights &amp; Biases experiments and data are regularly saved without manual intervention. By following these steps, you create a resilient and efficient backup workflow, safeguarding your valuable machine learning experiments.</p>"},{"location":"how-to-guides/automating-wandb-backups/#conclusion_1","title":"Conclusion","text":"<p>By following these steps, you've created a robust backup strategy for your Weights &amp; Biases experiments. Regular backups ensure that your data is secure and that your experiments are reproducible, regardless of unforeseen data loss.</p>"},{"location":"how-to-guides/automating-wandb-backups/#further-assistance","title":"Further Assistance","text":"<p>For more detailed information on W&amp;B features and troubleshooting, visit the Weights &amp; Biases documentation.</p>"},{"location":"how-to-guides/branching-strategy/","title":"Branching Strategies","text":"<p>Streamlining Version Control with Git Branching</p> <p>In software engineering, maintaining a clean and organized codebase is paramount. A key principle to achieve this is the effective use of Git branches. Git branching eliminates the need for multiple versions of the same document with suffixes like \"_v01\", \"_v02\", etc. Instead, we create separate branches for different experiments or features, allowing us to work in isolation without impacting the main codebase.</p> <p>For instance, if you are working on a new feature, you can create a branch specifically for that work. This ensures that our primary script, such as <code>main_script.py</code>, remains the only version in the main branch. When a new feature or experiment is finalized and ready for production, it is merged back into the main branch, replacing the old version of the same file. This practice maintains a single, up-to-date version in the main codebase.</p> <p>Proper use of Git branches allows us to keep our projects manageable and avoids the confusion and clutter associated with manual versioning. Leveraging branches is crucial to fully utilize Git\u2019s capabilities for effective version control.</p>"},{"location":"how-to-guides/branching-strategy/#introduction","title":"Introduction","text":"<p>This document outlines the GitHub Flow branching strategy for the development and experimentation of a Retrieval-Augmented Generation (RAG) Q&amp;A system on AWS using Python. The GitHub Flow provides a simple and effective approach to continuous deployment and iterative development, making it suitable for projects that require frequent updates and a streamlined workflow.</p>"},{"location":"how-to-guides/branching-strategy/#importance-of-sticking-to-github-flow-in-rag-system-development","title":"Importance of Sticking to GitHub Flow in RAG System Development","text":"<p>In AI projects like a Retrieval-Augmented Generation (RAG) system, adhering to a specific branching strategy such as GitHub Flow is critical for several reasons:</p> <ul> <li>Consistent Development Environment:</li> <li>Ensures a structured approach where team members can work on     features and experiments in isolation.</li> <li> <p>Reduces the risk of introducing conflicts and maintains overall code     stability.</p> </li> <li> <p>Facilitates Collaboration:</p> </li> <li>Emphasizes short-lived branches and pull requests, promoting     thorough code reviews.</li> <li> <p>Enhances continuous integration, allowing for faster iteration     cycles and better team collaboration.</p> </li> <li> <p>High Code Quality:</p> </li> <li>Structured reviews and testing processes ensure that only stable and     quality code is merged into the main branch.</li> <li> <p>Regular merging and testing help catch issues early, maintaining a     high standard of code quality.</p> </li> <li> <p>Reproducibility:</p> </li> <li>Clear and standardized workflow enables accurate tracking of     changes, experiments, and their outcomes.</li> <li> <p>Essential for debugging, refining models, and ensuring that results     are reliable and replicable.</p> </li> <li> <p>Efficient Workflow:</p> </li> <li>GitHub Flow provides a seamless and efficient workflow, crucial for     the complex and iterative nature of AI development.</li> <li>Reduces the overhead of managing multiple long-lived branches,     simplifying the development process.</li> </ul> <p>By sticking to a well-defined strategy like GitHub Flow, teams can ensure a robust, collaborative, and reproducible approach to the development of RAG systems, leading to more reliable and effective AI solutions.</p>"},{"location":"how-to-guides/branching-strategy/#github-flow-overview","title":"GitHub Flow Overview","text":"<ol> <li> <p>Main Branch (<code>main</code>): The branch that holds the production-ready    code. This branch should always be stable and ready for deployment.</p> </li> <li> <p>Feature/Experimentation Branches: Branches created from <code>main</code> to    develop new features or conduct experiments. These branches are    short-lived and are merged back into <code>main</code> upon completion.</p> </li> <li> <p>Pull Requests (PRs): Used to propose changes from    feature/experimentation branches to the <code>main</code> branch. PRs are    reviewed and tested before merging to ensure code quality and    stability.</p> </li> </ol>"},{"location":"how-to-guides/branching-strategy/#detailed-workflow","title":"Detailed Workflow","text":"<ol> <li>Main Branch:</li> <li>The <code>main</code> branch holds the stable, production-ready code.</li> <li> <p>Changes are only merged into <code>main</code> through pull requests to ensure      stability and code review.</p> </li> <li> <p>Starting Feature/Experimentation Work:</p> </li> <li>For each new feature or experiment, create a new branch from <code>main</code>      using a clear naming convention: <code>feature/&lt;description&gt;</code> or      <code>experiment/&lt;description&gt;</code> (e.g., <code>feature/improve-chunking</code>,      <code>experiment/chunking-strategy-1</code>).</li> <li> <p>Implement the feature or experimental approach in the new branch.</p> </li> <li> <p>Developing in Feature/Experimentation Branches:</p> </li> <li>Conduct development or experimentation work within the created      branch.</li> <li>Use tools like Weights &amp; Biases (W&amp;B) Sweeps for hyperparameter      tuning and evaluation during experimentation.</li> <li> <p>Document all changes, observations, and results during the      development and experimentation process.</p> </li> <li> <p>Creating Pull Requests:</p> </li> <li>Once the feature or experiment is complete and stable, open a pull      request to merge the changes back into the <code>main</code> branch.</li> <li> <p>Ensure the pull request includes a detailed description of the      changes, including any relevant documentation and test results.</p> </li> <li> <p>Review and Testing:</p> </li> <li>The pull request undergoes code review by team members to ensure      code quality and adherence to project standards.</li> <li>Automated tests and CI/CD pipelines are triggered to validate the      changes and ensure they do not break existing functionality.</li> <li> <p>Address any feedback or issues raised during the review process.</p> </li> <li> <p>Merging Changes:</p> </li> <li>Once the pull request is approved and all tests pass, merge the      changes into the <code>main</code> branch.</li> <li> <p>Ensure that the <code>main</code> branch is always in a deployable state after      merging.</p> </li> <li> <p>Continuous Deployment:</p> </li> <li>Deploy the changes from the <code>main</code> branch to the production      environment.</li> <li>Ensure that deployment scripts and configurations are up-to-date      and tested.</li> </ol>"},{"location":"how-to-guides/branching-strategy/#benefits-of-github-flow","title":"Benefits of GitHub Flow","text":"<ul> <li>Simplicity: GitHub Flow is straightforward and easy to understand,   making it suitable for projects that require frequent updates and   iterations.</li> <li>Continuous Deployment: The strategy supports continuous   deployment, allowing for quick and frequent releases of new features   and improvements.</li> <li>Code Quality: Pull requests ensure that all changes are reviewed   and tested before merging, maintaining high code quality and   stability.</li> <li>Flexibility: The use of short-lived branches for features and   experiments provides flexibility and isolation, enabling parallel   development and experimentation.</li> </ul>"},{"location":"how-to-guides/branching-strategy/#references","title":"References","text":"<p>For more detailed explanations and examples of GitHub Flow, you can refer to the following resources:</p> <ol> <li>GitHub Flow -    Official guide by GitHub introducing the GitHub Flow strategy.</li> <li>Understanding GitHub    Flow</li> <li>GitHub documentation explaining the key concepts of GitHub Flow.</li> <li>A successful Git branching    model -    While primarily discussing Gitflow, this article provides useful    context on branching strategies.</li> <li>GitHub Flow vs. Gitflow: A    Comparison</li> <li>Atlassian's comparison of different Git workflows, including GitHub    Flow.</li> </ol>"},{"location":"how-to-guides/branching-strategy/#conclusion","title":"Conclusion","text":"<p>The GitHub Flow branching strategy provides a simple yet effective approach to developing and optimizing the RAG Q&amp;A system. By using feature and experimentation branches, pull requests, and continuous deployment, the strategy ensures systematic and efficient workflows while maintaining stability and facilitating collaboration. This approach is well-suited for projects that require frequent updates and iterative development, making it an ideal choice for the RAG Q&amp;A system on AWS.</p>"},{"location":"how-to-guides/branching-strategy/#comparison-table-git-feature-branch-workflow-gitflow-workflow-github-flow-and-gitlab-flow","title":"Comparison Table: Git Feature Branch Workflow, Gitflow Workflow, GitHub Flow, and GitLab Flow","text":"Aspect Git Feature Branch Workflow Gitflow Workflow GitHub Flow GitLab Flow Main Branch <code>main</code> (production-ready code) <code>main</code> (production-ready code) <code>main</code> (production-ready code) <code>main</code> (production-ready code) Development Branch <code>dev</code> (central development branch) <code>develop</code> (central development branch) No separate development branch No separate development branch Feature Branches Created from <code>dev</code> for each new feature Created from <code>develop</code> for each new feature Created from <code>main</code> for new features Created from <code>main</code> or environment branches Experimentation Branches Created from <code>dev</code> for experimental work Created from <code>develop</code> for experimental work Created from <code>main</code> for experiments Created from <code>main</code> or environment branches Release Branches Not explicitly defined Created from <code>develop</code> for preparing new releases Not explicitly defined Created from <code>main</code> for specific releases Hotfix Branches Not explicitly defined Created from <code>main</code> for critical fixes Not explicitly defined Created from <code>main</code> for critical fixes Merge Strategy PRs to merge successful experiments/features into <code>dev</code> PRs to merge successful features/experiments into <code>develop</code>, releases into <code>main</code> PRs to merge changes directly into <code>main</code> Merge requests to merge changes into <code>main</code> or relevant environment branches Environment Branches Not explicitly defined Not explicitly defined Not explicitly defined Branches for different environments like <code>production</code>, <code>staging</code> CI/CD Integration Generally integrated with PRs for testing and validation Generally integrated with PRs and release branches for testing and validation Continuous deployment from <code>main</code> Continuous deployment from <code>main</code> or environment branches Documentation of Work All experiments documented via PRs, whether merged or not All experiments and features documented via PRs, releases documented via release branches All changes documented via PRs All changes documented via merge requests Workflow Complexity Moderate High Simple High Ideal Use Case Medium to large projects with moderate feature and experimental branching needs Large projects with structured release cycles and multiple environments Small to medium projects with continuous deployment needs Large projects with structured releases and environment-specific deployments"},{"location":"how-to-guides/branching-strategy/#key-similarities","title":"Key Similarities","text":"<ul> <li>Main Branch: All four strategies use a <code>main</code> branch for   production-ready code.</li> <li>Feature Branches: All support creating feature branches for   isolated development of new features.</li> <li>Experimentation Branches: All support the creation of branches for   experimental work.</li> <li>Pull Requests/Merge Requests: All use PRs or merge requests to   integrate changes back into the main development line.</li> <li>CI/CD Integration: All generally integrate with CI/CD pipelines   for testing and validation of changes.</li> </ul>"},{"location":"how-to-guides/branching-strategy/#key-differences","title":"Key Differences","text":"<ul> <li>Development Branch: Git Feature Branch Workflow and Gitflow   Workflow have separate <code>dev</code> or <code>develop</code> branches, while GitHub Flow   and GitLab Flow do not.</li> <li>Release and Hotfix Branches: Explicitly defined in Gitflow   Workflow and GitLab Flow but not in Git Feature Branch Workflow and   GitHub Flow.</li> <li>Workflow Complexity: Gitflow and GitLab Flow are more complex with   additional branching for releases and environments, whereas GitHub   Flow is simpler.</li> <li>Environment Branches: Only GitLab Flow explicitly supports   branches for different environments like <code>production</code> and <code>staging</code>.</li> </ul>"},{"location":"how-to-guides/branching-strategy/#experiment-branches","title":"Experiment Branches","text":"<p>Use this branch type for working on and collaborating on experiments. Because you'll often build lots of models and features that don't work, you don't want to commit everything to your collaboration branch.</p>"},{"location":"how-to-guides/branching-strategy/#failed-experiments","title":"Failed Experiments","text":"<p>If an experiment doesn't pan out, mark the Experiment issue as failed with a label, close the PR without merging and update the Experiment issue with a TLDR explaining what you tried and how it turned out. Link to your experiment tracking tool so that others can view the runs.</p> <pre><code>---\ntitle: Unsaccessful Experiment Branching Pattern\n---\n\n%%{init: { 'logLevel': 'debug', 'theme': 'base' } }%%\ngitGraph TB:\n  commit tag: \"Initial commit\"\n  branch experiment\n  checkout experiment\n  commit tag: \"First Commit on experiment\"\n  commit tag: \"Iterate Experiment\"\n  commit tag: \"Failed Experiment\"</code></pre>"},{"location":"how-to-guides/branching-strategy/#successful-experiments","title":"Successful Experiments","text":"<p>If an experiment is successful and you want to start the deployment process:</p> <ol> <li>Update the experiment issue with a TLDR and mark as successful using    a label.</li> <li>Open a Model Issue to prepare the model and pipelines for production.</li> <li>Create a \"Model\" branch using Main as the base.</li> <li>Merge upstream changes from Main into your experiment branch and    resolve any conflicts.</li> <li>Change the target of the Experiment PR to the Model branch.</li> <li>Merge the pull request to your Model branch and close the Experiment    Issue.</li> <li>Delete the experiment branch after merging the PR.</li> </ol> <pre><code>---\ntitle: Successful Experiment Branching Pattern\n---\n%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true} } }%%\ngitGraph TB:\n    commit tag: \"Initial commit\"\n    branch experiment\n    checkout experiment\n    commit tag: \"Experiment changes\"\n    commit tag: \"Interate Experiment\"\n    commit tag: \"Successful approach found\"\n    checkout main\n    commit tag: \"Changes on master\"\n    merge experiment tag: \"Merge successful experiment into master\"\n    commit tag: \"Changes on master\"</code></pre>"},{"location":"how-to-guides/code-review-best-practices/","title":"Guide to Code Review in Machine Learning Projects","text":""},{"location":"how-to-guides/code-review-best-practices/#introduction","title":"Introduction","text":"<p>Code reviews are a crucial part of maintaining high-quality, reproducible, and collaborative codebases in machine learning (ML) projects. Despite the robustness of libraries such as pandas, scikit-learn, and TensorFlow, custom code written for data preprocessing, feature engineering, model evaluation, and utility functions must be rigorously tested and reviewed. This guide summarizes best practices for code reviews, emphasizing the importance of a third-party review for maintaining code integrity in a production environment.</p>"},{"location":"how-to-guides/code-review-best-practices/#prerequisites","title":"Prerequisites","text":"<p>Before diving into code reviews, ensure you have a solid understanding of version control, particularly how GitHub arranges its branches, forks, and pull requests within repositories. This knowledge is essential for effectively managing code contributions and reviews.</p>"},{"location":"how-to-guides/code-review-best-practices/#importance-and-personal-benefits","title":"Importance and Personal Benefits","text":""},{"location":"how-to-guides/code-review-best-practices/#quality-assurance","title":"Quality Assurance","text":"<p>Code reviews provide an additional layer of quality assurance beyond automated tests. They help identify and correct errors that might have been overlooked by the original author. According to McConnell (2004), unit testing finds approximately 25% of defects, function testing 35%, integration testing 45%, and code review 55-60%. While no single method is sufficient on its own, combining these approaches, including code reviews, is crucial for maintaining high-quality software.</p>"},{"location":"how-to-guides/code-review-best-practices/#documentation-and-readability","title":"Documentation and Readability","text":"<p>Code reviews also help ensure that documentation is clear and adequate. They provide a fresh perspective on whether the documentation is comprehensive enough for new users. Additionally, reviews improve code readability, making it easier for other developers to understand and build upon the code.</p>"},{"location":"how-to-guides/code-review-best-practices/#style-enforcement","title":"Style Enforcement","text":"<p>Code reviews ensure that all code adheres to the project's style guidelines, whether they are widely adopted standards like PEP8 or project-specific conventions. This consistency enhances the overall quality and maintainability of the codebase.</p>"},{"location":"how-to-guides/code-review-best-practices/#group-knowledge-and-cohesion","title":"Group Knowledge and Cohesion","text":"<p>Reviews facilitate knowledge sharing and collaboration among team members. They promote a collective understanding of the codebase and help newcomers integrate more effectively. Good reviews also foster a sense of community and cohesion within the team.</p>"},{"location":"how-to-guides/code-review-best-practices/#recommendations-and-best-practices","title":"Recommendations and Best Practices","text":""},{"location":"how-to-guides/code-review-best-practices/#who-reviews","title":"Who Reviews?","text":"<p>For small-scale projects, the coder can tag someone within the group as the reviewer. For larger projects, established rules for reviewer allocation help balance the workload and maximize the benefits of the review process. Ensure that reviewers have sufficient knowledge of the changes they are reviewing to provide valuable feedback.</p>"},{"location":"how-to-guides/code-review-best-practices/#be-nice-and-collaborative","title":"Be Nice and Collaborative","text":"<p>Always assume good faith and be constructive in your feedback. Code reviews should be a collaborative process where reviewers and authors work together to improve the code. Open and respectful communication is key to effective reviews.</p>"},{"location":"how-to-guides/code-review-best-practices/#objective-and-clear-feedback","title":"Objective and Clear Feedback","text":"<p>Strive to provide objective feedback based on documented project standards. Distinguish between crucial changes and optional suggestions. Use clear language to specify the importance of each comment, helping the author prioritize their responses.</p>"},{"location":"how-to-guides/code-review-best-practices/#review-in-small-chunks","title":"Review in Small Chunks","text":"<p>Reviewing small, incremental changes is more effective than tackling large, monolithic pull requests. This approach makes it easier to catch mistakes early and ensures a smoother review process.</p>"},{"location":"how-to-guides/code-review-best-practices/#typical-workflows","title":"Typical Workflows","text":""},{"location":"how-to-guides/code-review-best-practices/#formal-vs-informal-reviews","title":"Formal vs. Informal Reviews","text":"<p>Both formal and informal code reviews are valuable. Formal reviews often occur within structured environments like GitHub, where pull requests and review comments are tracked systematically. Informal reviews, such as over-the-shoulder peer reviews, can complement formal processes by catching issues early.</p>"},{"location":"how-to-guides/code-review-best-practices/#prepare-the-code","title":"Prepare the Code","text":"<p>Before requesting a review, ensure your code meets the project's quality benchmarks. This includes adhering to style guides, passing all tests, and providing adequate documentation.</p>"},{"location":"how-to-guides/code-review-best-practices/#propose-changes-and-create-a-review","title":"Propose Changes and Create a Review","text":"<p>In GitHub, start the review process by creating a pull request. This allows reviewers to provide general and line-by-line comments, facilitating detailed discussions about the proposed changes.</p>"},{"location":"how-to-guides/code-review-best-practices/#discuss-and-implement-feedback","title":"Discuss and Implement Feedback","text":"<p>Engage in constructive discussions to address reviewer comments. Once all feedback is addressed, the reviewer can approve the changes, and the pull request can be merged.</p>"},{"location":"how-to-guides/code-review-best-practices/#communicate-results-and-merge-changes","title":"Communicate Results and Merge Changes","text":"<p>Use GitHub's commenting features to provide detailed feedback. Once the review is complete, merge the changes according to the project's guidelines.</p>"},{"location":"how-to-guides/code-review-best-practices/#resources","title":"Resources","text":"<p>For further reading and a deeper understanding of code review processes, refer to the following resources:</p> <ul> <li>Atwood, Jeff (2006) Code Reviews: Just Do   It</li> <li>Burke, Kevin (2011) Why code review beats testing: evidence from   decades of programming   research</li> <li>McConnell, Steve (2004) Code Complete: A Practical Handbook of   Software Construction, Second   Edition</li> <li>The Turing Way:   Reviewing</li> <li>The Turing Way: Reviewing   Motivation</li> <li>The Turing Way: Reviewing   Recommendations</li> <li>The Turing Way: Reviewing   Workflow</li> <li>The Turing Way: Reviewing   Checklist</li> <li>The Turing Way: Reviewing   Resources</li> </ul>"},{"location":"how-to-guides/code-review-best-practices/#conclusion","title":"Conclusion","text":"<p>Implementing a structured and thorough code review process is essential for the success of any machine learning project. By following the best practices outlined in this guide, you can ensure high-quality, reproducible, and collaborative code development. Remember, effective code reviews not only improve the codebase but also enhance team collaboration and knowledge sharing.</p>"},{"location":"how-to-guides/cofig-files/","title":"Using Configuration Files to Avoid Hardcoding Values","text":"<p>ChatGPT Prompt for creating config files</p> <pre><code>You are a world-class Python developer with an eagle eye for detail and\na deep understanding of best practices in detecting and refactoring\nhardcoded values in Python code. I have a Python script, and I want to\ndetect and refactor the following issues based on these recommendations:\n\n1. Parameters and Defaults: Identify function parameter defaults\n   related to configuration settings such as chunk sizes, timeouts, and\n   AWS session durations. These should be externalized to a\n   configuration file or environment variables.\n2. File Extensions and Paths: Detect hardcoded paths and file\n   extensions used frequently or subject to change. These should be\n   placed in configuration files to avoid having to search through code\n   to make changes and to ensure consistency.\n3. Environment Variables: Identify hardcoded environment variable\n   names and their default values. These should be defined in\n   configuration files or environment variables to manage different\n   environments (development, testing, production) more effectively.\n4. Model and Service Configurations: Look for hardcoded model IDs,\n   AWS service regions, and similar configurations. These should be\n   externalized for easier deployments across different regions or when\n   switching between different models.\n5. Logger Messages and Formats: Ensure logger formats are defined in\n   a centralized logging configuration. Detect any repetitive or\n   template-based log messages that can benefit from configuration or\n   templating approaches.\n\nPlease produce two files: a configuration file using YAML language and\nthe refactored version of the Python code.\n\nHere is the script:\n\n```python\n[Insert your Python script here]\n</code></pre> <ul> <li>Using Configuration Files to Avoid Hardcoding Values</li> <li>Overview</li> <li>Benefits of Using Configuration Files</li> <li>Directory Structure for Configuration<ul> <li>Example Directory Structure</li> </ul> </li> <li>What Should Be Included in Configuration Files<ul> <li>Example Content of a Configuration File</li> <li>module1.yaml</li> </ul> </li> <li>What Should Be Included in the <code>.env</code> File<ul> <li>Example Content of the <code>.env</code> File</li> </ul> </li> <li>Loading Configuration in Code<ul> <li>Loading YAML Configuration Files</li> <li>Loading Environment Variables</li> </ul> </li> <li>Conclusion</li> <li>Best Practices for Refactoring Hardcoded Values in Python Code<ul> <li>Introduction</li> <li>Best Practices</li> </ul> </li> </ul>"},{"location":"how-to-guides/cofig-files/#example","title":"Example","text":""},{"location":"how-to-guides/cofig-files/#original-python-script","title":"Original Python Script","text":"<pre><code>import logging\nimport os\n\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n\ndef process_data(file_path='/data/input.txt', chunk_size=1024):\n    with open(file_path, 'r') as file:\n        while chunk := file.read(chunk_size):\n            process_chunk(chunk)\n\ndef process_chunk(chunk):\n    logging.info(f'Processing chunk of size {len(chunk)}')\n    # Processing logic here\n\nif __name__ == \"__main__\":\n    process_data()\n</code></pre>"},{"location":"how-to-guides/cofig-files/#using-the-prompt","title":"Using the Prompt","text":"<p>To use the prompt, copy the original Python script into the <code>[Insert your Python script here]</code> section and provide it to ChatGPT. The output will include two files: a configuration file in YAML format and the refactored Python script.</p>"},{"location":"how-to-guides/cofig-files/#expected-configuration-file-configyaml","title":"Expected Configuration File (config.yaml)","text":"<pre><code>logging:\n  level: INFO\n  format: '%(asctime)s - %(message)s'\n\ndefaults:\n  file_path: '/data/input.txt'\n  chunk_size: 1024\n</code></pre>"},{"location":"how-to-guides/cofig-files/#expected-refactored-python-script","title":"Expected Refactored Python Script","text":"<pre><code>import logging\nimport os\nimport yaml\n\nwith open('config.yaml', 'r') as file:\n    config = yaml.safe_load(file)\n\nlogging.basicConfig(level=getattr(logging, config['logging']['level']), format=config['logging']['format'])\n\ndef process_data(file_path=config['defaults']['file_path'], chunk_size=config['defaults']['chunk_size']):\n    with open(file_path, 'r') as file:\n        while chunk := file.read(chunk_size):\n            process_chunk(chunk)\n\ndef process_chunk(chunk):\n    logging.info(f'Processing chunk of size {len(chunk)}')\n    # Processing logic here\n\nif __name__ == \"__main__\":\n    process_data()\n</code></pre>"},{"location":"how-to-guides/cofig-files/#overview","title":"Overview","text":"<p>This document provides guidelines on how to use configuration files to avoid hardcoding values in your codebase. It serves as an introduction to best practices for creating and managing YAML configuration files in your project.</p> <p>Note</p> <p>This introduction explains what should be included in configuration files and what should be stored in the <code>.env</code> file.</p>"},{"location":"how-to-guides/cofig-files/#benefits-of-using-configuration-files","title":"Benefits of Using Configuration Files","text":"<ul> <li>Maintainability: Centralizes configuration settings, making it   easier to manage and update without modifying the code.</li> <li>Separation of Concerns: Differentiates between configuration   settings and application logic.</li> <li>Security: Keeps sensitive information, such as API keys and   passwords, separate from non-sensitive configuration settings.</li> </ul>"},{"location":"how-to-guides/cofig-files/#directory-structure-for-configuration","title":"Directory Structure for Configuration","text":"<p>Info</p> <p>Place all non-sensitive configuration settings in a dedicated <code>config</code> directory and use the <code>.env</code> file for storing secrets and sensitive information.</p>"},{"location":"how-to-guides/cofig-files/#example-directory-structure","title":"Example Directory Structure","text":"<pre><code>src/\n\u251c\u2500\u2500 __pycache__/\n\u251c\u2500\u2500 module1/\n\u251c\u2500\u2500 module2/\n\u251c\u2500\u2500 module3/\n\u251c\u2500\u2500 main.py\nconfig/\n\u251c\u2500\u2500 module1.yaml\n\u251c\u2500\u2500 config.yaml\n\u251c\u2500\u2500 module2.yaml\n\u251c\u2500\u2500 module3.yaml\n\u2514\u2500\u2500 logging.yaml\n.env\n</code></pre>"},{"location":"how-to-guides/cofig-files/#what-should-be-included-in-configuration-files","title":"What Should Be Included in Configuration Files","text":"<p>Configuration files should include non-sensitive settings that are required for the application to function. These can be categorized by module and stored in individual YAML files.</p>"},{"location":"how-to-guides/cofig-files/#example-content-of-a-configuration-file","title":"Example Content of a Configuration File","text":""},{"location":"how-to-guides/cofig-files/#module1yaml","title":"module1.yaml","text":"<pre><code># Configuration for module1\n\nparameter1: value1\nparameter2: value2\n\nlogging:\n  level: INFO\n  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n</code></pre>"},{"location":"how-to-guides/cofig-files/#what-should-be-included-in-the-env-file","title":"What Should Be Included in the <code>.env</code> File","text":"<p>The <code>.env</code> file should include sensitive information, such as API keys, secrets, and credentials that should not be exposed in the codebase or configuration files.</p>"},{"location":"how-to-guides/cofig-files/#example-content-of-the-env-file","title":"Example Content of the <code>.env</code> File","text":"<pre><code># Store of global environment variables. \n\nAPI_KEY=\"your_api_key_here\" \nDATABASE_URL=\"your_database_url_here\" \nSECRET_KEY=\"your_secret_key_here\" \n</code></pre> <p>Summary:</p> <ol> <li>Configuration Files (YAML): Store non-sensitive configuration   settings related to application logic and modules.</li> <li>.env File: Store sensitive information and secrets that should   not be exposed in the configuration files or codebase.</li> </ol>"},{"location":"how-to-guides/cofig-files/#loading-configuration-in-code","title":"Loading Configuration in Code","text":"<p>To load the configuration files and environment variables in your code, use appropriate libraries and functions.</p>"},{"location":"how-to-guides/cofig-files/#loading-yaml-configuration-files","title":"Loading YAML Configuration Files","text":"<pre><code>import yaml\n\ndef load_config(config_path: str) -&gt; dict:\n    \"\"\"Loads a YAML configuration file.\n\n    Args:\n        config_path (str): The path to the YAML configuration file.\n\n    Returns:\n        dict: A dictionary containing the configuration data \n              loaded from the YAML file.\n    \"\"\"\n    with open(config_path, \"r\") as file:\n        config = yaml.safe_load(file)\n    return config\n\n# Example usage\nmodule1_config = load_config('config/module1.yaml')\n</code></pre>"},{"location":"how-to-guides/cofig-files/#loading-environment-variables","title":"Loading Environment Variables","text":"<p>Use a library such as <code>python-dotenv</code> to load environment variables from the <code>.env</code> file:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Access environment variables\napi_key = os.getenv(\"API_KEY\")\ndatabase_url = os.getenv(\"DATABASE_URL\")\n</code></pre>"},{"location":"how-to-guides/cofig-files/#conclusion","title":"Conclusion","text":"<p>By following these guidelines, you can ensure that your project\u2019s configuration files are well-organized, maintainable, and secure. This approach enhances the readability and manageability of your codebase, making it easier to implement and modify configurations as your project evolves. Keep sensitive information in the <code>.env</code> file and other configuration settings in the respective YAML files within the <code>config</code> directory. For detailed best practices, refer to the main technical documentation on best practices for configuration files using YAML.</p>"},{"location":"how-to-guides/cofig-files/#best-practices-for-refactoring-hardcoded-values-in-python-code","title":"Best Practices for Refactoring Hardcoded Values in Python Code","text":""},{"location":"how-to-guides/cofig-files/#introduction","title":"Introduction","text":"<p>Refactoring code to externalize hardcoded values is a crucial step in improving the maintainability, flexibility, and scalability of software. This document explains the rationale behind five best practices for refactoring hardcoded values in Python code, provides a prompt for automating the detection and refactoring process, and demonstrates a basic example of how to use the prompt with a simple Python script.</p>"},{"location":"how-to-guides/cofig-files/#best-practices","title":"Best Practices","text":"<ol> <li>Parameters and Defaults:</li> <li>Reason: Hardcoding function parameter defaults, especially      those related to configuration settings such as chunk sizes,      timeouts, and AWS session durations, limits flexibility.      Externalizing these parameters to configuration files or      environment variables allows for easier modifications and better      adaptability to different environments.</li> <li> <p>Example: Instead of setting a timeout of 30 seconds directly in      the function definition, it should be read from a configuration      file or environment variable.</p> </li> <li> <p>File Extensions and Paths:</p> </li> <li>Reason: Hardcoded paths and file extensions make the code less      flexible and more difficult to maintain. Placing these values in      configuration files ensures consistency and makes updates easier      without searching through the codebase.</li> <li> <p>Example: Hardcoded paths like <code>/data/files</code> should be stored in      a configuration file to facilitate changes across environments      (development, testing, production).</p> </li> <li> <p>Environment Variables:</p> </li> <li>Reason: Hardcoding environment variable names and their default      values makes it challenging to manage different environments      effectively. Defining these values in configuration files or      environment variables enhances flexibility and maintainability.</li> <li> <p>Example: Instead of using <code>os.getenv('DB_HOST', 'localhost')</code>      directly in the code, the default value <code>localhost</code> should be      defined in a configuration file.</p> </li> <li> <p>Model and Service Configurations:</p> </li> <li>Reason: Hardcoding model IDs, AWS service regions, and similar      configurations hampers the ability to deploy across different      regions or switch between models. Externalizing these      configurations simplifies deployments and model management.</li> <li> <p>Example: Instead of specifying <code>region='us-west-2'</code> directly in      the code, the region should be read from a configuration file.</p> </li> <li> <p>Logger Messages and Formats:</p> </li> <li>Reason: Centralizing logger formats and repetitive log messages      in a configuration or templating approach improves consistency and      maintainability. It also allows for easy updates to logging formats      across the application.</li> <li>Example: Logger formats like <code>%(asctime)s - %(name)s -      %(levelname)s - %(message)s</code> should be defined in a centralized      logging configuration file.</li> </ol>"},{"location":"how-to-guides/column-naming-conventions/","title":"Column Naming Conventions for ML/AI Projects","text":""},{"location":"how-to-guides/column-naming-conventions/#overview","title":"Overview","text":"<p>To maintain consistency and readability in our project, we use the \"snake_case\" naming convention for all column names in CSV, JSON, and other data files. This guide provides a set of rules and examples to help you name columns correctly, applicable to all types of data including structured, semi-structured, and unstructured data.</p>"},{"location":"how-to-guides/column-naming-conventions/#what-is-snake_case","title":"What is Snake_Case?","text":"<p>Snake_case is a naming convention where each word in a column name is lowercase and separated by underscores (<code>_</code>). This format is widely used in the Python ecosystem and ensures that column names are easily readable and consistent.</p>"},{"location":"how-to-guides/column-naming-conventions/#rules-for-snake_case-column-naming","title":"Rules for Snake_Case Column Naming","text":"<ol> <li>Use Lowercase Letters: All letters should be in lowercase.</li> <li>Separate Words with Underscores: Use underscores (<code>_</code>) to    separate words.</li> <li>Be Descriptive: Column names should clearly indicate the contents    or purpose of the column.</li> <li>Avoid Special Characters and Spaces: Do not use spaces, hyphens,    or any special characters other than underscores.</li> </ol>"},{"location":"how-to-guides/column-naming-conventions/#examples","title":"Examples","text":""},{"location":"how-to-guides/column-naming-conventions/#structured-data-csv-file","title":"Structured Data (CSV File)","text":"<pre><code>user_id, age, gender, purchase_amount, purchase_date\n1, 34, \"M\", 45.67, \"2023-05-12\"\n2, 29, \"F\", 78.90, \"2023-05-13\"\n</code></pre>"},{"location":"how-to-guides/column-naming-conventions/#semi-structured-data-json-file","title":"Semi-Structured Data (JSON File)","text":"<pre><code>[\n  {\n    \"user_id\": 1,\n    \"age\": 34,\n    \"gender\": \"M\",\n    \"purchase_amount\": 45.67,\n    \"purchase_date\": \"2023-05-12\"\n  },\n  {\n    \"user_id\": 2,\n    \"age\": 29,\n    \"gender\": \"F\",\n    \"purchase_amount\": 78.90,\n    \"purchase_date\": \"2023-05-13\"\n  }\n]\n</code></pre>"},{"location":"how-to-guides/column-naming-conventions/#unstructured-data-text-with-metadata","title":"Unstructured Data (Text with Metadata)","text":"<pre><code>{\n  \"documents\": [\n    {\n      \"doc_id\": \"12345\",\n      \"title\": \"Analysis of Sales Data\",\n      \"author\": \"John Doe\",\n      \"content\": \"The sales data for Q1 shows a significant increase in revenue.\",\n      \"timestamp\": \"2023-06-01T12:34:56Z\"\n    },\n    {\n      \"doc_id\": \"12346\",\n      \"title\": \"Customer Feedback Summary\",\n      \"author\": \"Jane Smith\",\n      \"content\": \"Customer feedback for Q1 has been overwhelmingly positive.\",\n      \"timestamp\": \"2023-06-02T14:22:33Z\"\n    }\n  ]\n}\n</code></pre>"},{"location":"how-to-guides/column-naming-conventions/#common-column-names-in-mlai-projects","title":"Common Column Names in ML/AI Projects","text":"<ul> <li><code>user_id</code></li> <li><code>age</code></li> <li><code>gender</code></li> <li><code>purchase_amount</code></li> <li><code>purchase_date</code></li> <li><code>doc_id</code></li> <li><code>title</code></li> <li><code>author</code></li> <li><code>content</code></li> <li><code>timestamp</code></li> </ul>"},{"location":"how-to-guides/column-naming-conventions/#tips","title":"Tips","text":"<ul> <li>Keep it Short and Simple: Aim for clarity but avoid overly long   column names.</li> <li>Use Consistent Terminology: Use the same terms consistently across   different columns (e.g., <code>timestamp</code> vs. <code>date_time</code>).</li> <li>Context-Specific Names: Tailor column names to the specific   context and requirements of your project.</li> </ul>"},{"location":"how-to-guides/column-naming-conventions/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ul> <li>Uppercase Letters: <code>UserID</code> (Incorrect)</li> <li>Spaces: <code>user id</code> (Incorrect)</li> <li>Hyphens: <code>user-id</code> (Incorrect)</li> <li>Special Characters: <code>user@id!</code> (Incorrect)</li> </ul> <p>By following these guidelines, we ensure that our column names are consistent, readable, and easy to work with in data processing and analysis tasks, regardless of the type of data.</p>"},{"location":"how-to-guides/commit-message-standards-ml/","title":"Git Commit Message Standards for ML Projects","text":"<p>ChatGPT Prompt for Git Commit Message</p> <p>To create a standardized git commit message using ChatGPT, provide the following details: the output of <code>git diff</code> (1) command (including one or more modified files), the JIRA story number, and optionally, any additional context. ChatGPT will generate the appropriate commit message based on these inputs.</p> <ol> <li> To ensure you get enough information from <code>git diff</code>  to create a rich git commit message with ChatGPT, use the following  command: <code>git diff -U10 my-script.py</code>.</li> </ol> <p>Include this admonition in your <code>docs/how-to-guides/git-commit-message-standards.md</code> file. This example explains how to request a standardized git commit message from ChatGPT, providing a detailed prompt and the expected output.</p> <p>Introduction</p> <p>Clear, informative commit messages are vital for effective team collaboration in machine learning projects. This guide provides standards and tools for creating standardized commit messages, including how to reference GitHub issue numbers or Jira keys. We follow the Conventional Commits specification.</p> <p>Commit Message Structure</p> <p>Commit messages should follow this structure:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; [#issue_number | #jira_key]\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre> <p>Commit Types</p> <p>Maintain consistency and organization with these commit types:</p> Type Description <code>feat</code> New features or enhancements <code>fix</code> Bug fixes <code>data</code> Data processing or management changes <code>experiment</code> Experimental or exploratory code changes <code>model</code> Model development, testing, or deployment changes <code>docs</code> Documentation additions or updates <code>refactor</code> Performance enhancements without functionality changes <code>test</code> Test writing or fixing <code>chore</code> Routine tasks or non-production code updates <code>build</code> Changes that affect the build system or external dependencies <code>ci</code> Changes to CI configuration files and scripts <code>revert</code> Reverts a previous commit <code>style</code> Code style changes (formatting, missing semicolons, etc.) <p>Best Practices</p> <ol> <li>Concise Subject: Keep it short and descriptive.</li> <li>Imperative Mood: Phrase as an instruction, e.g., \u201cfix\u201d not \u201cfixed\u201d.</li> <li>Capitalized Subject Line: Start with a capital letter.</li> <li>No Period in Subject Line: Treat it as a title.</li> <li>Separate Body with a Blank Line: For tool compatibility.</li> <li>Explain What and Why: Not just how, in the body.</li> <li>Reference Issues and Pull Requests: Include relevant links for context.</li> </ol>"},{"location":"how-to-guides/commit-message-standards-ml/#example-prompt","title":"Example Prompt","text":"<p>Copy and paste the example prompt in ChatGPT or similar chat:</p> <pre><code>Generate git commands for staging and committing the following changes. Follow\nthe structure:\n\n&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; [#issue_number | #jira_key]\n\nInclude all modified files in the `git diff` command. Provide the\ncomplete `git add` and `git commit` commands, including the body and \nfooter. Select the right git commit types with emojis. Limit \ncomments to 72 characters per line and code lines to 79 characters. \n\nOutput of `git diff` command:\n\n[Please insert the output of the git diff here]\n\nJIRA story number:\n\n[Please enter the JIRA story number here]\n\nAdditional Context (Optional):\n-----------------------------\n\n[Please insert additional context]\n\nCommit types:\n-------------\nHere\u2019s the list of git commit types with emojis included:\n\n### Commit Types:\n\n- \ud83c\udfa8 Improve structure/format of the code\n- \u26a1\ufe0f Improve performance\n- \ud83d\udd25 Remove code or files\n- \ud83d\udc1b Fix a bug\n- \ud83d\ude91\ufe0f Critical hotfix\n- \u2728 Introduce new features\n- \ud83d\udcdd Add or update documentation\n- \ud83d\ude80 Deploy stuff\n- \ud83d\udc84 Add or update the UI and style files\n- \ud83c\udf89 Begin a project\n- \u2705 Add, update, or pass tests\n- \ud83d\udd12 Fix security or privacy issues\n- \ud83d\udd10 Add or update secrets\n- \ud83d\udd16 Release/Version tags\n- \ud83d\udea8 Fix compiler/linter warnings\n- \ud83d\udea7 Work in progress\n- \ud83d\udc9a Fix CI build\n- \u2b07\ufe0f Downgrade dependencies\n- \u2b06\ufe0f Upgrade dependencies\n- \ud83d\udccc Pin dependencies to specific versions\n- \ud83d\udc77 Add or update CI build system\n- \ud83d\udcc8 Add or update analytics or track code\n- \u267b\ufe0f Refactor code\n- \u2795 Add a dependency\n- \ud83d\udd27 Add or update configuration files\n- \ud83d\udd28 Add or update development scripts\n- \ud83c\udf0d Internationalization and localization\n- \u270f\ufe0f Fix typos\n- \ud83d\udca9 Write bad code that needs to be improved\n- \u23ea\ufe0f Revert changes\n- \ud83d\udd00 Merge branches\n- \ud83d\udce6\ufe0f Add or update compiled files or packages\n- \ud83d\udc7d Update code due to external API changes\n- \ud83d\ude9a Move or rename resources (e.g., files, paths, routes)\n- \ud83d\udcc4 Add or update license\n- \ud83d\udca5 Introduce breaking changes\n- \ud83c\udf71 Add or update assets\n- \u267f\ufe0f Improve accessibility\n- \ud83d\udcac Add or update comments in source code\n- \ud83d\uddef\ufe0f Write code drunkenly\n- \ud83e\uddea Add or update text and literals\n- \ud83d\uddc3\ufe0f Perform database-related changes\n- \ud83d\udcdd Add or update logs\n- \ud83d\udd07 Remove logs\n- \ud83d\udc64 Add or update contributor(s)\n- \ud83d\udee0\ufe0f Improve user experience/usability\n- \ud83c\udfd7\ufe0f Make architectural changes\n- \ud83d\udcf1 Work on responsive design\n- \ud83d\udd0d Mock things\n- \ud83e\udd5a Add or update an easter egg\n- \u2795 Add or update a .gitignore file\n- \ud83e\uddea Add or update snapshots\n- \ud83d\udd2c Perform experiments\n- \ud83c\udf10 Improve SEO\n- \ud83d\uded1 Add or update types\n- \ud83c\udf31 Add or update seed files\n- \ud83d\udea9 Add, update, or remove feature flags\n- \u2757 Catch errors\n- \ud83d\udcf2 Add or update animations and transitions\n- \ud83d\uddd1\ufe0f Deprecate code that needs to be cleaned up\n- \ud83d\udd11 Work on code related to authorization, roles, and permissions\n- \ud83d\udc55 Simple fix for a non-critical issue\n- \ud83d\udd0d Data exploration/inspection\n- \ud83e\uddf9 Remove dead code\n- \ud83d\udea8 Add a failing test\n- \ud83e\udde0 Add or update business logic\n- \ud83e\ude7a Add or update health checks\n- \ud83c\udfd7\ufe0f Infrastructure-related changes\n- \ud83e\udd1d Improve developer experience\n- \ud83d\udcb5 Add sponsorships or money-related infrastructure\n- \u2696\ufe0f Add or update code related to multithreading or concurrency\n- \ud83d\udee1\ufe0f Add or update code related to validation\n\nExample of Expected Output:\n---------------------------\n\ngit add mkdocs.yml poetry.lock pyproject.toml\ngit commit -m \"feat(deps): add mkdocs versioning and testing dependencies [#JIRA-123]\n\nAdded dependencies for MkDocs versioning and testing to enhance\ndocumentation and development workflows. Updated pyproject.toml\nwith the following:\n- mkdocs-glightbox: to support image lightboxes in documentation\n- mike: to manage multiple versions of documentation\n\nUpdated poetry.lock with new packages:\n- importlib-resources: for resource management in tests\n- mike: for documentation version management\n- mkdocs-glightbox: for enhanced documentation visuals\n- verspec: for flexible version handling\n\nThese changes improve documentation flexibility and testing coverage.\n\nFooter notes:\n- Consult package documentation for more details.\n\"\n</code></pre>"},{"location":"how-to-guides/commit-message-standards-ml/#generated-commit-message","title":"Generated Commit Message","text":"<p>ChatGPT might respond with:</p> <pre><code>git add utils/helpers.py\ngit commit -m \"\ud83c\udfa8 refactor(helpers): update pi precision and greeting format [#JIRA-123]\n\nUpdated the `calculate_area` function to use a more precise value\nof pi (3.14159) for improved accuracy in area calculations. Also,\nrefined the `greeting` function to include a comma for better\nreadability in the output.\n\nThese changes enhance both the mathematical accuracy of area\ncalculations and the user experience with improved greeting\nformatting.\n\nFooter notes:\n- Consider further enhancements to include localization\n  for the greeting function.\n\"\n</code></pre>"},{"location":"how-to-guides/commit-message-standards-ml/#running-the-git-command","title":"Running the Git Command","text":"<p>To commit your changes with the generated message, run the following command in your terminal:</p> <pre><code>git add utils/helpers.py\ngit commit -m \"\ud83c\udfa8 refactor(helpers): update pi precision and greeting format [#JIRA-123]\n\nUpdated the `calculate_area` function to use a more precise value\nof pi (3.14159) for improved accuracy in area calculations. Also,\nrefined the `greeting` function to include a comma for better\nreadability in the output.\n\nThese changes enhance both the mathematical accuracy of area\ncalculations and the user experience with improved greeting\nformatting.\n\nFooter notes:\n- Consider further enhancements to include localization\n  for the greeting function.\n\"\n</code></pre>"},{"location":"how-to-guides/commit-message-standards-ml/#components","title":"Components","text":"<ul> <li>Type: Category of your commit.</li> <li>Scope: Area of the codebase affected.</li> <li>Subject: Brief description of changes, including issue tracker reference.</li> <li>Body: Detailed explanation of changes and reasoning.</li> <li>Footer: Additional notes or references.</li> </ul>"},{"location":"how-to-guides/commit-message-standards-ml/#examples","title":"Examples","text":"<p>Here are some examples of commit messages that meet our standards:</p> <pre><code>feat(auth): add user authentication feature [#DATA123]\n\nAdded a new user authentication feature to enhance security.\nThis includes login, registration, and password recovery.\n\nFooter: Reviewed by Jane Doe\n</code></pre> <pre><code>fix(data): resolve data loading error [#GH45]\n\nFixed an issue where data loading was failing due to incorrect file paths.\nUpdated the file paths and added error handling.\n\nFooter: Closes #GH45\n</code></pre> <pre><code>docs: update API documentation\n\nImproved the API documentation to include new endpoints and usage examples.\nFixed typos and clarified descriptions.\n\nFooter: Documentation reviewed by the tech writing team\n</code></pre> <p>VS Code Extensions</p> <p>Enhance commit message standardization with these VS Code extensions: - Gitmoji for VS Code: Add emoji to commit messages. Gitmoji. - Commit Message Editor: Enforce structured commit messages.</p> <p>Reference for Best Practices</p> <p>For an in-depth understanding, refer to Git Commit Message Best Practices.</p> <p>Conclusion</p> <p>By following these guidelines, our team can ensure a uniform and informative history in our repository, turning commit messages into a rich log that enhances clarity and streamlines collaboration.</p>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/","title":"Data Conversion Workflow for NLP Projects","text":""},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#overview","title":"Overview","text":"<p>This document outlines the process and provides the necessary Python functions for converting data files between XLSX and CSV formats within a Natural Language Processing (NLP) project. The workflow is designed to facilitate seamless data sharing and manipulation among non-technical stakeholders and machine learning (ML) experts, ensuring uniform encoding and formatting across different operating systems.</p>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#procedure","title":"Procedure","text":""},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#context","title":"Context","text":"<p>Non-technical stakeholders often manually create and share data in XLSX format using Windows OS, without employing programming tools like Python. ML experts, on the other hand, utilize containers running Linux to manipulate this data with Python and incorporate it into the project's codebase. To ensure compatibility and uniformity in data handling, two Python functions have been developed for converting data files between XLSX and CSV formats.</p> <p>Note</p> <p>This Guide works even if the developer uses Mac or Windows without Docker.</p>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#conversion-from-xlsx-to-csv","title":"Conversion from XLSX to CSV","text":"<p>When ML experts receive an XLSX file from business stakeholders, the following Python function should be executed in a Linux-based container to convert the file into a CSV format. This conversion ensures the data is encoded in UTF-8 and retains consistent formatting for use in the project's NLP codebase.</p>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#function-convert_xlsx_to_csv","title":"Function: <code>convert_xlsx_to_csv</code>","text":"<pre><code>def convert_xlsx_to_csv(xlsx_path, csv_path, sheet_name=0):\n    \"\"\"\n    Converts an XLSX file to a CSV file with UTF-8 encoding.\n\n    This function takes the path to an XLSX file and a destination path\n    for the CSV file as arguments. It optionally accepts the name or\n    index of the sheet to convert. It uses `pandas` to read the\n    specified sheet, ensuring all data is treated as strings\n    (`dtype=str`) to preserve formatting. It then fills any missing\n    values with empty strings to avoid the string 'nan' appearing in\n    your CSV. Finally, it writes the data to a CSV file, specifying\n    UTF-8 encoding and using `\\n` as the line terminator to ensure\n    compatibility across different operating systems.\n\n    Parameters: - xlsx_path: Path to the source XLSX file.  - csv_path:\n    Path where the output CSV file should be saved.  - sheet_name: Name\n    or index of the sheet in the XLSX file to convert.\n                  Defaults to the first sheet.\n    \"\"\"\n    try:\n        # Load the specified sheet from the XLSX file\n        data = pd.read_excel(xlsx_path, sheet_name=sheet_name, dtype=str)\n\n        # Convert any NaN values to empty strings to avoid 'nan' in the CSV\n        data.fillna(\"\", inplace=True)\n\n        # Save the DataFrame to a CSV file with UTF-8 encoding\n        data.to_csv(\n            csv_path, encoding=\"utf-8\", index=False, lineterminator=\"\\n\"\n        )\n\n        print(\n            f\"Successfully converted {xlsx_path} to {csv_path} in UTF-8 encoding.\"\n        )\n    except Exception as e:\n        print(f\"Error converting {xlsx_path} to CSV: {e}\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>xlsx_path</code>: The file path to the source XLSX file.</li> <li><code>csv_path</code>: The file path where the output CSV file will be saved.</li> <li><code>sheet_name</code>: (Optional) The name or index of the sheet in the XLSX   file to be converted.</li> </ul> <p>Usage:</p> <pre><code>convert_xlsx_to_csv('path/to/input.xlsx', 'path/to/output.csv')\n</code></pre>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#conversion-from-csv-to-xlsx","title":"Conversion from CSV to XLSX","text":"<p>For sharing processed data back with non-technical stakeholders, ML experts can convert the CSV files (previously created or manipulated within the Linux containers) back into XLSX format. This conversion facilitates easy access and interpretation of the results by business users.</p>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#function-convert_csv_to_xlsx","title":"Function: <code>convert_csv_to_xlsx</code>","text":"<pre><code>import pandas as pd\n\n\ndef convert_csv_to_xlsx(csv_path, xlsx_path):\n    \"\"\"\n    Converts a CSV file to an XLSX file.\n\n    This function takes the path to a CSV file and the desired output\n    path for the XLSX file. It uses `pandas` to read the CSV file,\n    assuming it is encoded in UTF-8 (as ensured by the CSV creation\n    function). Then, it writes the data to an XLSX file using the\n    `openpyxl` engine, which is required for writing to XLSX format and\n    needs to be installed alongside `pandas` if it's not already present\n    in your environment.\n\n    Parameters:\n    - csv_path: Path to the source CSV file.\n    - xlsx_path: Path where the output XLSX file should be saved.\n    \"\"\"\n    try:\n        # Load the CSV file, assuming UTF-8 encoding\n        data = pd.read_csv(csv_path, encoding=\"utf-8\")\n\n        # Save the DataFrame to an XLSX file\n        with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n            data.to_excel(writer, index=False)\n\n        print(f\"Successfully converted {csv_path} to {xlsx_path}.\")\n    except Exception as e:\n        print(f\"Error converting {csv_path} to XLSX: {e}\")\n</code></pre> <p>Parameters:</p> <ul> <li><code>csv_path</code>: The file path to the source CSV file.</li> <li><code>xlsx_path</code>: The file path where the output XLSX file will be saved.</li> </ul> <p>Usage:</p> <pre><code>convert_csv_to_xlsx('path/to/input.csv', 'path/to/output.xlsx')\n</code></pre>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#requirements","title":"Requirements","text":"<ul> <li>Python 3.x</li> <li>Pandas library</li> <li>Openpyxl library (for XLSX file writing)</li> </ul>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#installation","title":"Installation","text":"<p>Ensure the required libraries are installed in your Linux container:</p> <pre><code>pip install pandas openpyxl\n</code></pre>"},{"location":"how-to-guides/data-conversion-from-xlsx-to-csv/#conclusion","title":"Conclusion","text":"<p>This procedure guide facilitates a smooth data exchange and processing workflow for NLP projects, bridging the gap between non-technical stakeholders and ML experts. By following the documented steps and utilizing the provided Python functions, teams can ensure data consistency, compatibility, and efficiency throughout the project lifecycle.</p>"},{"location":"how-to-guides/data_management_practices/","title":"How to organize project's data","text":"<p>Warning</p> <p>It is not a best practice to save sensitive or large project data on GitHub. GitHub is designed for source code management and version control, and while it does allow for limited file storage, it's not designed for storing large binary files or sensitive information.</p>"},{"location":"how-to-guides/data_management_practices/#data-files","title":"Data files","text":"<p>Best Practice</p> <p>Store project's data on the remote centralized data repository like AWS S3.</p> <p>Creating separate folders for raw, external, interim, and processed data is the best practice for organizing and managing data in a project. This approach helps to maintain a clear and organized structure for the data, and can also help to ensure that the different stages of data processing are clearly defined and documented.</p>"},{"location":"how-to-guides/data_management_practices/#folder-structure-and-naming-convention","title":"Folder Structure and Naming Convention","text":"<p>The data folder structure should be organized into distinct categories, each serving a specific purpose in the data processing pipeline. Here\u2019s the recommended structure:</p> <pre><code>data\n\u251c\u2500\u2500 raw                   # Original, immutable data dump\n\u251c\u2500\u2500 external              # Data from third-party sources\n\u251c\u2500\u2500 interim               # Intermediate data, partially processed\n\u251c\u2500\u2500 processed             # Fully processed data, ready for analysis\n\u2514\u2500\u2500 features              # Engineered features ready for model training\n</code></pre>"},{"location":"how-to-guides/data_management_practices/#explanation-of-categories","title":"Explanation of Categories","text":"<ul> <li>Raw: </li> <li> <p>Contains the original datasets. Data in this folder is immutable      and serves as a backup for all processing steps.</p> </li> <li> <p>External: </p> </li> <li> <p>For data sourced from outside the original dataset. This includes      third-party data, external APIs, or any supplementary data.</p> </li> <li> <p>Interim: </p> </li> <li> <p>Holds data that is in the process of being cleaned or transformed.      Useful for datasets that are not final but are needed in      intermediate stages.</p> </li> <li> <p>Processed: </p> </li> <li> <p>Contains the final version of the dataset, which is ready for      analysis or modeling. Data in this folder is cleaned, formatted,      and pre-processed.</p> </li> <li> <p>Features: </p> </li> <li>Dedicated to storing feature sets that are used in machine learning      models. This includes transformed data, new features, and selected      features.</li> </ul> <p>Having a clear and organized structure for the data can help to ensure that it is properly managed throughout the project and that any necessary modifications or transformations can be easily traced and recorded. Additionally, it can also make it easier for other team members or stakeholders to understand and access the data as needed.</p>"},{"location":"how-to-guides/data_management_practices/#centralized-data-repository","title":"Centralized data repository","text":"<p>In most cases, it is a best practice to use a centralized shared data folder that is stored on a networked storage system or a cloud-based data storage service. This allows all team members to access the data, and ensures that the data is stored in a secure and organized manner. Additionally, using a centralized data folder can help to avoid duplication and ensure that everyone is working with the same version of the data.</p>"},{"location":"how-to-guides/data_management_practices/#using-vs-code-on-a-project-with-a-centralized-shared-data-folder","title":"Using VS Code on a project with a centralized shared data folder","text":"<p>Step 4 of the Getting Started guide involves creating a shared data directory for the project. To make the data accessible and visible in VS Code, symbolic links need to be created.</p> <p>What is a symbolic link?</p> <p>The purpose of a symbolic link (symlink) in Linux is to create a shortcut or alias to a file or directory. A symlink acts as a reference to the original file, allowing multiple paths to access the same data. This can be useful for creating shortcuts to frequently used files, linking to files in different directories, and creating backups of important files.</p> <p>Here's a shell script that creates symbolic links for the specified folders:</p> symlink_data_folders.sh<pre><code>folders=(raw external interim processed model_output)\n\nfor folder in \"${folders[@]}\"; do\n  ln -s /path/to/original/\"$folder\" /path/to/link/\"$folder\"\ndone\n</code></pre> <p>Where the path to the original folder corresponds to the project's shared data folder:</p> <pre><code>/nyl/data/tenants/insurance/cdsamktg/mediamix/data\n</code></pre> <p>And the link folder corresponds to the location of the project's data folder on the cloned Git repository on server user space:</p> <pre><code>/nyl/data/home/t93kqi0/projects/Media-Mix/data/\"$folder\"\n</code></pre> symlink_data_folders.sh<pre><code>folders=(raw external interim processed model_output)\n\nfor folder in \"${folders[@]}\"; do\n  ln -s /nyl/data/tenants/insurance/cdsamktg/mediamix/data/\"$folder\" \\\n  /nyl/data/home/t93kqi0/projects/Media-Mix/data/\"$folder\"\ndone\n</code></pre> <p>Warning</p> <p>The link <code>data</code> folder must be empty before running the <code>symlink_data_folders.sh</code> script.</p> <p>Now all shared data in each sub-directory of the <code>data</code> shared folder must be visible from the VS Code Explorer pane.</p>"},{"location":"how-to-guides/data_management_practices/#metadata-and-documentation","title":"Metadata and Documentation","text":"<p>Discussing the importance of Metadata and Documentation in the context of your data science project is crucial. Here's a detailed breakdown:</p>"},{"location":"how-to-guides/data_management_practices/#importance-of-metadata-and-documentation","title":"Importance of Metadata and Documentation","text":"<ol> <li> <p>Contextual Understanding: Metadata provides context to the data.    It includes details like the source of the data, when and how it was    collected, its format, and any changes it has undergone. This    information is essential for anyone trying to understand or use the    data effectively.</p> </li> <li> <p>Reproducibility and Traceability: Proper documentation ensures    that data processing steps can be replicated and understood by    others. This is particularly important in data science, where    reproducibility is a key aspect of project credibility and    validation.</p> </li> <li> <p>Data Quality Insights: Metadata can include information about    data quality, such as accuracy, completeness, and consistency. This    is valuable for assessing the reliability of the data and    understanding its limitations.</p> </li> <li> <p>Compliance and Auditing: In many industries, there are regulatory    requirements for data management. Detailed metadata and documentation    help in complying with these regulations and make audits more    manageable.</p> </li> </ol>"},{"location":"how-to-guides/data_management_practices/#recommendations-for-effective-metadata-and-documentation","title":"Recommendations for Effective Metadata and Documentation","text":"<ol> <li> <p>Standardized Format: Adopt a standardized format or template for    metadata and documentation. This could be a structured file like JSON    or XML for metadata, and markdown or structured text files for    documentation.</p> </li> <li> <p>Automated Generation: Where possible, automate the generation of    metadata. For instance, when data is imported or processed, scripts    can automatically capture and record key information.</p> </li> <li> <p>Versioning of Data and Metadata: Just like the data, its metadata    should also be version-controlled. This is important as the data    evolves over time due to various processing steps.</p> </li> <li> <p>Inclusion of Key Elements: Ensure that metadata includes    essential elements like data source, date of acquisition, data    format, any preprocessing steps applied, data schema (if applicable),    and information on confidentiality or sensitivity.</p> </li> <li> <p>Accessibility: Store metadata and documentation in an easily    accessible location. It should be clearly linked to the corresponding    data.</p> </li> <li> <p>Training and Guidelines: Provide team members with training or    guidelines on how to create and maintain proper documentation and    metadata. This ensures consistency and compliance with established    standards.</p> </li> <li> <p>Regular Updates: Just as data changes, so should its    documentation. It's important to update documentation whenever the    data or its handling procedures change.</p> </li> <li> <p>Use of Tools: Leverage tools that support metadata management and    documentation. For instance, data cataloging tools can be very    helpful in larger organizations for maintaining a central repository    of metadata.</p> </li> <li> <p>Collaboration and Reviews: Encourage regular reviews of metadata    and documentation by different team members. This not only improves    the quality but also ensures that the data remains understandable and    usable by everyone.</p> </li> <li> <p>Integration with Data Pipeline: Integrate metadata generation     and documentation updates into your data processing pipelines. This     ensures that changes in data are automatically reflected in the     metadata and documentation.</p> </li> </ol> <p>In summary, comprehensive and up-to-date metadata and documentation are fundamental to the effective management, use, and understanding of data in any data science project. They facilitate better collaboration, ensure compliance with standards and regulations, and significantly contribute to the overall integrity and usability of the data.</p>"},{"location":"how-to-guides/data_management_practices/#example-of-metadata-creation","title":"Example of Metadata Creation","text":"<p>Using a JSON file to store metadata for a CSV file is an efficient way to keep track of important information about your data. Below is an example of how this can be done, along with a method to automate the process.</p>"},{"location":"how-to-guides/data_management_practices/#example-json-metadata-for-a-csv-file","title":"Example: JSON Metadata for a CSV File","text":"<p>Suppose you have a CSV file named <code>sales_data.csv</code>. The metadata for this file could include information such as the source of the data, the date of creation, the number of rows and columns, column names, and any preprocessing steps applied.</p> <p>Here's an example of what the JSON metadata might look like:</p> <pre><code>{\n  \"file_name\": \"sales_data.csv\",\n  \"creation_date\": \"2024-01-22\",\n  \"source\": \"Internal Sales System\",\n  \"number_of_rows\": 1200,\n  \"number_of_columns\": 5,\n  \"columns\": [\n    {\"name\": \"Date\", \"type\": \"Date\", \"description\": \"Date of sale\"},\n    {\"name\": \"Product_ID\", \"type\": \"String\", \"description\": \"Unique identifier for the product\"},\n    {\"name\": \"Quantity\", \"type\": \"Integer\", \"description\": \"Number of products sold\"},\n    {\"name\": \"Price\", \"type\": \"Float\", \"description\": \"Sale price per unit\"},\n    {\"name\": \"Total_Sales\", \"type\": \"Float\", \"description\": \"Total sales amount\"}\n  ],\n  \"preprocessing\": [\n    {\"step\": \"Data Cleaning\", \"description\": \"Removed null values and corrected data formats\"},\n    {\"step\": \"Normalization\", \"description\": \"Normalized the Price column using min-max scaling\"}\n  ],\n  \"notes\": \"Data updated monthly. Last update included Q4 2023 sales data.\"\n}\n</code></pre>"},{"location":"how-to-guides/data_management_practices/#automating-the-metadata-generation","title":"Automating the Metadata Generation","text":"<p>To automate the process of generating this metadata, you can use a script in Python. This script will:</p> <ol> <li>Read the CSV file.</li> <li>Extract relevant information such as the number of rows and columns,    column names, etc.</li> <li>Generate and save the metadata in a JSON file.</li> </ol> <p>Here's a simple Python script to achieve this:</p> <pre><code>import json\nimport pandas as pd\nfrom datetime import datetime\n\ndef generate_metadata(csv_file_path):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Extracting information\n    file_name = csv_file_path.split('/')[-1]\n    creation_date = datetime.now().strftime(\"%Y-%m-%d\")\n    number_of_rows = df.shape[0]\n    number_of_columns = df.shape[1]\n    columns = [{\"name\": col, \"type\": str(df[col].dtype)} for col in df.columns]\n\n    # Metadata dictionary\n    metadata = {\n        \"file_name\": file_name,\n        \"creation_date\": creation_date,\n        \"source\": \"Specify the data source\",\n        \"number_of_rows\": number_of_rows,\n        \"number_of_columns\": number_of_columns,\n        \"columns\": columns,\n        \"preprocessing\": [],  # Add any preprocessing steps manually or through code\n        \"notes\": \"Add any additional notes here\"\n    }\n\n    # Saving metadata to a JSON file\n    with open(file_name.replace('.csv', '_metadata.json'), 'w') as json_file:\n        json.dump(metadata, json_file, indent=4)\n\n# Example usage\ngenerate_metadata('path/to/your/sales_data.csv')\n</code></pre>"},{"location":"how-to-guides/data_management_practices/#notes","title":"Notes:","text":"<ul> <li>The <code>preprocessing</code> section is left empty in the script. This is   because preprocessing steps can vary widely and might need to be added   manually or captured through additional scripting logic based on your   specific data pipeline.</li> <li>The script assumes a basic understanding of the data's source and   nature. Adjustments may be required based on the specific context of   your data.</li> <li>The script uses the <code>pandas</code> library for data handling. Ensure this   library is installed in your Python environment (<code>pip install   pandas</code>).</li> </ul> <p>This automated approach can save significant time and reduce errors in metadata creation, especially for large datasets or frequent data updates.</p>"},{"location":"how-to-guides/data_management_practices/#data-documentation-example","title":"Data Documentation Example","text":""},{"location":"how-to-guides/data_management_practices/#overview","title":"Overview","text":"<p>This document describes the <code>sales_data.csv</code> dataset, which contains records of sales transactions for a retail company. The dataset is updated monthly and includes detailed information about each sale, including the date, product details, and sale amounts.</p>"},{"location":"how-to-guides/data_management_practices/#file-details","title":"File Details","text":"<p>The file is named <code>sales_data_documentation.md</code> which clearly indicates its purpose and content.</p> <ul> <li>File Name: <code>sales_data.csv</code></li> <li>Creation Date: 2024-01-22</li> <li>Last Updated: 2024-01-22</li> <li>Total Records: 1200</li> <li>Source: Internal Sales System</li> </ul>"},{"location":"how-to-guides/data_management_practices/#data-dictionary","title":"Data Dictionary","text":"Column Name Data Type Description Date Date Date of sale (format: YYYY-MM-DD) Product_ID String Unique identifier for the product Quantity Integer Number of products sold Price Float Sale price per unit (in USD) Total_Sales Float Total sales amount (in USD)"},{"location":"how-to-guides/data_management_practices/#preprocessing-steps","title":"Preprocessing Steps","text":"<ol> <li>Data Cleaning</li> <li>Null values in the <code>Price</code> and <code>Quantity</code> columns were removed.</li> <li> <p>Date formats were standardized to <code>YYYY-MM-DD</code>.</p> </li> <li> <p>Normalization</p> </li> <li>The <code>Price</code> column was normalized using min-max scaling.</li> </ol>"},{"location":"how-to-guides/data_management_practices/#usage-notes","title":"Usage Notes","text":"<ul> <li>The dataset is intended for internal use only.</li> <li>Data is confidential and should not be shared outside the organization   without proper authorization.</li> <li>For any discrepancies or data requests, contact the Data Management   Team.</li> </ul>"},{"location":"how-to-guides/data_management_practices/#versioning","title":"Versioning","text":"<ul> <li>Current Version: 1.2</li> <li>Previous Versions:</li> <li>1.1 - Included Q3 2023 sales data.</li> <li>1.0 - Initial dataset creation.</li> </ul>"},{"location":"how-to-guides/dvc-communication/","title":"How-To Guide: Communication of Data Version in a Collaborative Setting","text":""},{"location":"how-to-guides/dvc-communication/#introduction","title":"Introduction","text":"<p>Goal of This Guide</p> <p>Master the art of communicating data version changes effectively in collaborative projects using Data Version Control (DVC) and GitHub. This guide is crucial for teams striving for clarity and consistency in their data-driven projects.</p>"},{"location":"how-to-guides/dvc-communication/#prerequisites","title":"Prerequisites","text":"<ul> <li> Familiarity with DVC and GitHub.</li> <li> DVC and Git installed and set up in your project.</li> </ul>"},{"location":"how-to-guides/dvc-communication/#effective-data-version-communication-steps","title":"Effective Data Version Communication Steps","text":""},{"location":"how-to-guides/dvc-communication/#1-clear-commit-messages-and-pull-requests","title":"1. Clear Commit Messages and Pull Requests","text":"<p>Commit Messages</p> <ul> <li>Be descriptive with your commit messages when including DVC-tracked files.</li> <li>Example: \"Update dataset.csv to version 2.1 for model training.\"</li> </ul> <p>Pull Requests (PRs)</p> <ul> <li>PR descriptions should clearly state data changes and their purposes.</li> <li>This clarity aids team members in understanding the update's context.</li> </ul>"},{"location":"how-to-guides/dvc-communication/#2-using-dvc-files-in-git","title":"2. Using DVC Files in Git","text":"<ul> <li> DVC tracks changes using <code>.dvc</code> files, committed to Git.</li> <li> Team members can view these files in commits or PRs on GitHub to identify data versions.</li> </ul>"},{"location":"how-to-guides/dvc-communication/#3-syncing-data-with-dvc-pull","title":"3. Syncing Data with DVC Pull","text":"<ul> <li>After updates, run <code>dvc pull</code> to synchronize local data with the tracked branch version.</li> </ul> <pre><code>dvc pull\n</code></pre>"},{"location":"how-to-guides/dvc-communication/#4-automated-notifications-on-github","title":"4. Automated Notifications on GitHub","text":"<ul> <li> Utilize GitHub's notification features for real-time update alerts.</li> <li>:octicons-watch-16: Watching a repository can also keep the team informed.</li> </ul>"},{"location":"how-to-guides/dvc-communication/#5-keeping-project-documentation-updated","title":"5. Keeping Project Documentation Updated","text":"<ul> <li> Maintain a section in your project's documentation or README about current data versions.</li> <li> Update this section following any significant dataset changes.</li> </ul>"},{"location":"how-to-guides/dvc-communication/#6-regular-communication-channels","title":"6. Regular Communication Channels","text":"<ul> <li> Hold regular team meetings or send updates through emails or chat channels.</li> <li> Summarize key project changes, focusing on data and model updates.</li> </ul>"},{"location":"how-to-guides/dvc-communication/#conclusion","title":"Conclusion","text":"<p>Final Thoughts</p> <p>Adhering to these steps ensures that every team member is informed about data version changes, fostering an environment of efficient collaboration and consistent outcomes in your data-driven projects.</p>"},{"location":"how-to-guides/dvc-set-up/","title":"How-To Guide: Setting Up and Using Data Version Control with DVC","text":""},{"location":"how-to-guides/dvc-set-up/#setting-up-dvc-in-your-project","title":"Setting Up DVC in Your Project","text":""},{"location":"how-to-guides/dvc-set-up/#step-1-initialize-dvc","title":"Step 1: Initialize DVC","text":"<p>Initialization</p> <ol> <li>Open your terminal.</li> <li>Navigate to your project repository.</li> <li>Run the following command:    <pre><code>dvc init\n</code></pre>    This initializes DVC in your repository, creating a <code>.dvc</code> directory.</li> </ol>"},{"location":"how-to-guides/dvc-set-up/#step-2-set-up-remote-storage","title":"Step 2: Set Up Remote Storage","text":"<p>Choose Your Storage Backend</p> <p>DVC supports various storage backends. Depending on your choice (Azure, AWS, local), follow the appropriate steps below.</p>"},{"location":"how-to-guides/dvc-set-up/#for-azure-blob-storage","title":"For Azure Blob Storage","text":"<ol> <li>Run the commands:    <pre><code>dvc remote add -d myremote azure://mycontainer/path\ndvc remote modify myremote connection_string 'myconnectionstring'\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#for-aws-s3","title":"For AWS S3","text":"<ol> <li>Configure AWS and run:    <pre><code>dvc remote add -d myremote s3://mybucket/path\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#for-local-storage","title":"For Local Storage","text":"<ol> <li>Set up local storage:    <pre><code>dvc remote add -d myremote /path/to/local/storage\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#step-3-add-data-to-dvc","title":"Step 3: Add Data to DVC","text":"<p>Tracking Data</p> <ol> <li>Use the <code>dvc add</code> command to track files or directories:    <pre><code>dvc add data/dataset.csv\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#step-4-commit-changes-to-version-control","title":"Step 4: Commit Changes to Version Control","text":"<ol> <li>Commit the changes to both DVC and Git:    <pre><code>git add data/dataset.csv.dvc data/.gitignore\ngit commit -m \"Add dataset with DVC\"\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#step-5-push-data-to-remote-storage","title":"Step 5: Push Data to Remote Storage","text":"<ol> <li>Push your data to the remote storage:    <pre><code>dvc push\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#updating-data-with-dvc","title":"Updating Data with DVC","text":""},{"location":"how-to-guides/dvc-set-up/#local-data-updates","title":"Local Data Updates","text":"<p>Updating Local Data</p> <ol> <li>Modify your dataset file (e.g., <code>dataset.csv</code>).</li> <li>Run <code>dvc add dataset.csv</code> to update the <code>.dvc</code> file.</li> <li>Commit and push the changes:    <pre><code>git add dataset.csv.dvc\ngit commit -m \"Update dataset.csv\"\ndvc push\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#cloud-data-updates","title":"Cloud Data Updates","text":"<p>Cloud Updates</p> <ol> <li>Synchronize with the cloud using <code>dvc pull</code>.</li> <li>After detecting changes, pull the updated data.</li> <li>Commit and push any local changes:    <pre><code>dvc add dataset.csv\ngit add dataset.csv.dvc\ngit commit -m \"Update dataset.csv with cloud changes\"\ndvc push\n</code></pre></li> </ol>"},{"location":"how-to-guides/dvc-set-up/#conclusion","title":"Conclusion","text":"<p>By following these steps, you can effectively utilize DVC for data version control, ensuring the consistency and reproducibility of your data science and machine learning projects.</p>"},{"location":"how-to-guides/dvc-vscode-extension/","title":"How-To Guide: Using DVC Extension in Visual Studio Code for Data Management","text":""},{"location":"how-to-guides/dvc-vscode-extension/#introduction","title":"Introduction","text":"<p>Getting Started</p> <p>Enhance your project's data management in VS Code using the DVC extension. This guide walks you through key features to streamline your data version control workflows.</p>"},{"location":"how-to-guides/dvc-vscode-extension/#prerequisites","title":"Prerequisites","text":"<ul> <li> Visual Studio Code installed.</li> <li> DVC extension added to VS Code.</li> <li>A DVC-initialized project.</li> </ul>"},{"location":"how-to-guides/dvc-vscode-extension/#using-the-dvc-extension-in-vs-code","title":"Using the DVC Extension in VS Code","text":""},{"location":"how-to-guides/dvc-vscode-extension/#step-1-view-dvc-tracked-files","title":"Step 1: View DVC Tracked Files","text":"<ol> <li>Open the DVC Sidebar:</li> <li>Click the DVC icon in the VS Code activity bar to access the DVC sidebar.</li> <li>This sidebar shows all DVC-tracked files and stages in your project.</li> </ol>"},{"location":"how-to-guides/dvc-vscode-extension/#step-2-execute-dvc-commands","title":"Step 2: Execute DVC Commands","text":"<p>Efficient DVC Operations</p> <p>Interact with DVC-tracked files directly in VS Code to execute common DVC commands.</p> <ol> <li>Right-click on a DVC-tracked file or stage.</li> <li>Select from commands like <code>dvc pull</code>, <code>dvc push</code>, or <code>dvc repro</code>.</li> <li>Manage your data versions without switching from the VS Code interface.</li> </ol>"},{"location":"how-to-guides/dvc-vscode-extension/#step-3-monitor-pipeline-status","title":"Step 3: Monitor Pipeline Status","text":"<ol> <li>Utilize Pipeline Visualization:</li> <li>The DVC extension provides a graphical view of your data pipelines.</li> <li>Monitor progress and status, and easily spot issues.</li> </ol>"},{"location":"how-to-guides/dvc-vscode-extension/#step-4-manage-experiments","title":"Step 4: Manage Experiments","text":"<ol> <li>Handle Machine Learning Experiments:</li> <li>Use the extension to browse, compare, and manage your DVC experiments.</li> <li>This is particularly useful for evaluating different model iterations.</li> </ol>"},{"location":"how-to-guides/dvc-vscode-extension/#conclusion","title":"Conclusion","text":"<p>Enhance Your Workflow</p> <p>The DVC extension in VS Code is a powerful tool for data version control. It simplifies tracking changes, monitoring pipelines, and managing experiments, leading to a more organized and efficient development process.</p>"},{"location":"how-to-guides/file-naming-conventions/","title":"File Naming Conventions Guide","text":""},{"location":"how-to-guides/file-naming-conventions/#overview","title":"Overview","text":"<p>To maintain consistency and readability in our project, we use the \"snake_case\" naming convention for all file names. This guide provides a set of rules and examples to help you name files correctly.</p>"},{"location":"how-to-guides/file-naming-conventions/#what-is-snake_case","title":"What is Snake_Case?","text":"<p>Snake_case is a naming convention where each word in a file name is lowercase and separated by underscores (<code>_</code>). This format is easy to read and avoids issues with different operating systems handling file names differently.</p>"},{"location":"how-to-guides/file-naming-conventions/#rules-for-snake_case-file-naming","title":"Rules for Snake_Case File Naming","text":"<ol> <li>Use Lowercase Letters: All letters should be in lowercase.</li> <li>Separate Words with Underscores: Use underscores (<code>_</code>) to    separate words.</li> <li>Be Descriptive: File names should be descriptive enough to    indicate their contents or purpose.</li> <li>Avoid Special Characters: Do not use spaces, hyphens, or any    special characters other than underscores.</li> <li>Use Consistent Extensions: Ensure file extensions are consistent    and appropriate for the file type (e.g., <code>.py</code> for Python files,    <code>.md</code> for Markdown files).</li> </ol>"},{"location":"how-to-guides/file-naming-conventions/#examples","title":"Examples","text":""},{"location":"how-to-guides/file-naming-conventions/#python-scripts","title":"Python Scripts","text":"<ul> <li><code>data_cleaning.py</code></li> <li><code>config_settings.py</code></li> <li><code>test_data_processing.py</code></li> </ul>"},{"location":"how-to-guides/file-naming-conventions/#configuration-files","title":"Configuration Files","text":"<ul> <li><code>project_config.yaml</code></li> <li><code>database_setup.json</code></li> </ul>"},{"location":"how-to-guides/file-naming-conventions/#documentation-files","title":"Documentation Files","text":"<ul> <li><code>user_guide.md</code></li> <li><code>api_reference.md</code></li> </ul>"},{"location":"how-to-guides/file-naming-conventions/#jupyter-notebooks","title":"Jupyter Notebooks","text":"<ul> <li><code>data_analysis_notebook.ipynb</code></li> <li><code>machine_learning_experiments.ipynb</code></li> </ul>"},{"location":"how-to-guides/file-naming-conventions/#data-files","title":"Data Files","text":"<ul> <li>CSV files: <code>sales_data_2023.csv</code></li> <li>JSON files: <code>user_profiles.json</code></li> </ul>"},{"location":"how-to-guides/file-naming-conventions/#tips","title":"Tips","text":"<ul> <li>Keep it Short and Simple: Aim for clarity but avoid overly long   file names.</li> <li>Versioning: If versioning is needed, append the version number at   the end (e.g., <code>data_cleaning_v2.py</code>).</li> <li>Avoid Redundancy: Avoid repeating the same word unnecessarily in   file names.</li> </ul>"},{"location":"how-to-guides/file-naming-conventions/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":"<ul> <li>Uppercase Letters: <code>Data_Cleaning.py</code> (Incorrect)</li> <li>Hyphens: <code>data-cleaning.py</code> (Incorrect)</li> <li>Spaces: <code>data cleaning.py</code> (Incorrect)</li> <li>Special Characters: <code>data@cleaning!.py</code> (Incorrect)</li> </ul> <p>By following these guidelines, we ensure that our file names are consistent, readable, and easy to manage across different systems and environments.</p>"},{"location":"how-to-guides/git-branch-naming-standards/","title":"Git Branch Naming Standards for ML Projects","text":"<p>ChatGPT Prompt for Branch Name</p> <p>Copy and paste the following prompt into ChatGPT to generate a branch name based on your JIRA story:</p> <pre><code>I need a branch name for a new task. Here are the details:\n1. The description of the JIRA story: [Enter JIRA story description]\n2. The JIRA number: [Enter JIRA number]\n3. The standard to follow: &lt;category&gt;/&lt;description&gt;-&lt;issue_number_or_jira_key&gt;\n\nPlease use one of the following categories: feature, bugfix, data, experiment, model, docs, refactor, test, chore.\n\nProvide the git command to create this branch.\n</code></pre> <p>Example Prompt:</p> <pre><code>I need a branch name for a new task. Here are the details:\n4. The description of the JIRA story: Add user authentication\n5. The JIRA number: DATA123\n6. The standard to follow: feature/add-user-authentication-DATA123\n\nProvide the git command to create this branch.\n</code></pre> <p>ChatGPT Output:</p> <pre><code>git checkout -b feature/add-user-authentication-DATA123\n</code></pre> <p>Overview</p> <p>For machine learning projects, maintaining clarity in the Git repository is crucial. A consistent approach to branch naming is key to this clarity, facilitating rapid identification of the branch's purpose, aiding in collaboration, and making repository navigation intuitive.</p> <p>Naming Convention Structure</p> <p>Branch names should follow this pattern:</p> <pre><code>&lt;category&gt;/&lt;description&gt;-&lt;issue_number_or_jira_key&gt;\n</code></pre> <ul> <li>Category: A keyword indicating the branch's work nature.</li> <li>Description: A clear descriptor of the task or feature.</li> <li>Issue Number or Jira Key: Mandatory for linking to a corresponding task in your project management tool.</li> </ul> <p>Categories</p> <p>Standard categories provide context on the branch's work domain:</p> Category Description <code>feature</code> New feature development or enhancements <code>bugfix</code> Targeted branches for bug resolution <code>data</code> Data management activities, like acquisition or processing <code>experiment</code> Exploratory or experimental development <code>model</code> Model creation, testing, or deployment <code>docs</code> Documentation creation or updates <code>refactor</code> Code restructuring to improve performance without altering functionality <code>test</code> Test development or modification <code>chore</code> Routine tasks or minor updates <p>Examples</p> <p>Branch names that meet our standards:</p> <ul> <li><code>feature/user-authentication-DATA123</code></li> <li><code>data/dataset-enhancement-GH15</code></li> <li><code>model/performance-improvement-DATA22</code></li> <li><code>bugfix/data-loading-error-GH45</code></li> <li><code>docs/api-documentation-update</code></li> <li><code>refactor/code-optimization-DATA78</code></li> <li><code>test/new-model-tests-GH27</code></li> </ul> <p>Guidelines</p> <ul> <li>Use lowercase for all branch names.</li> <li>Hyphens for word separation enhance readability.</li> <li>Keep names brief yet descriptive.</li> <li>Including the issue number or Jira key is mandatory for traceability.</li> </ul> <p>Conclusion</p> <p>Following these naming conventions is vital for organizing and accessing our ML project repositories, fostering team collaboration and efficient project management.</p>"},{"location":"how-to-guides/git-branch-naming-standards/#connecting-jira-stories-and-github","title":"Connecting JIRA stories and GitHub","text":"<p>Using JIRA story numbers or sub-task numbers in GitHub branch names and commit messages can be a good practice as it helps maintain traceability between code changes and project management tasks. Here are some considerations and recommendations to optimize this practice:</p>"},{"location":"how-to-guides/git-branch-naming-standards/#using-jira-story-number-in-branch-names","title":"Using JIRA Story Number in Branch Names","text":"<p>Creating a branch for each JIRA story and using the story number in the branch name is a widely adopted practice. This approach ensures that all code changes related to a specific story are organized in one branch.</p> <p>Example:</p> <ul> <li>Branch name for a JIRA story: <code>feature/add-churn-prediction-model-JIRA-123</code></li> </ul>"},{"location":"how-to-guides/git-branch-naming-standards/#using-sub-task-numbers-in-branch-names","title":"Using Sub-task Numbers in Branch Names","text":"<p>If the story is large and has several sub-tasks, creating separate branches for each sub-task can provide more granular control and better organization of work. This is especially useful if multiple team members are working on different sub-tasks of the same story.</p> <p>Example:</p> <ul> <li>Branch name for a sub-task: <code>feature/preprocess-data-JIRA-123-subtask-1</code></li> <li>Branch name for another sub-task: <code>feature/train-model-JIRA-123-subtask-2</code></li> </ul>"},{"location":"how-to-guides/git-branch-naming-standards/#recommendations","title":"Recommendations","text":"<ol> <li>Small to Medium Stories:</li> <li>Use a single branch for the entire story if the scope is manageable.</li> <li>Include the JIRA story number in the branch name.</li> <li> <p>Example: <code>feature/add-churn-prediction-model-JIRA-123</code></p> </li> <li> <p>Large Stories with Multiple Sub-tasks:</p> </li> <li>Create separate branches for each sub-task to allow parallel development.</li> <li>Include both the JIRA story number and the sub-task number in the branch name.</li> <li>Example: <code>feature/preprocess-data-JIRA-123-subtask-1</code></li> </ol>"},{"location":"how-to-guides/git-branch-naming-standards/#commit-message-conventions","title":"Commit Message Conventions","text":"<p>Including JIRA story or sub-task numbers in commit messages helps maintain a clear history of changes and their associated tasks.</p> <p>Example:</p> <ul> <li>Commit message for a story: <code>Added data preprocessing steps-JIRA-123</code></li> <li>Commit message for a sub-task: <code>Implemented initial data cleaning-JIRA-123-subtask-1</code></li> </ul>"},{"location":"how-to-guides/git-branch-naming-standards/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Branch Naming:</li> <li>Use <code>feature/</code>, <code>bugfix/</code>, or <code>hotfix/</code> prefixes to indicate the type of work.</li> <li>Include the JIRA story or sub-task number for traceability.</li> <li> <p>Use descriptive names to indicate the purpose of the branch.</p> </li> <li> <p>Commit Messages:</p> </li> <li>Start with the description of the change.</li> <li>End with the JIRA story or sub-task number.</li> <li> <p>Optionally, add more detail in the body of the commit message.</p> </li> <li> <p>Branch Management:</p> </li> <li>For small to medium stories, create a single branch per story.</li> <li>For large stories, create separate branches for each sub-task to facilitate parallel work.</li> <li>Ensure all sub-task branches are merged into the main story branch before final integration.</li> </ol>"},{"location":"how-to-guides/git-branch-naming-standards/#example-workflow","title":"Example Workflow","text":"<ol> <li>Create a Branch for the Story:</li> <li> <p><code>feature/add-churn-prediction-model-JIRA-123</code></p> </li> <li> <p>Create Sub-task Branches:</p> </li> <li><code>feature/preprocess-data-JIRA-123-subtask-1</code></li> <li> <p><code>feature/train-model-JIRA-123-subtask-2</code></p> </li> <li> <p>Commit Changes with Clear Messages:</p> </li> <li><code>Implemented initial data cleaning-JIRA-123-subtask-1</code></li> <li> <p><code>Trained logistic regression model-JIRA-123-subtask-2</code></p> </li> <li> <p>Merge Sub-task Branches into Story Branch:</p> </li> <li>Merge <code>feature/preprocess-data-JIRA-123-subtask-1</code> into <code>feature/add-churn-prediction-model-JIRA-123</code></li> <li> <p>Merge <code>feature/train-model-JIRA-123-subtask-2</code> into <code>feature/add-churn-prediction-model-JIRA-123</code></p> </li> <li> <p>Integrate the Story Branch into Main Development Branch:</p> </li> <li>Merge <code>feature/add-churn-prediction-model-JIRA-123</code> into <code>dev</code> or <code>main</code></li> </ol> <p>By following these practices, you can ensure that your development process remains organized and traceable, facilitating better project management and collaboration.</p>"},{"location":"how-to-guides/github-actions-naming-convention/","title":"Enforcing Naming Conventions Using GitHub Actions","text":"<p>Overview</p> <p>Enforcing consistent naming conventions in machine learning projects is crucial for organization and clarity. GitHub Actions provides a seamless solution to automate naming convention checks in your GitHub repositories.</p>"},{"location":"how-to-guides/github-actions-naming-convention/#step-by-step-guide-to-setup-github-actions-for-naming-convention-checks","title":"Step-by-Step Guide to Setup GitHub Actions for Naming Convention Checks","text":"<ol> <li>Creating the Workflow File:</li> <li>Place a YAML file named <code>check_naming_convention.yml</code> in the <code>.github/workflows/</code> directory.</li> <li> <p>Define triggers for the workflow (e.g., on push and pull requests) and the script to execute.</p> </li> <li> <p>Writing the Validation Script:</p> </li> <li>Add a Python script <code>validate_naming.py</code> to <code>.github/scripts/</code>.</li> <li> <p>The script should contain functions to validate naming conventions using regular expressions.</p> </li> <li> <p>Executing the Workflow:</p> </li> <li>The workflow runs on each push or pull request, executing the Python script.</li> <li>It checks file and branch names, failing if naming conventions are violated.</li> </ol> <p>Example Workflow File</p> <pre><code>name: Check Naming Convention\n\non: [push, pull_request]\n\njobs:\n  naming-convention-check:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Validate Naming Convention\n      run: python .github/scripts/validate_naming.py\n      env:\n        GITHUB_REF: ${{ github.ref }}\n</code></pre> <p>Example Python Script Structure</p> <pre><code>\"\"\"\nThis module `validate_naming.py` is part of a GitHub Actions workflow\ndesigned to enforce naming conventions across various assets in a\nmachine learning project. It includes functions to validate the naming\nof Jupyter notebooks, Python scripts, model persistence files, data\nfiles, and Git branches.\n\nThe module contains the following key functions: -\n`validate_ml_file_naming`: Checks naming conventions for Jupyter\nnotebooks and Python scripts.  - `validate_model_file_naming`: Ensures\nmodel file names adhere to the specified naming standards.  -\n`validate_data_file_naming`: Validates data file names against the\nproject's naming conventions.  - `validate_git_branch_naming`: Checks if\nthe Git branch names follow the predefined naming patterns.\n\nThe script is executed as part of a GitHub Actions workflow and is\nessential for maintaining consistency, readability, and organization of\nfiles in the repository.\n\nNote: - This script is meant to be run as a GitHub Action and relies on\nenvironmental variables provided by GitHub workflows.  - Regular\nexpressions are used for pattern matching to validate the naming\nconventions.\n\"\"\"\n\nimport os\nimport re\nimport sys\n\n\ndef validate_ml_file_naming(filename):\n    \"\"\"\n    Check if the filename adheres to the ML project's naming convention.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the filename matches the naming convention, False\n        otherwise.\n\n    Notes\n    -----\n    The naming convention expected:\n    '&lt;type&gt;_&lt;topic&gt;_&lt;version&gt;_&lt;YYYYMMDD&gt;.&lt;extension&gt;' Where: - &lt;type&gt; is\n    a flexible, user-defined category (e.g., 'eda', 'preprocess').  -\n    &lt;topic&gt; is a concise descriptor of the main focus.  - &lt;version&gt; is\n    an optional version identifier.  - &lt;YYYYMMDD&gt; is the creation or\n    last modified date.  - &lt;extension&gt; is either '.ipynb' for Jupyter\n    notebooks or '.py' for Python scripts.\n    \"\"\"\n    # Regex pattern with a more flexible 'type' component\n    pattern = re.compile(r\"^\\w+_\\w+(_v\\d+)?_\\d{8}\\.[ipynb|py]$\")\n    return pattern.match(filename)\n\n\ndef validate_model_file_naming(filename):\n    \"\"\"\n    Check if the filename adheres to the model persistence file naming\n    convention.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the model file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the filename matches the naming convention, False\n        otherwise.\n\n    Notes\n    -----\n    The naming convention for model persistence files is:\n    '&lt;project_name&gt;_&lt;model_version&gt;_&lt;model_type&gt;_&lt;timestamp&gt;.pkl' This\n    includes: - `project_name`: Name of the project.  - `model_version`:\n    Version of the model, following semantic versioning.  -\n    `model_type`: Type or name of the model.  - `timestamp`: Date when\n    the model was saved (YYYYMMDD format).\n    \"\"\"\n    pattern = re.compile(r\"^\\w+_(v\\d+\\.\\d+\\.\\d+)_\\w+_\\d{8}\\.pkl$\")\n    return pattern.match(filename)\n\n\ndef validate_data_file_naming(filename):\n    \"\"\"\n    Check if the filename adheres to the data file naming convention.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the data file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the filename matches the naming convention, False\n        otherwise.\n\n    Notes\n    -----\n    The naming convention for data files is:\n    '&lt;dataset_name&gt;_&lt;version&gt;_&lt;creation_date&gt;_&lt;description&gt;.&lt;extension&gt;'\n    This includes: - `dataset_name`: Identifier for the dataset.  -\n    `version`: Version of the dataset, following semantic versioning or\n    numbering.  - `creation_date`: Date of creation or last modification\n    (YYYYMMDD format).  - `description`: Brief description of the\n    dataset or subset.  - `extension`: File extension indicating the\n    file type (e.g., 'csv', 'xlsx', 'json').\n    \"\"\"\n    pattern = re.compile(r\"^\\w+_(v\\d+\\.\\d+\\.\\d+|\\d+)_\\d{8}_\\w+\\.\\w+$\")\n    return pattern.match(filename)\n\n\ndef validate_git_branch_naming():\n    \"\"\"\n    Check if the Git branch name adheres to the naming convention.\n    \"\"\"\n    branch_ref = os.getenv(\"GITHUB_REF\")\n    if not branch_ref:\n        print(\"No branch reference found.\")\n        return True  # Might not be a branch push, so don't fail the check.\n\n    # Extract the branch name from refs/heads/your-branch-name\n    branch_name = branch_ref.split(\"/\")[-1]\n    pattern = re.compile(\n        r\"^(feature|bugfix|data|experiment|model|docs|refactor|test|chore)/[\\w-]+(_\\d+)?$\"\n    )\n    return pattern.match(branch_name)\n\n\ndef main():\n    \"\"\"\n    The main function to validate naming conventions in a machine\n    learning project.\n\n    This function executes a series of naming convention checks on\n    various file types, including model files, Jupyter notebooks, Python\n    scripts, and data files. It also validates Git branch names. If any\n    file or branch name does not adhere to the predefined naming\n    conventions, the function flags an error.\n\n    The checks are performed as follows: - For Git branch names, using\n    the `validate_git_branch_naming` function.  - For model files\n    (*.pkl), using the `validate_model_file_naming` function.  - For\n    Jupyter notebooks and Python scripts (*.ipynb, *.py), using the\n      `validate_ml_file_naming` function.\n    - For data files (*.csv, *.xlsx, *.json), using the\n      `validate_data_file_naming` function.\n\n    If any naming convention violations are found, the script exits with\n    a status code of 1, indicating an error. This is used in the context\n    of GitHub Actions workflows to flag naming convention violations in\n    pull requests or pushes.\n    \"\"\"\n    error = False\n    if not validate_git_branch_naming():\n        print(f\"Invalid Git branch naming convention.\")\n        error = True\n\n    for root, _, files in os.walk(\".\"):\n        for file in files:\n            if file.endswith(\".pkl\") and not validate_model_file_naming(file):\n                print(f\"Invalid model file naming convention: {file}\")\n                error = True\n            elif file.endswith((\".ipynb\", \".py\")) and not \\\n                validate_ml_file_naming(file):\n                print(f\"Invalid notebook/script naming convention: {file}\")\n                error = True\n            elif file.endswith((\".csv\", \".xlsx\", \".json\")) and \\\n                 not validate_data_file_naming(file):\n                print(f\"Invalid data file naming convention: {file}\")\n                error = True\n\n\n    if error:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Conclusion</p> <p>This GitHub Actions workflow promotes consistent and organized naming across your project, serving as a preventive measure against naming irregularities in the main codebase.</p>"},{"location":"how-to-guides/introduction-to-doctest/","title":"Introduction to Testing with Doctest for Data Scientists","text":"<p>This guide introduces you to <code>doctest</code>, a simple and effective tool for testing and documenting your Python code. As a team of data scientists, you might not have extensive coding experience, but <code>doctest</code> provides a straightforward way to ensure your code works as expected while also serving as a form of documentation.</p>"},{"location":"how-to-guides/introduction-to-doctest/#what-is-doctest","title":"What is Doctest?","text":"<p><code>doctest</code> is a module included in the Python standard library that allows you to test your code by running examples embedded in the documentation and verifying that they produce the expected results. This approach helps to ensure that your code works as documented.</p>"},{"location":"how-to-guides/introduction-to-doctest/#why-use-doctest","title":"Why Use Doctest?","text":"<ul> <li>Ease of Use: <code>doctest</code> is simple to set up and use, making it an   excellent starting point for those new to testing.</li> <li>Documentation: Tests written with <code>doctest</code> serve as both test   cases and examples of how to use your functions, improving code   readability and usability.</li> <li>Immediate Feedback: Running <code>doctest</code> provides immediate feedback   on whether your code is working as expected.</li> </ul>"},{"location":"how-to-guides/introduction-to-doctest/#why-unit-testing-is-important","title":"Why Unit Testing is Important","text":"<p>Even though libraries like pandas, scikit-learn, and TensorFlow are well-tested and reliable, it\u2019s crucial to unit test your custom code. In machine learning projects, custom code often includes data preprocessing, feature engineering, model evaluation, and utility functions. Ensuring the correctness of this code is vital for the accuracy and reliability of your models. Unit testing helps catch errors early, facilitates code maintenance, and promotes confidence in your code\u2019s functionality.</p> <p>As a data scientist, testing your code before opening a pull request (PR) and merging it into the production branch is critical. This practice is especially important in projects like Retrieval-Augmented Generation (RAG) systems, where data scientists are expected to contribute to production code from the very beginning. Unlike traditional machine learning projects, RAG systems require seamless integration of custom code into the production environment, making thorough testing essential to prevent potential issues and ensure smooth operation.</p> <p>By incorporating unit testing into your workflow, you ensure that your contributions are reliable and maintain the integrity of the overall project. This not only enhances the quality of your code but also promotes collaborative development, where team members can confidently build upon each other's work.</p>"},{"location":"how-to-guides/introduction-to-doctest/#doctest-for-simple-unit-tests","title":"Doctest for Simple Unit Tests","text":"<p>Here's an example of how to use doctest with Google-style docstrings:</p> <pre><code>def add(a, b):\n    \"\"\"\n    Adds two numbers together.\n\n    Args:\n        a (int): The first number.\n        b (int): The second number.\n\n    Returns:\n        int: The sum of the two numbers.\n\n    Example:\n        &gt;&gt;&gt; add(2, 3)\n        5\n        &gt;&gt;&gt; add(-1, 1)\n        0\n        &gt;&gt;&gt; add(0, 0)\n        0\n    \"\"\"\n    return a + b\n</code></pre>"},{"location":"how-to-guides/introduction-to-doctest/#running-doctests","title":"Running Doctests","text":"<p>To run the doctests in your module, use the following command:</p> <pre><code>python -m doctest -v your_module.py\n</code></pre>"},{"location":"how-to-guides/introduction-to-doctest/#additional-resources","title":"Additional Resources","text":"<p>For further learning on testing in Python, including more advanced techniques, consider the following resources:</p> <ul> <li>Introduction to Testing in Python -   DataCamp</li> <li>Python Testing - Real Python</li> <li>Python Unit Testing - Real   Python</li> <li>Unit Testing in Python -   DataCamp</li> </ul> <p>By following this guide and incorporating doctest into your development workflow, you can ensure that your code is well-documented, tested, and reliable. This foundational practice will help you and your team produce high-quality, maintainable machine learning projects.</p>"},{"location":"how-to-guides/jira-story-best-practices/","title":"Best Practices for Creating JIRA Stories for ML/AI Projects","text":"<p>Creating well-defined JIRA stories is crucial for the success of any ML/AI project. Clear and concise stories help ensure that team members understand the tasks, leading to better planning, execution, and tracking. Here are some best practices for creating JIRA stories in ML/AI projects:</p>"},{"location":"how-to-guides/jira-story-best-practices/#1-define-the-goal-clearly","title":"1. Define the Goal Clearly","text":"<ul> <li>Objective: Start with a clear and concise statement of what the story aims to achieve. The goal should align with the project\u2019s overall objectives.</li> <li>Example: \"Develop a model to predict customer churn with at least 80% accuracy.\"</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#2-include-detailed-requirements","title":"2. Include Detailed Requirements","text":"<ul> <li>Data Requirements: Specify the datasets needed, including their sources and any preprocessing steps required.</li> <li>Technical Requirements: Mention the algorithms, libraries, frameworks, and tools to be used.</li> <li>Performance Metrics: Define the metrics that will be used to evaluate the success of the task (e.g., accuracy, precision, recall).</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#3-break-down-the-task","title":"3. Break Down the Task","text":"<ul> <li>Subtasks: Divide the story into smaller, manageable subtasks. Each subtask should represent a specific piece of work that contributes to the story.</li> <li>Example Subtasks:</li> <li>Collect and preprocess data.</li> <li>Perform exploratory data analysis.</li> <li>Train and validate the model.</li> <li>Deploy the model to production.</li> <li>Monitor model performance.</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#4-define-acceptance-criteria","title":"4. Define Acceptance Criteria","text":"<ul> <li>Clear Criteria: List the conditions that must be met for the story to be considered complete. Acceptance criteria should be specific, measurable, and testable.</li> <li>Example: \"The model should achieve at least 80% accuracy on the validation dataset.\"</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#5-provide-context-and-background","title":"5. Provide Context and Background","text":"<ul> <li>Context: Include any relevant background information or context that helps team members understand the importance and scope of the story.</li> <li>Example: \"This model will help reduce customer churn by identifying at-risk customers early, allowing the marketing team to take proactive measures.\"</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#6-link-to-related-stories-and-documentation","title":"6. Link to Related Stories and Documentation","text":"<ul> <li>Dependencies: Link the story to any related JIRA issues or epics to provide visibility into dependencies.</li> <li>Documentation: Provide links to relevant documentation, datasets, research papers, or other resources that may assist in completing the story.</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#7-estimate-effort-and-assign-responsibilities","title":"7. Estimate Effort and Assign Responsibilities","text":"<ul> <li>Effort Estimation: Estimate the effort required to complete the story, using story points or another appropriate metric.</li> <li>Assignees: Assign the story to the appropriate team member(s) with the necessary skills and expertise.</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#8-use-descriptive-titles","title":"8. Use Descriptive Titles","text":"<ul> <li>Title: Ensure the story title is descriptive and provides a quick understanding of the task.</li> <li>Example: \"Develop a customer churn prediction model using logistic regression.\"</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#9-add-labels-and-components","title":"9. Add Labels and Components","text":"<ul> <li>Labels: Use relevant labels to categorize the story (e.g., <code>data-preprocessing</code>, <code>model-training</code>, <code>deployment</code>).</li> <li>Components: If your JIRA project uses components, assign the story to the appropriate component (e.g., <code>Data Engineering</code>, <code>Model Development</code>).</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#10-review-and-refine","title":"10. Review and Refine","text":"<ul> <li>Review: Regularly review and refine JIRA stories during sprint planning or backlog grooming sessions to ensure they remain relevant and up-to-date.</li> <li>Feedback: Encourage team members to provide feedback on stories to improve clarity and completeness.</li> </ul>"},{"location":"how-to-guides/jira-story-best-practices/#example-jira-story-template","title":"Example JIRA Story Template","text":"<p>Title: Develop a customer churn prediction model using logistic regression</p> <p>Description: Create a machine learning model to predict customer churn. The model should achieve at least 80% accuracy on the validation dataset. This will help the marketing team to identify at-risk customers and take proactive measures to retain them.</p> <p>Requirements:</p> <ul> <li>Data: Customer transaction data from the last 12 months.</li> <li>Algorithms: Logistic regression.</li> <li>Libraries: Scikit-learn, Pandas, NumPy.</li> <li>Performance Metrics: Accuracy, precision, recall.</li> </ul> <p>Subtasks:</p> <ol> <li>Collect and preprocess data.</li> <li>Perform exploratory data analysis.</li> <li>Train and validate the logistic regression model.</li> <li>Deploy the model to production.</li> <li>Monitor model performance.</li> </ol> <p>Acceptance Criteria:</p> <ul> <li>The model achieves at least 80% accuracy on the validation dataset.</li> <li>The model is deployed to production and accessible via API.</li> <li>Model performance is monitored, and results are reported weekly.</li> </ul> <p>Context: This model will help reduce customer churn by identifying at-risk customers early, allowing the marketing team to take proactive measures.</p> <p>Links:</p> <ul> <li>Customer Transaction Dataset</li> <li>Model Training Documentation</li> </ul> <p>Estimate: 5 Story Points</p> <p>Assignee: John Doe</p> <p>Labels: data-preprocessing, model-training, deployment</p> <p>Components: Model Development</p> <p>By following these best practices, you can create JIRA stories that are clear, actionable, and aligned with the goals of your ML/AI project.</p>"},{"location":"how-to-guides/jupyter-vscode-directory/","title":"How to Set Up Jupyter Notebooks in VS Code to Use the Correct Start-up Directory","text":"<p>Ensuring that Jupyter notebooks in Visual Studio Code (VS Code) start in the correct directory is crucial for maintaining a smooth and efficient workflow, especially when dealing with file paths. This guide will walk you through configuring VS Code so that Jupyter notebooks always use your project's root directory as their start-up directory, eliminating the need to adjust paths frequently.</p>"},{"location":"how-to-guides/jupyter-vscode-directory/#prerequisites","title":"Prerequisites","text":"<ul> <li>Ensure you have Visual Studio Code installed.</li> <li>Ensure the Jupyter extension for VS Code is installed and enabled.</li> </ul>"},{"location":"how-to-guides/jupyter-vscode-directory/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li> <p>Open VS Code Settings in JSON Format</p> </li> <li> <p>Open VS Code.</p> </li> <li>Access the Command Palette by pressing <code>Ctrl+Shift+P</code> on      Windows/Linux or <code>Cmd+Shift+P</code> on macOS.</li> <li> <p>Type <code>Open Settings (JSON)</code> and select the option that appears to      open your settings file in JSON format.</p> </li> <li> <p>Configure the Start-up Directory</p> </li> <li> <p>In the <code>settings.json</code> file, look for the      <code>jupyter.notebookFileRoot</code> setting. If it's not present, you'll      need to add it.</p> </li> <li> <p>Insert or modify the line as follows:</p> <pre><code>\"jupyter.notebookFileRoot\": \"${workspaceFolder}\",\n</code></pre> </li> </ol> <p>This setting instructs VS Code to use the workspace folder (the    project root directory) as the default directory for Jupyter    notebooks. The workspace folder is determined by the location of your    <code>.code-workspace</code> file or the primary folder you have opened in VS    Code.</p> <ol> <li> <p>Save the Changes</p> </li> <li> <p>After adding or modifying the line, save the <code>settings.json</code> file.</p> </li> <li>Close and reopen any active Jupyter notebooks for the changes to      take effect.</li> </ol>"},{"location":"how-to-guides/jupyter-vscode-directory/#conclusion","title":"Conclusion","text":"<p>By following these steps, you have successfully configured Visual Studio Code to set the project's root directory as the default start-up directory for Jupyter notebooks. This configuration streamlines your workflow by removing the need to frequently adjust paths within your notebooks. Now, you can focus more on your data analysis or development tasks without the hassle of directory-related issues.</p>"},{"location":"how-to-guides/lifecycle-git-github/","title":"Comprehensive Guide to AI/ML Project Life-Cycle Using Git and GitHub","text":""},{"location":"how-to-guides/lifecycle-git-github/#overview","title":"Overview","text":"<p>Effective use of Git and GitHub is essential for collaboration and maintaining a robust codebase in AI/ML projects. This guide outlines best practices for managing the life-cycle of your project using Git and GitHub, ensuring reproducibility, scalability, and seamless collaboration.</p>"},{"location":"how-to-guides/lifecycle-git-github/#steps-in-the-life-cycle","title":"Steps in the Life-Cycle","text":""},{"location":"how-to-guides/lifecycle-git-github/#1-open-a-jira-issue","title":"1. Open a JIRA Issue","text":"<ul> <li>Open a JIRA issue to track your work and get the issue number. If   necessary, create sub-tasks and use their issue numbers for more   granular tracking.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#2-create-a-branch","title":"2. Create a Branch","text":"<ul> <li>Create a branch following the naming standard described in the Git   Branch Naming   Standards.   Including the JIRA number in the branch name aids in navigation across   the repository.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#3-start-exploratory-analysis","title":"3. Start Exploratory Analysis","text":"<ul> <li>Begin your exploratory analysis using a Jupyter notebook. This allows   for quick prototyping and visualization. Before sharing your notebook,   use the Black formatter and the Ruff linter to ensure code quality.   Black ensures consistent code formatting, and Ruff helps with sorting   imports, including docstrings, and other linting tasks.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#4-evaluate-exploration-outcomes","title":"4. Evaluate Exploration Outcomes","text":"<ul> <li>Unsuccessful Experiment: If your exploration/experiment does not   yield a viable solution, document the process and findings in the   notebook. Share the notebook, keep the branch open but unmerged, and   open a Pull Request (PR) for review. Close the JIRA issue.</li> <li>Successful Experiment: If your experiment is successful and needs   to be tested in production, proceed with the next steps to transition   from exploratory code to production-ready code.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#5-transition-to-python-script","title":"5. Transition to Python Script","text":"<ul> <li>Move your successful exploratory code from notebooks to Python scripts   (.py). Design your classes, methods, and modules carefully. Ensure   proper documentation, formatting, linting, testing, commenting,   refactoring, and error handling to prepare the code for production.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#6-document-your-code","title":"6. Document Your Code","text":"<ul> <li>If the code is moving to production, document your class and methods   using MkDocs. Refer to Build Your Python Project Documentation With   MkDocs   for guidance. Incorporate the generated documentation into the <code>docs</code>   folder.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#7-open-a-pull-request","title":"7. Open a Pull Request","text":"<ul> <li>Open a Pull Request, undergo peer review, merge the branch into the   production branch, close the JIRA issue, and delete the branch.</li> </ul>"},{"location":"how-to-guides/lifecycle-git-github/#life-cycle-flow-chart","title":"Life-Cycle Flow Chart","text":"<pre><code>---\ntitle: AI/ML Project Life-Cycle\n---\nflowchart TB\n    A(Open a JIRA Issue) --&gt; B(Create a Branch)\n    B --&gt; C(Start Exploratory Analysis)\n    C --&gt; D{Is the Experiment Successful?}\n    D --&gt; |No| E(Document and Share Notebook)\n    E --&gt; I(Open a Pull Request)\n    D --&gt; |Yes| F(Transition to Python Script)\n    F --&gt; G(Document Your Code)\n    G --&gt; H(Open a Pull Request)\n    H --&gt; J(Merge Code into Production)</code></pre>"},{"location":"how-to-guides/machine-learning-metadata-automation/","title":"Automating Machine Learning Model Metadata Creation","text":"<p>Automate Metadata Creation using <code>JSON</code> files</p> <p>This guide provides a step-by-step approach to automate the creation of metadata for machine learning models, using a Python script. It includes an example script that trains a linear regression model, saves it, and generates a corresponding JSON metadata file.</p>"},{"location":"how-to-guides/machine-learning-metadata-automation/#overview","title":"Overview","text":"<ul> <li>The script demonstrates training a linear regression model using Scikit-Learn, saving the model as a <code>.pkl</code> file, and generating a metadata file in JSON format.</li> <li>The metadata includes model parameters, performance metrics, and data description.</li> </ul>"},{"location":"how-to-guides/machine-learning-metadata-automation/#example-script","title":"Example Script","text":"<pre><code>import json\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.datasets import load_boston\nimport joblib\n\n# Load sample data\ndata = load_boston()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict using the model\ny_pred = model.predict(X_test)\n\n# Calculate performance metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Save the model\nmodel_filename = 'linear_regression_model.pkl'\njoblib.dump(model, model_filename)\n\n# Metadata\nmetadata = {\n    'model_name': 'Linear Regression',\n    'timestamp': '20240123',\n    'model_parameters': model.get_params(),\n    'performance_metrics': {\n        'mean_squared_error': mse,\n        'r2_score': r2\n    },\n    'data_description': 'Boston housing dataset',\n    'feature_names': data.feature_names.tolist(),\n    'target_name': 'Housing Price'\n}\n\n# Save metadata to a JSON file\nmetadata_filename = 'service_sage_v1.2.0_linearReg_20240123_metadata.json'\nwith open(metadata_filename, 'w') as f:\n    json.dump(metadata, f, indent=4)\n\nprint(f\"Model and metadata saved as {model_filename} and {metadata_filename} respectively.\")\n</code></pre>"},{"location":"how-to-guides/machine-learning-metadata-automation/#script-explanation","title":"Script Explanation","text":"<ul> <li>The Boston housing dataset is used for demonstration purposes.</li> <li>A linear regression model is trained on the dataset.</li> <li>Performance metrics like Mean Squared Error (MSE) and R-squared (R2) are calculated.</li> <li>The model is saved as a <code>.pkl</code> file, and metadata is saved in a <code>.json</code> file.</li> </ul>"},{"location":"how-to-guides/machine-learning-metadata-automation/#hypothetical-output","title":"Hypothetical Output","text":"<pre><code>{\n    \"model_name\": \"Linear Regression\",\n    \"timestamp\": \"20240123\",\n    \"model_parameters\": {\n        \"copy_X\": true,\n        \"fit_intercept\": true,\n        \"n_jobs\": null,\n        \"normalize\": false\n    },\n    \"performance_metrics\": {\n        \"mean_squared_error\": 24.291119474973684,\n        \"r2_score\": 0.6687594935356314\n    },\n    \"data_description\": \"Boston housing dataset\",\n    \"feature_names\": [\n        \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"\n    ],\n    \"target_name\": \"Housing Price\"\n}\n</code></pre>"},{"location":"how-to-guides/metadata-integration-data-analysis/","title":"Integrating Metadata with Data Analysis Tools","text":""},{"location":"how-to-guides/metadata-integration-data-analysis/#step-by-step-guide-for-metadata-integration","title":"Step-by-Step Guide for Metadata Integration","text":"<ol> <li> <p>Selection of Tools: Choose data analysis tools compatible with your metadata format (e.g., Python libraries for JSON/XML).</p> </li> <li> <p>Metadata Reading: Develop scripts or use built-in functions to read metadata into your analysis environment.</p> </li> <li> <p>Linking Data and Metadata: Ensure a seamless connection between metadata and the corresponding data sets.</p> </li> <li> <p>Utilizing Metadata in Analysis: Use metadata to inform data preprocessing, analysis choices, and interpretation.</p> </li> <li> <p>Metadata-Driven Workflows: Create workflows where metadata dictates certain analysis paths or decisions.</p> </li> <li> <p>Updating Metadata Post-Analysis: After analysis, update metadata to include new insights or derived data characteristics.</p> </li> <li> <p>Version Control: Use version control systems to track changes in both data and metadata.</p> </li> <li> <p>Collaboration: Share metadata along with data among team members to ensure consistent understanding and analysis approaches.</p> </li> <li> <p>Documentation of Process: Document how metadata is used in the analysis process, enhancing reproducibility.</p> </li> <li> <p>Feedback Loop: Establish a feedback mechanism to continually improve metadata usage in data analysis.</p> </li> </ol> Example: Python Script for Metadata Integration<pre><code>import json\nimport pandas as pd\nfrom datetime import datetime\n\ndef generate_metadata(csv_file_path):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Extracting information\n    file_name = csv_file_path.split('/')[-1]\n    creation_date = datetime.now().strftime(\"%Y-%m-%d\")\n    number_of_rows = df.shape[0]\n    number_of_columns = df.shape[1]\n    columns = [{\"name\": col, \"type\": str(df[col].dtype)} for col in df.columns]\n\n    # Metadata dictionary\n    metadata = {\n        \"file_name\": file_name,\n        \"creation_date\": creation_date,\n        \"source\": \"Specify the data source\",\n        \"number_of_rows\": number_of_rows,\n        \"number_of_columns\": number_of_columns,\n        \"columns\": columns,\n        \"preprocessing\": [],  # Add any preprocessing steps manually or through code\n        \"notes\": \"Add any additional notes here\"\n    }\n\n    # Saving metadata to a JSON file\n    with open(file_name.replace('.csv', '_metadata.json'), 'w') as json_file:\n        json.dump(metadata, json_file, indent=4)\n\n# Example usage\ngenerate_metadata('path/to/your/sales_data.csv')\n</code></pre>"},{"location":"how-to-guides/metadata-integration-data-analysis/#collaborative-metadata-management","title":"Collaborative Metadata Management","text":""},{"location":"how-to-guides/metadata-integration-data-analysis/#strategies-for-team-based-metadata-handling","title":"Strategies for Team-Based Metadata Handling","text":"<ol> <li> <p>Centralized Metadata Repository: Establish a central repository for metadata, accessible to all team members.</p> </li> <li> <p>Standardization of Formats: Agree on standardized metadata formats to ensure consistency across different datasets.</p> </li> <li> <p>Regular Updates and Reviews: Implement a schedule for regular metadata updates and reviews by team members.</p> </li> <li> <p>Role-Based Access: Define roles and corresponding access levels for different team members in the metadata repository.</p> </li> <li> <p>Integration with Collaboration Tools: Integrate metadata management with existing collaboration tools (e.g., version control systems, project management software).</p> </li> <li> <p>Training Sessions: Conduct training sessions to familiarize team members with metadata standards and tools.</p> </li> <li> <p>Feedback Mechanisms: Implement mechanisms for team members to provide feedback on metadata usage and management.</p> </li> <li> <p>Audit Trails: Maintain audit trails for metadata changes to track modifications and the responsible parties.</p> </li> <li> <p>Continuous Improvement: Regularly evaluate and improve the metadata management process based on team feedback and changing project needs.</p> </li> <li> <p>Best Practices Documentation: Document best practices for metadata management and ensure they are readily accessible to the team.</p> </li> </ol>"},{"location":"how-to-guides/ml-data-folder-naming/","title":"Naming Conventions for Data Folders in ML Projects","text":"<p>Overview</p> <p>In any Machine Learning (ML) project, organizing and managing data efficiently is crucial. Adopting clear, consistent naming conventions for data folders aids in better data management, collaboration, and project comprehension. This guide provides guidelines for naming data folders in ML projects, ensuring a well-structured and manageable data environment.</p> <p>Folder Structure and Naming Convention</p> <p>Organize the data folder structure into categories based on their role in the data processing pipeline:</p> <pre><code>data\n\u251c\u2500\u2500 raw                   # Original, unaltered data\n\u251c\u2500\u2500 external              # Data from third-party sources\n\u251c\u2500\u2500 interim               # Partially processed data\n\u251c\u2500\u2500 processed             # Data ready for analysis\n\u2514\u2500\u2500 features              # Features for model training\n</code></pre> <p>Explanation of Categories</p> <ul> <li>Raw: Original datasets, unaltered and serving as a backup.</li> <li>External: Data from outside sources, including third-party data and APIs.</li> <li>Interim: Data in the process of cleaning or transformation.</li> <li>Processed: The final dataset version, ready for analysis or modeling.</li> <li>Features: Feature sets for ML models, including transformed and selected features.</li> </ul> <p>Conclusion</p> <p>Adopting these naming conventions for data folders in your ML projects will lead to a more organized, accessible, and efficient data management process, crucial for any ML project's success.</p>"},{"location":"how-to-guides/ml-data-naming-conventions/","title":"Data Files Naming Conventions in ML Projects","text":""},{"location":"how-to-guides/ml-data-naming-conventions/#overview","title":"Overview","text":"<p>Proper naming conventions for data files are essential in Machine Learning (ML) projects to ensure easy identification, management, and tracking of datasets. This guide provides a structured approach to naming data files, particularly when handling multiple versions, subsets, or types of data.</p> <p>Naming Convention Structure</p> <p>Data file names should follow this format: <pre><code>&lt;dataset_name&gt;_&lt;version&gt;_&lt;creation_date&gt;_&lt;description&gt;.&lt;extension&gt;\n</code></pre> Components</p> <ul> <li>Dataset Name: A concise identifier for the dataset.</li> <li>Version: Version number or identifier of the dataset.</li> <li>Creation Date: Date when the dataset was created or last modified, in the format <code>YYYYMMDD</code>.</li> <li>Description: A brief, clear description of the dataset or its specific subset.</li> <li>Extension: The appropriate file extension (e.g., <code>.csv</code>, <code>.xlsx</code>, <code>.json</code>).</li> </ul> <p>Guidelines</p> <ol> <li>Clarity and Descriptiveness: Ensure the name is descriptive enough to give an immediate understanding of the dataset\u2019s content and scope.</li> <li>Consistency: Maintain consistency in the naming convention across all data files. This includes consistent use of underscores, date formats, and versioning systems.</li> <li>Versioning: Use a logical versioning system, like semantic versioning (e.g., <code>v1.0</code>, <code>v1.1</code>, <code>v2.0</code>) or sequential numbering (<code>01</code>, <code>02</code>, etc.).</li> <li>Date Format: Stick to a standard date format (<code>YYYYMMDD</code>). This avoids ambiguity and makes it easy to sort files chronologically.</li> <li>Concise Descriptions: Keep the description part brief yet informative. Avoid overly long names but provide enough context to distinguish the dataset.</li> <li>File Extensions: Use appropriate file extensions to indicate the file type, which helps in quickly identifying the software or tools needed to open them.</li> </ol> <p>Examples</p> <pre><code>- `customer_data_v1.0_20240101_initial.csv`\n- `sales_report_v2.2_20240305_updated.xlsx`\n- `image_dataset_v1.0_20240220_raw.json`\n</code></pre>"},{"location":"how-to-guides/ml-data-naming-conventions/#conclusion","title":"Conclusion","text":"<p>Adhering to these naming conventions for data files in ML projects will significantly enhance data manageability. It ensures ease of access, effective version control, and clear understanding, facilitating efficient data analysis and collaboration within the team.</p>"},{"location":"how-to-guides/notebook-to-production/","title":"Steps to Move a Prototype from Jupyter Notebook to Production-Ready Python Class","text":""},{"location":"how-to-guides/notebook-to-production/#1-design-the-class-and-methods","title":"1. Design the Class and Methods","text":"<p>a. Identify Functionality:</p> <ul> <li>Review the notebook and identify the main functionalities that need      to be encapsulated within the class.</li> <li>Group related functions and processes logically.</li> </ul> <p>b. Define Class:</p> <ul> <li>Create a class that represents the main entity or process of your      ML/AI project.</li> <li>Ensure the class name is descriptive and adheres to naming      conventions (e.g., CamelCase).</li> </ul> <p>c. Design Methods:</p> <ul> <li>Break down the identified functionalities into methods. Each method      should have a single responsibility.</li> <li>Consider the inputs and outputs of each method and how they      interact with each other.</li> </ul> <p>Example:</p> <pre><code>class DataPreprocessor:\n    \"\"\"\n    A class to handle data preprocessing tasks for ML models.\n\n    Methods:\n        __init__: Initializes the DataPreprocessor with data source.\n        load_data: Loads data from the source.\n        clean_data: Cleans the loaded data.\n        transform_data: Transforms the data for model training.\n    \"\"\"\n\n    def __init__(self, data_source: str):\n        \"\"\"\n        Initializes the DataPreprocessor with a data source.\n\n        Args:\n            data_source (str): Path to the data source.\n        \"\"\"\n        self.data_source = data_source\n        self.data = None\n\n    def load_data(self) -&gt; None:\n        \"\"\"Loads data from the source.\"\"\"\n        # Implementation\n        pass\n\n    def clean_data(self) -&gt; None:\n        \"\"\"Cleans the loaded data.\"\"\"\n        # Implementation\n        pass\n\n    def transform_data(self) -&gt; None:\n        \"\"\"Transforms the data for model training.\"\"\"\n        # Implementation\n        pass\n</code></pre>"},{"location":"how-to-guides/notebook-to-production/#2-module-naming","title":"2. Module Naming","text":"<p>a. Naming Conventions:</p> <ul> <li>Use meaningful and descriptive names for your module and class.      Follow PEP 8 guidelines for naming conventions.</li> </ul> <p>Example:</p> <pre><code># Filename: data_preprocessor.py\nclass DataPreprocessor:\n    ...\n</code></pre>"},{"location":"how-to-guides/notebook-to-production/#3-formatting","title":"3. Formatting","text":"<p>a. Use Code Formatters:</p> <ul> <li>Use Black to ensure consistent code style. Black formats the code      according to PEP 8 guidelines, making it readable and maintainable.</li> </ul> <p>Command: <pre><code>black data_preprocessor.py\n</code></pre></p>"},{"location":"how-to-guides/notebook-to-production/#4-linting","title":"4. Linting","text":"<p>a. Use Linters:</p> <ul> <li>Use Ruff to identify and fix potential issues. Ruff enforces code      quality by checking for syntax errors, code smells, and enforcing      coding standards.</li> </ul> <p>Command: <pre><code>ruff data_preprocessor.py\n</code></pre></p>"},{"location":"how-to-guides/notebook-to-production/#5-testing","title":"5. Testing","text":"<p>a. Write Unit Tests:</p> <ul> <li>Use <code>unittest</code> or <code>pytest</code> to write tests for each method in your      class.</li> <li>Ensure comprehensive test coverage to catch edge cases and ensure      robustness.</li> </ul> <p>Example:</p> <pre><code>import unittest\nfrom data_preprocessor import DataPreprocessor\n\nclass TestDataPreprocessor(unittest.TestCase):\n\n    def setUp(self):\n        self.preprocessor = DataPreprocessor(\"data_source_path\")\n\n    def test_load_data(self):\n        self.preprocessor.load_data()\n        self.assertIsNotNone(self.preprocessor.data)\n\n    def test_clean_data(self):\n        self.preprocessor.load_data()\n        self.preprocessor.clean_data()\n        # Add assertions to check if data is cleaned\n\n    def test_transform_data(self):\n        self.preprocessor.load_data()\n        self.preprocessor.clean_data()\n        self.preprocessor.transform_data()\n        # Add assertions to check if data is transformed\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>"},{"location":"how-to-guides/notebook-to-production/#6-commenting-and-docstrings","title":"6. Commenting and Docstrings","text":"<p>a. Add Docstrings:</p> <ul> <li>Use Google-style docstrings for classes and methods. This improves      readability and helps in generating documentation.</li> </ul> <p>Example:</p> <pre><code>class DataPreprocessor:\n    \"\"\"\n    A class to handle data preprocessing tasks for ML models.\n\n    Methods:\n        __init__: Initializes the DataPreprocessor with data source.\n        load_data: Loads data from the source.\n        clean_data: Cleans the loaded data.\n        transform_data: Transforms the data for model training.\n    \"\"\"\n\n    def __init__(self, data_source: str):\n        \"\"\"\n        Initializes the DataPreprocessor with a data source.\n\n        Args:\n            data_source (str): Path to the data source.\n        \"\"\"\n        self.data_source = data_source\n        self.data = None\n\n    def load_data(self) -&gt; None:\n        \"\"\"\n        Loads data from the source.\n\n        Raises:\n            FileNotFoundError: If the data source is not found.\n        \"\"\"\n        # Implementation\n        pass\n\n    def clean_data(self) -&gt; None:\n        \"\"\"\n        Cleans the loaded data.\n\n        Returns:\n            None\n        \"\"\"\n        # Implementation\n        pass\n\n    def transform_data(self) -&gt; None:\n        \"\"\"\n        Transforms the data for model training.\n\n        Returns:\n            None\n        \"\"\"\n        # Implementation\n        pass\n</code></pre>"},{"location":"how-to-guides/notebook-to-production/#7-refactoring","title":"7. Refactoring","text":"<p>a. Improve Code Structure:</p> <ul> <li>Refactor code to improve readability and maintainability.</li> <li>Apply the Single Responsibility Principle and other SOLID      principles.</li> </ul> <p>b. Remove Duplications:</p> <ul> <li>Ensure there are no code duplications and apply the DRY (Don't      Repeat Yourself) principle.</li> </ul>"},{"location":"how-to-guides/notebook-to-production/#8-debugging","title":"8. Debugging","text":"<p>a. Use Debuggers:</p> <ul> <li>Use debugging tools to identify and fix issues in the code.</li> <li>Utilize breakpoints and inspect variables to trace the execution      flow.</li> </ul> <p>b. Logging:</p> <ul> <li>Implement logging to help with debugging in production. Use      appropriate log levels (INFO, DEBUG, ERROR).</li> </ul> <p>Example: <pre><code>import logging\n\nlogging.basicConfig(level=logging.INFO)\n\nclass DataPreprocessor:\n    ...\n    def load_data(self) -&gt; None:\n        \"\"\"Loads data from the source.\"\"\"\n        try:\n            logging.info(\"Loading data from source.\")\n            # Implementation\n        except FileNotFoundError as e:\n            logging.error(f\"File not found: {e}\")\n            raise\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n            raise\n</code></pre></p>"},{"location":"how-to-guides/notebook-to-production/#9-error-handling","title":"9. Error Handling","text":"<p>a. Implement Error Handling:</p> <ul> <li>Use try-except blocks to handle potential errors gracefully.</li> <li>Provide meaningful error messages and log the errors.</li> </ul> <p>Example:</p> <pre><code>    def load_data(self) -&gt; None:\n        \"\"\"\n        Loads data from the source.\n\n        Raises:\n            FileNotFoundError: If the data source is not found.\n            Exception: For other generic errors.\n        \"\"\"\n        try:\n            logging.info(\"Loading data from source.\")\n            # Implementation\n        except FileNotFoundError as e:\n            logging.error(f\"File not found: {e}\")\n            raise\n        except Exception as e:\n            logging.error(f\"An error occurred: {e}\")\n            raise\n</code></pre>"},{"location":"how-to-guides/notebook-to-production/#10-final-review-and-integration","title":"10. Final Review and Integration","text":"<p>a. Peer Review:</p> <ul> <li>Submit the code for peer review to ensure code quality and      correctness.</li> <li>Use tools like GitHub or GitLab for code reviews.</li> </ul> <p>b. Integration:</p> <ul> <li>Integrate the code into the main codebase following the branching      strategy and version control best practices.</li> <li>Ensure all tests pass before merging.</li> </ul> <p>By following these steps, you can ensure that your code transitions smoothly from a prototype in a Jupyter notebook to a well-structured, maintainable, and production-ready Python class. This process emphasizes best practices in software engineering and machine learning, promoting collaboration and reproducibility in your projects.</p>"},{"location":"how-to-guides/offline-cookiecutter-setup/","title":"Using Cookiecutter Without Direct GitHub Command","text":"<p>Goal</p> <p>Learn how to manually download and use a Cookiecutter template from GitHub when direct command line access to GitHub (<code>cookiecutter gh:...</code>) is not available.</p>"},{"location":"how-to-guides/offline-cookiecutter-setup/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of command line operations.</li> <li>Python and pip installed on your system.</li> <li>Access to GitHub through a web browser.</li> </ul>"},{"location":"how-to-guides/offline-cookiecutter-setup/#steps","title":"Steps","text":""},{"location":"how-to-guides/offline-cookiecutter-setup/#1-downloading-the-cookiecutter-template","title":"1. Downloading the Cookiecutter Template","text":"Find and Download the Repository <ul> <li>Download the Repository: Click on the 'Code' button, select 'Download ZIP', save, and extract it to a known location on your computer.</li> </ul>"},{"location":"how-to-guides/offline-cookiecutter-setup/#2-setting-up-your-environment","title":"2. Setting Up Your Environment","text":"<pre><code># Install Cookiecutter if not already installed\npip install cookiecutter\n\n# Navigate to your desired project directory\ncd path/to/your/Projects\n</code></pre> <p>Warning</p> <p>Replace <code>path/to/your/Projects</code> with the actual path where you want to create your project.</p>"},{"location":"how-to-guides/offline-cookiecutter-setup/#3-creating-your-project","title":"3. Creating Your Project","text":"Run Cookiecutter and Input Project Details <p>Run Cookiecutter and provide the path to the unzipped template folder:</p> <pre><code>cookiecutter path/to/unzipped/cookiecutter-collabora\n</code></pre> <p>Replace <code>path/to/unzipped/cookiecutter-collabora</code> with the actual path to the unzipped template. Follow the prompts to input details for your new project.</p>"},{"location":"how-to-guides/offline-cookiecutter-setup/#4-finalizing-the-project","title":"4. Finalizing the Project","text":"<p>After running the command, a new directory with your project name will be created. Navigate to this directory to perform any additional setup required by the template.</p> <p>Conclusion</p> <p>You've successfully used a Cookiecutter template without direct GitHub command line integration. This approach is useful in environments with restricted internet access or limitations in using GitHub tools directly.</p>"},{"location":"how-to-guides/pre-commit-hooks-guide/","title":"Using Pre-Commit Hooks to Enforce Coding Standards","text":"<p>Interacting with Pre-Commit Hooks</p> <p>To ensure code quality and adherence to coding standards, you must use the terminal for all Git operations. The pre-commit hooks are automatically triggered during these operations, so it's crucial to follow these steps:</p> <ol> <li>Install Pre-Commit Hooks (This step is needed only once, when you clone the repo and follow the installation steps):</li> </ol> <pre><code>pre-commit install\n</code></pre> <ol> <li>Run Pre-Commit Hooks on All Files:</li> </ol> <pre><code>pre-commit run --all-files\n</code></pre> <ol> <li>Add and Commit Changes:</li> </ol> <p>After fixing any issues reported by the pre-commit hooks, use the following commands:</p> <pre><code>git add &lt;fixed_files&gt;\ngit commit -m \"your commit message\"\n</code></pre> <ol> <li>Avoid Using the VS Code Version Control Pane:</li> </ol> <p>Ensure you perform all Git operations through the terminal to properly trigger and handle pre-commit hooks.</p>"},{"location":"how-to-guides/pre-commit-hooks-guide/#introduction","title":"Introduction","text":"<p>Pre-commit hooks are a powerful tool to enforce coding standards and ensure code quality before changes are committed to the repository. These hooks automatically run checks and tasks, such as linting, formatting, running tests, and verifying branch names. This document provides an overview of the pre-commit hooks implemented in our project and how to use them effectively.</p>"},{"location":"how-to-guides/pre-commit-hooks-guide/#pre-commit-hooks-overview","title":"Pre-Commit Hooks Overview","text":"<p>The following table provides a synthetic description of each pre-commit hook, including whether you need to repeat the <code>git add</code> and <code>git commit</code> commands after fixing issues:</p> Name Description Repeat git add/commit Run PyTest Runs PyTest to execute all tests in the <code>tests/</code> directory. Yes Commit Message Check Ensures commit messages follow the specified format. No Restricted File and Section Check Prevents modifications to restricted files. Yes Filename Snake Case Check Verifies that filenames follow the snake_case convention. Yes Generate Documentation Generates project documentation and stages it for commit. No Branch Name Check Validates branch names against the specified pattern. No ruff-linter Lints Python code using Ruff. Yes black-formatter Formats Python code using Black. Yes black-jupyter-formatter Formats Jupyter notebooks using Black. Yes trailing-whitespace Trims trailing whitespace from files. Yes end-of-file-fixer Ensures files end with a newline. Yes check-added-large-files Warns if large files are added to the commit. Yes debug-statements Detects and prevents debug statements in the code. Yes detect-private-key Prevents private keys from being committed. Yes prettier Formats YAML files using Prettier. Yes mypy Checks static typing using Mypy. Yes"},{"location":"how-to-guides/pre-commit-hooks-guide/#using-the-terminal-for-git-and-pre-commit-hooks","title":"Using the Terminal for Git and Pre-Commit Hooks","text":"<p>To ensure that pre-commit hooks run correctly, it is recommended to use the terminal for all Git operations. Avoid using the VS Code version control pane, as it may not properly trigger the hooks.</p>"},{"location":"how-to-guides/pre-commit-hooks-guide/#detailed-descriptions-and-examples","title":"Detailed Descriptions and Examples","text":""},{"location":"how-to-guides/pre-commit-hooks-guide/#1-run-pytest","title":"1. Run PyTest","text":"<p>Description: Runs all tests in the <code>tests/</code> directory using PyTest.</p> <p>Example: If a test fails, you will see an error message indicating which test failed. Fix the issue, stage the changes, and commit again.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre> <p>Error:  <pre><code>============================= test session starts ==============================\nplatform linux -- Python 3.10.0, pytest-6.2.4, py-1.10.0, pluggy-0.13.1\nrootdir: /path/to/repo\ncollected 2 items                                                              \n\ntests/test_sample.py .F                                                  [100%]\n\n=================================== FAILURES ===================================\n______________________________ test_function_fail ______________________________\n\n    def test_function_fail():\n&gt;       assert 1 == 2\nE       assert 1 == 2\n\ntests/test_sample.py:10: AssertionError\n</code></pre></p>"},{"location":"how-to-guides/pre-commit-hooks-guide/#2-commit-message-check","title":"2. Commit Message Check","text":"<p>Description: Ensures commit messages follow the specified format.</p> <p>Example: If the commit message format is incorrect, you will receive an error message. Correct the commit message and try again.</p> <pre><code>git commit -m \"feat(auth): add user authentication feature [#DATA123]\"\n</code></pre> <p>Error: <pre><code>Error: Commit message must follow the format '&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; [#JIRA-123]'\nExample: feat(auth): add user authentication feature [#DATA-123]\n</code></pre></p>"},{"location":"how-to-guides/pre-commit-hooks-guide/#3-restricted-file-and-section-check","title":"3. Restricted File and Section Check","text":"<p>Description: Prevents modifications to restricted files.</p> <p>Example: If a restricted file is modified, an error will be displayed. Revert the changes to the restricted file and commit again.</p> <pre><code>Error: Modifications to '.vscode/settings.json' are not allowed.\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#4-filename-snake-case-check","title":"4. Filename Snake Case Check","text":"<p>Description: Verifies that filenames follow the snake_case convention.</p> <p>Example: If a filename does not follow the snake_case convention, you will receive an error. Rename the file accordingly and commit again.</p> <pre><code>Error: Filename 'MyFile.py' does not follow the snake_case naming convention.\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#5-generate-documentation","title":"5. Generate Documentation","text":"<p>Description: Generates project documentation and stages it for commit.</p> <p>Example: If the documentation generation fails, fix the issues and run the command again.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#6-branch-name-check","title":"6. Branch Name Check","text":"<p>Description: Validates branch names against the specified pattern.</p> <p>Example: If the branch name does not follow the pattern, an error will be displayed. Create a new branch with the correct naming convention.</p> <pre><code>git checkout -b \"feature/add-user-authentication-DATA123\"\n</code></pre> <p>Error: <pre><code>Error: Branch name must follow the pattern '&lt;category&gt;/&lt;description&gt;-&lt;JIRA_KEY&gt;'\nExample: feature/user-authentication-DATA-123\nDeleting branch: invalid-branch-name\n</code></pre></p>"},{"location":"how-to-guides/pre-commit-hooks-guide/#7-ruff-linter","title":"7. ruff-linter","text":"<p>Description: Lints Python code using Ruff.</p> <p>Example: If the linter finds issues, fix them and commit again.</p> <pre><code>ruff check --preview --fix project_directory/utils/advanced_parser.py\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#8-black-formatter","title":"8. black-formatter","text":"<p>Description: Formats Python code using Black.</p> <p>Example: If the formatter makes changes, stage the changes and commit again.</p> <pre><code>black sample_script.py\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#9-black-jupyter-formatter","title":"9. black-jupyter-formatter","text":"<p>Description: Formats Jupyter notebooks using Black.</p> <p>Example: If the formatter makes changes, stage the changes and commit again.</p> <pre><code>black sample_notebook.ipynb\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#10-trailing-whitespace","title":"10. Trailing Whitespace","text":"<p>Description: Trims trailing whitespace from files.</p> <p>Example: If trailing whitespace is found and removed, stage the changes and commit again.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#11-end-of-file-fixer","title":"11. End of File Fixer","text":"<p>Description: Ensures files end with a newline.</p> <p>Example: If a file does not end with a newline, fix the issue, stage the changes, and commit again.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#12-check-added-large-files","title":"12. Check Added Large Files","text":"<p>Description: Warns if large files are added to the commit.</p> <p>Example: If a large file is detected, you will receive a warning. Decide whether to proceed with the commit or remove the large file.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#13-debug-statements","title":"13. Debug Statements","text":"<p>Description: Detects and prevents debug statements in the code.</p> <p>Example: If debug statements are found, remove them and commit again.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#14-detect-private-key","title":"14. Detect Private Key","text":"<p>Description: Prevents private keys from being committed.</p> <p>Example: If a private key is detected, remove it and commit again.</p> <pre><code>pre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#15-prettier","title":"15. Prettier","text":"<p>Description: Formats YAML files using Prettier.</p> <p>Example: If the formatter makes changes, stage the changes and commit again.</p> <pre><code>prettier --write config.yaml\n</code></pre>"},{"location":"how-to-guides/pre-commit-hooks-guide/#16-mypy","title":"16. Mypy","text":"<p>Description: Checks static typing using Mypy.</p> <p>Example: If Mypy finds type errors, fix them and commit again.</p> <pre><code>mypy my_script.py\n</code></pre> <p>By following these guidelines and using the pre-commit hooks, you can ensure that your code adheres to the project's coding standards and that any issues are caught early in the development process.</p>"},{"location":"how-to-guides/project-scaffolding-standards/","title":"Project Scaffolding Standards for AI/ML Projects","text":""},{"location":"how-to-guides/project-scaffolding-standards/#overview","title":"Overview","text":"<p>Establishing a standardized project structure is essential for maintaining organization, enhancing collaboration, and ensuring consistency across all AI/ML projects. This guide outlines the recommended folder and file structure to follow for all projects within the AI team.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#folder-and-file-structure","title":"Folder and File Structure","text":"<p>Here is the standardized folder and file structure for AI/ML projects:</p>"},{"location":"how-to-guides/project-scaffolding-standards/#project-structure","title":"Project Structure","text":"<pre><code>\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 CONTRIBUTING.md             &lt;- Guidelines for contributing to the project.\n\u2502   \u251c\u2500\u2500 PULL_REQUEST_TEMPLATE\n\u2502   \u2502   \u2514\u2500\u2500 pull_request_template.md\n\u2502   \u2514\u2500\u2500 workflows\n\u2502       \u251c\u2500\u2500 black.yaml              &lt;- GitHub Actions workflow for Black formatter.\n\u2502       \u2514\u2500\u2500 ruff.yaml               &lt;- GitHub Actions workflow for Ruff linter.\n\u251c\u2500\u2500 .gitignore                      &lt;- Specifies intentionally untracked files to ignore.\n\u251c\u2500\u2500 .vscode\n\u2502   \u251c\u2500\u2500 cspell.json                 &lt;- Spell checker configuration.\n\u2502   \u251c\u2500\u2500 dictionaries\n\u2502   \u2502   \u2514\u2500\u2500 data-science-en.txt     &lt;- Custom dictionary for spell checker.\n\u2502   \u251c\u2500\u2500 extensions.json             &lt;- List of recommended VS Code extensions.\n\u2502   \u2514\u2500\u2500 settings.json               &lt;- VS Code workspace settings.\n\u251c\u2500\u2500 .env                            &lt;- Environment variables configuration file.\n\u251c\u2500\u2500 README.md                       &lt;- The top-level README for developers using this project.\n\u251c\u2500\u2500 Makefile                        &lt;- Makefile with commands like `make data` or `make train`.\n\u251c\u2500\u2500 config                          &lt;- Configuration files for the project.\n\u2502   \u251c\u2500\u2500 base_config.cfg\n\u2502   \u251c\u2500\u2500 config.cfg\n\u2502   \u2514\u2500\u2500 settings.yaml\n\u251c\u2500\u2500 data                            &lt;- Data files for the project.\n\u2502   \u251c\u2500\u2500 raw                         &lt;- Raw data files.\n\u2502   \u2514\u2500\u2500 processed                   &lt;- Processed data files.\n\u251c\u2500\u2500 docs                            &lt;- Documentation files for the project.\n\u2502   \u251c\u2500\u2500 api-reference.md\n\u2502   \u251c\u2500\u2500 explanation.md\n\u2502   \u251c\u2500\u2500 how-to-guides.md\n\u2502   \u251c\u2500\u2500 index.md\n\u2502   \u251c\u2500\u2500 tutorials.md\n\u2502   \u2514\u2500\u2500 assets                      &lt;- Assets for documentation (images, CSS, etc.).\n\u2502       \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 logs                            &lt;- Logs generated by the project.\n\u2502   \u2514\u2500\u2500 application.log\n\u251c\u2500\u2500 mkdocs.yml                      &lt;- Configuration for mkdocs documentation generation.\n\u251c\u2500\u2500 notebooks                       &lt;- Jupyter notebooks for exploration and analysis.\n\u2502   \u2514\u2500\u2500 ...                         &lt;- Individual notebook files.\n\u251c\u2500\u2500 pyproject.toml                  &lt;- Project configuration file for dependencies and tools.\n\u251c\u2500\u2500 poetry.lock                     &lt;- Lock file for poetry to ensure reproducibility.\n\u251c\u2500\u2500 scripts                         &lt;- Utility scripts for various tasks.\n\u2502   \u251c\u2500\u2500 preprocess_data.py          &lt;- Script for data preprocessing\n\u2502   \u251c\u2500\u2500 train_model.py              &lt;- Script for model training\n\u2502   \u251c\u2500\u2500 deploy_model.py             &lt;- Script for model deployment\n\u2502   \u251c\u2500\u2500 evaluate_model.py           &lt;- Script for evaluating model performance\n\u2502   \u2514\u2500\u2500 data_operations.py          &lt;- Script for S3 data operations\n\u251c\u2500\u2500 sagemaker                       &lt;- SageMaker-specific scripts and configurations.\n\u2502   \u251c\u2500\u2500 train_script.py             &lt;- Training script for SageMaker.\n\u2502   \u251c\u2500\u2500 deploy_script.py            &lt;- Deployment script for SageMaker.\n\u2502   \u251c\u2500\u2500 preprocessing_script.py     &lt;- Preprocessing script for SageMaker.\n\u2502   \u251c\u2500\u2500 hyperparameters.json        &lt;- Hyperparameters for SageMaker training jobs.\n\u2502   \u2514\u2500\u2500 sagemaker_config.json       &lt;- Configuration for SageMaker jobs.\n\u251c\u2500\u2500 src                             &lt;- Source code for the project.\n\u2502   \u2514\u2500\u2500 package_name                &lt;- Main package directory.\n\u2502       \u251c\u2500\u2500 __init__.py             &lt;- Module docstring\n\u2502       \u251c\u2500\u2500 data_preprocessing\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py         &lt;- Module docstring\n\u2502       \u2502   \u251c\u2500\u2500 data_reader.py      &lt;- Module for reading original documents\n\u2502       \u2502   \u251c\u2500\u2500 data_cleaning.py    &lt;- Module for cleaning and preprocessing documents\n\u2502       \u2502   \u2514\u2500\u2500 data_chunking.py    &lt;- Module for chunking documents\n\u2502       \u251c\u2500\u2500 embeddings\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py         &lt;- Module docstring\n\u2502       \u2502   \u251c\u2500\u2500 embedding_generator.py &lt;- Module for generating embeddings\n\u2502       \u2502   \u2514\u2500\u2500 embedding_utils.py  &lt;- Utility functions for embeddings\n\u2502       \u251c\u2500\u2500 retrievers\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py         &lt;- Module docstring\n\u2502       \u2502   \u251c\u2500\u2500 retriever.py        &lt;- Module for retrieving relevant chunks\n\u2502       \u2502   \u251c\u2500\u2500 retriever_utils.py  &lt;- Utility functions for retrievers\n\u2502       \u2502   \u2514\u2500\u2500 retriever_preprocessing.py &lt;- Module for pre-processing queries before retrieval\n\u2502       \u251c\u2500\u2500 generators\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py         &lt;- Module docstring\n\u2502       \u2502   \u251c\u2500\u2500 generator.py        &lt;- Module for generating answers from retrieved chunks\n\u2502       \u2502   \u251c\u2500\u2500 generator_utils.py  &lt;- Utility functions for generators\n\u2502       \u2502   \u2514\u2500\u2500 generation_postprocessing.py &lt;- Module for post-processing generated answers\n\u2502       \u251c\u2500\u2500 evaluation\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py         &lt;- Module docstring\n\u2502       \u2502   \u251c\u2500\u2500 evaluation_metrics.py &lt;- Module for evaluating model performance\n\u2502       \u2502   \u2514\u2500\u2500 evaluation_utils.py &lt;- Utility functions for evaluation\n\u2502       \u251c\u2500\u2500 utils\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py         &lt;- Module docstring\n\u2502       \u2502   \u251c\u2500\u2500 logger.py           &lt;- Module for logging\n\u2502       \u2502   \u251c\u2500\u2500 config.py           &lt;- Module for configuration management\n\u2502       \u2502   \u251c\u2500\u2500 constants.py        &lt;- Module for constants used across the project\n\u2502       \u2502   \u2514\u2500\u2500 utils.py            &lt;- General utility functions used across the project\n\u2502       \u251c\u2500\u2500 main.py                 &lt;- Main entry point for the application\n\u2502       \u2514\u2500\u2500 app.py                  &lt;- Application setup and configuration\n\u251c\u2500\u2500 Dockerfile                      &lt;- Dockerfile for containerizing the application.\n\u2514\u2500\u2500 tests                           &lt;- Unit and integration tests.\n    \u251c\u2500\u2500 __init__.py                 &lt;- Module docstring\n    \u251c\u2500\u2500 test_data_preprocessing.py  &lt;- Tests for data preprocessing\n    \u251c\u2500\u2500 test_embeddings.py          &lt;- Tests for embedding generation\n    \u251c\u2500\u2500 test_retrievers.py          &lt;- Tests for retrievers\n    \u2502   \u2514\u2500\u2500 test_retriever_preprocessing.py &lt;- Tests for query pre-processing\n    \u251c\u2500\u2500 test_generators.py          &lt;- Tests for generators\n    \u2502   \u2514\u2500\u2500 test_generation_postprocessing.py &lt;- Tests for post-processing generated answers\n    \u2514\u2500\u2500 test_evaluation.py          &lt;- Tests for evaluation\n</code></pre>"},{"location":"how-to-guides/project-scaffolding-standards/#notes-from-conversation-wth-guy","title":"Notes from conversation wth Guy","text":"<p>I beleive we need a spearated moducle fo the cunking techniques.</p> <p>yes, the data folder must be there because we need local data for you to run evaluation etc. So, update the .gitignore file accordignly. Let's put in the docs the .gitingore that aligns with the above tree.</p> <p>The <code>embedding_generator.py</code> generets the embeddings and push them into the vector database. Mention this. Both actions happens in the same moducle. Generate embeddings and populate them into OpenSearch. Impliciity this module also ingest the metadata.</p> <p>For the retrivers we need to add another script there, one for the pre-processing. We do not need it to populate it in the first interaction but we keep it as placeholder. For example, the pre-processing is useful if we get questions that are not well formed so, if we want to do query expansion we can hadle it in a pre-process module. So, here we are going to do query pre-processing things like. For example query expansion or acronym expansion. So, anything we do with the query before put in into the retrieval.</p> <p>For generation, we need a post-processing module to include thinghs such as content moderation. Or  to include the code aobut showoing the citation.</p> <p>Include a general util for cross function modules maybe the name can be helper.py.</p> <p>Notice that we already have a Docker file there.</p> <p>Docker compose, for backend We need also the requiremts.txt file, the dependencies. The readme.md put links to all data etc....</p> <p>We need the dependicies for the backend.</p> <p>In summary, I need to put there:</p> <ul> <li>README.md</li> <li>CONTRIBUTING.md</li> <li>pyproject.toml (instead of the requirements.txt)</li> <li>.gitignore</li> <li>empty files and folders with the right names.</li> </ul>"},{"location":"how-to-guides/project-scaffolding-standards/#summary-of-changes","title":"Summary of Changes","text":"<ol> <li>.env File: Included for managing environment variables locally.</li> <li>data Folder: Kept for storing raw and processed data files, useful during local development.</li> <li>SageMaker Folder: Added for SageMaker-specific scripts and configurations to manage training and deployment.</li> <li>Scripts Folder: Added for utility scripts that support various tasks such as preprocessing, training, deployment, and data operations.</li> <li>Dockerfile: Included for containerizing the application, ensuring a consistent development and production environment.</li> <li>Additional Documentation Files: Included <code>LICENSE</code> and <code>CODE_OF_CONDUCT.md</code> for legal and community guidelines.</li> </ol> <p>This structure is designed to support development workflows for Data Scientists initially and can be easily extended as Machine Learning Engineers join the project. It also ensures that both local development and deployment to AWS SageMaker are well-organized and manageable. o <pre><code>## Directory Descriptions\n\n- **README.md**: A top-level README file that provides an overview of\n  the project, how to set up and use it, and any other relevant\n  information for developers.\n\n- **Makefile**: Contains commands for setting up and running the\n  project, such as `make data` for data preparation or `make train` for\n  model training.\n\n- **config/**: Contains configuration files used by the project. These\n  files define various settings and parameters needed for the project.\n\n- **dist/**: Stores distribution files for the project, such as wheels\n  and tarballs for package distribution.\n\n- **docs/**: Contains all documentation files, including API references,\n  explanations, how-to guides, and tutorials. The assets subdirectory\n  holds images and CSS used in the documentation.\n\n- **logs/**: Stores log files generated by the project, useful for\n  debugging and monitoring.\n\n- **mkdocs.yml**: Configuration file for generating project\n  documentation using mkdocs.\n\n- **notebooks/**: Contains Jupyter notebooks for exploratory data\n  analysis and experiments.\n\n- **poetry.lock**: Poetry lock file that specifies the exact versions of\n  dependencies used in the project.\n\n- **pyproject.toml**: Project configuration file that defines\n  dependencies, tools, and other settings for the project.\n\n- **src/**: Contains all source code for the project. This includes data\n  management scripts, model definitions, utility scripts, and any other\n  code needed for the project.\n\n- **tests/**: Contains all unit and integration tests for the project.\n  Each test module typically corresponds to a module in the src\n  directory.\n\n- **.devcontainer/**: Configuration for VS Code Dev Containers to ensure\n  a consistent development environment.\n\n- **.github/**: Contains GitHub-specific files for repository\n  management, including issue templates, pull request templates,\n  workflows, and contributing guidelines.\n\n- **.gitignore**: Specifies intentionally untracked files to ignore,\n  helping to keep the repository clean.\n\n- **.vscode/**: Contains settings and extensions for Visual Studio Code\n  to ensure a consistent development environment.\n\n## Example Structure\n\n### Config Directory\n\nThe config directory holds all configuration files required by the\nproject:\n\n```plaintext\nconfig/\n\u251c\u2500\u2500 base_config.cfg\n\u251c\u2500\u2500 config.cfg\n\u2514\u2500\u2500 settings.yaml\n</code></pre></p>"},{"location":"how-to-guides/project-scaffolding-standards/#documentation-directory","title":"Documentation Directory","text":"<p>The docs directory includes various types of documentation and assets:</p> <pre><code>docs/\n\u251c\u2500\u2500 api-reference.md\n\u251c\u2500\u2500 explanation.md\n\u251c\u2500\u2500 how-to-guides.md\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 tutorials.md\n\u2514\u2500\u2500 assets/\n    \u251c\u2500\u2500 images/\n    \u2502   \u2514\u2500\u2500 example.png\n    \u251c\u2500\u2500 css/\n    \u2502   \u2514\u2500\u2500 custom.css\n    \u2514\u2500\u2500 logo.png\n</code></pre>"},{"location":"how-to-guides/project-scaffolding-standards/#source-directory","title":"Source Directory","text":"<p>The src directory contains all the source code organized into subdirectories:</p> <pre><code>src/\n\u251c\u2500\u2500 project_name/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 data/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 external/\n\u2502   \u2502   \u251c\u2500\u2500 features/\n\u2502   \u2502   \u251c\u2500\u2500 interim/\n\u2502   \u2502   \u2514\u2500\u2500 processed/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 model.py\n\u2502   \u251c\u2500\u2500 utils/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 utils.py\n\u2502   \u2514\u2500\u2500 logs/\n\u2502       \u2514\u2500\u2500 application.log\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 test_data.py\n    \u251c\u2500\u2500 test_models.py\n    \u2514\u2500\u2500 test_utils.py\n</code></pre>"},{"location":"how-to-guides/project-scaffolding-standards/#note-on-shared-configuration-files-for-mlai-projects","title":"Note on Shared Configuration Files for ML/AI Projects","text":"<p>To ensure consistency and maintain high standards across all ML/AI projects, specific files have been pre-configured and included in the GitHub repository. These files contain settings and configurations that enforce standardized practices for all developers involved in the project. As such, these files should not be modified by individual developers. The pre-configured files help to ensure that everyone follows the same standards, enhancing collaboration and project management.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#files-that-should-not-be-modified-by-developers","title":"Files That Should Not Be Modified by Developers","text":"<p>The following files and sections are crucial for maintaining the project's standards and should not be altered:</p>"},{"location":"how-to-guides/project-scaffolding-standards/#vscode-folder-content","title":".vscode Folder Content","text":"<p>The <code>.vscode</code> folder contains settings and extensions for Visual Studio Code to ensure a consistent development environment for all team members.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#github-folder-content","title":".github Folder Content","text":"<p>The <code>.github</code> folder includes files for repository management, such as issue templates, pull request templates, workflows, and contributing guidelines.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#devcontainer-folder-content","title":".devcontainer Folder Content","text":"<p>The <code>.devcontainer</code> folder holds configuration files for VS Code Dev Containers, providing a consistent development environment setup.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#pyprojecttoml-sections","title":"pyproject.toml Sections","text":"<p>Specific sections in the <code>pyproject.toml</code> file are configured to enforce coding standards and dependencies. These sections should not be modified:</p> <ul> <li>[tool.poetry.dev-dependencies]</li> <li>[tool.poetry.group.dev.dependencies]</li> <li>[tool.black]</li> <li>[tool.ruff]</li> <li>[tool.pymarkdown]</li> <li>[tool.coverage.report]</li> </ul> <p>By keeping these files and sections untouched, we ensure that all developers are adhering to the same standards, which is critical for maintaining code quality and project consistency. The person who set up the GitHub repository has included these configurations to streamline the development process and enforce best practices.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#summary","title":"Summary","text":"<ul> <li>.vscode Folder: Contains settings and extensions for a consistent   VS Code environment.</li> <li>.github Folder: Includes repository management files and   templates.</li> <li>.devcontainer Folder: Provides configuration for VS Code Dev   Containers.</li> <li>pyproject.toml Sections: Enforces coding standards and   dependencies.</li> </ul> <p>These pre-configured files and settings help to maintain uniformity and high standards across the project, ensuring that all team members follow the same practices. Therefore, developers should not modify these files.</p>"},{"location":"how-to-guides/project-scaffolding-standards/#conclusion","title":"Conclusion","text":"<p>Following this standardized folder and file structure will help maintain consistency and readability across all AI/ML projects, making it easier for team members to collaborate and manage the project effectively. This structure ensures that all necessary components are well-organized and easily accessible.</p>"},{"location":"how-to-guides/pushing-to-githug-best-practices/","title":"Best Practices for Using Git and Pushing to GitHub","text":""},{"location":"how-to-guides/pushing-to-githug-best-practices/#overview","title":"Overview","text":"<p>Effective use of Git and GitHub is crucial for collaboration and maintaining a clean and functional codebase. This guide outlines best practices for committing and pushing code to ensure smooth collaboration and project management.</p>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#commit-frequency","title":"Commit Frequency","text":"<ol> <li>Commit Often: Make small, frequent commits. Each commit should    represent a single logical change or addition.</li> <li>Atomic Commits: Ensure each commit is atomic, meaning it should    contain only related changes. Avoid mixing unrelated changes in a    single commit.</li> <li>Meaningful Commits: Commit only meaningful changes. Avoid    committing broken code or temporary debugging statements.</li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#commit-messages","title":"Commit Messages","text":"<ol> <li>Follow Standards: Adhere to the commit message standards (e.g.,    Conventional Commits) as outlined in your project guidelines.</li> <li>Descriptive Messages: Write clear and concise commit messages    that explain the what and why of the changes.</li> <li>Reference Issues: Include issue numbers or JIRA keys in the    commit messages to link changes to specific tasks or bugs.</li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#pushing-code","title":"Pushing Code","text":"<ol> <li>Push Regularly: Push your commits to the remote repository    regularly to share your progress with the team. This helps avoid    large, complex merges and conflicts.</li> <li>Pull Before Pushing: Always pull the latest changes from the    remote repository before pushing your commits to ensure you are    working with the most recent code and to minimize merge conflicts.</li> <li>Use Feature Branches: Work on feature branches and push them to    the remote repository. Create pull requests to merge these branches    into the main or development branches.</li> <li>Review and Test: Review your changes and run tests locally before    pushing to ensure code quality and functionality.</li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#collaboration-and-communication","title":"Collaboration and Communication","text":"<ol> <li>Code Reviews: Participate in code reviews by creating pull    requests for your feature branches. Review others\u2019 code and provide    constructive feedback.</li> <li>Sync Regularly: Sync with the main branch frequently to stay    up-to-date with the latest changes. This helps identify and resolve    conflicts early.</li> <li>Communicate Changes: Communicate significant changes or issues to    the team via pull request descriptions, commit messages, or team    communication channels.</li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#branch-management","title":"Branch Management","text":"<ol> <li>Clean Up: Delete feature branches from the remote repository    after they have been merged to keep the repository clean and    organized.</li> <li>Branch Naming: Follow the branch naming conventions as outlined    in your project guidelines to ensure clarity and consistency.</li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#automation-and-tools","title":"Automation and Tools","text":"<ol> <li>Continuous Integration (CI): Use CI tools to automate the testing    and integration of code changes. This helps catch issues early and    maintain code quality.</li> <li>Pre-commit Hooks: Use pre-commit hooks to enforce coding    standards and run tests before committing. This helps prevent bad    code from being committed.</li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#example-workflow","title":"Example Workflow","text":"<ol> <li>Create a Branch: Create a new branch for your feature or bug fix.</li> </ol> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <ol> <li>Make Changes: Work on your changes locally and commit frequently.</li> </ol> <pre><code>git add .\ngit commit -m \"feat(scope): description of your changes [#issue_number]\"\n</code></pre> <ol> <li>Pull Latest Changes: Pull the latest changes from the main    branch.</li> </ol> <pre><code>git pull origin main\n</code></pre> <ol> <li>Push Your Branch: Push your branch to the remote repository.</li> </ol> <pre><code>git push origin feature/your-feature-name\n</code></pre> <ol> <li> <p>Create a Pull Request: Create a pull request to merge your    changes into the main branch.</p> </li> <li> <p>Code Review: Participate in the code review process, addressing    feedback and making necessary changes.</p> </li> <li> <p>Merge and Clean Up: Once approved, merge your pull request and    delete the feature branch.</p> </li> </ol>"},{"location":"how-to-guides/pushing-to-githug-best-practices/#conclusion","title":"Conclusion","text":"<p>By following these best practices for committing and pushing code, you ensure a smooth and efficient workflow, enhance collaboration, and maintain a high-quality codebase. Regular, meaningful commits and effective use of branches and pull requests are key to successful project management and teamwork.</p>"},{"location":"how-to-guides/pytest-inroduction-guide/","title":"Introduction to Unit Testing with Pytest","text":""},{"location":"how-to-guides/pytest-inroduction-guide/#introduction","title":"Introduction","text":"<p>Unit testing is a fundamental practice in software development that involves testing individual units of code to ensure they work as intended. This guide introduces you to unit testing with Pytest, a popular testing framework for Python. We will cover the basics of unit testing, why it is essential, and how to set up and write unit tests using Pytest with the configuration provided in our project.</p> <p>Detailed Pytest Configuration Guide</p> <p>For a detailed explanation and setup for Pytest configuration within VS Code and Poetry, see the Pytest Configuration Guide.</p>"},{"location":"how-to-guides/pytest-inroduction-guide/#what-is-unit-testing","title":"What is Unit Testing?","text":""},{"location":"how-to-guides/pytest-inroduction-guide/#definition","title":"Definition","text":"<p>Unit testing involves testing the smallest parts of an application, called units, in isolation. A unit can be a function, method, or class. The goal is to verify that each unit of the software performs as expected.</p>"},{"location":"how-to-guides/pytest-inroduction-guide/#importance","title":"Importance","text":"<ul> <li>Early Bug Detection: Unit tests help catch bugs early in the development cycle.</li> <li>Documentation: They serve as documentation, explaining what the code is supposed to do.</li> <li>Refactoring Support: Unit tests make it safer to refactor code since you can quickly verify that your changes didn't break anything.</li> <li>Confidence in Code: They provide confidence that the code works correctly, leading to more reliable and maintainable software.</li> </ul>"},{"location":"how-to-guides/pytest-inroduction-guide/#why-use-pytest","title":"Why Use Pytest?","text":"<p>Pytest is a powerful and flexible testing framework that makes it easy to write simple and scalable test cases. Here are some reasons to use Pytest:</p> <ul> <li>Simple Syntax: Pytest's syntax is straightforward, making it easy to write and understand tests.</li> <li>Fixtures: Pytest provides fixtures for managing setup and teardown code.</li> <li>Plugins: A rich ecosystem of plugins extends Pytest's functionality.</li> <li>Parameterization: Easily parameterize tests to run them with different inputs.</li> </ul>"},{"location":"how-to-guides/pytest-inroduction-guide/#writing-your-first-unit-test","title":"Writing Your First Unit Test","text":"<p>Let's write a simple unit test for a function that adds two numbers. Create a Python file named <code>calculator.py</code> in your <code>src</code> directory with the following content:</p> <pre><code># src/calculator.py\n\ndef add(a, b):\n    return a + b\n</code></pre> <p>Next, create a test file named <code>test_calculator.py</code> in the <code>tests</code> directory with the following content:</p> <pre><code># tests/test_calculator.py\n\nfrom src.calculator import add\n\ndef test_add():\n    assert add(2, 3) == 5\n    assert add(-1, 1) == 0\n    assert add(-1, -1) == -2\n</code></pre>"},{"location":"how-to-guides/pytest-inroduction-guide/#running-the-tests","title":"Running the Tests","text":"<p>To run the tests, use the following command:</p> <pre><code>poetry run pytest\n</code></pre> <p>You should see output indicating that the tests have passed.</p>"},{"location":"how-to-guides/pytest-inroduction-guide/#advanced-features","title":"Advanced Features","text":""},{"location":"how-to-guides/pytest-inroduction-guide/#fixtures","title":"Fixtures","text":"<p>Fixtures are used to manage setup and teardown code. Here\u2019s an example of using a fixture:</p> <pre><code># tests/test_calculator.py\n\nimport pytest\nfrom src.calculator import add\n\n@pytest.fixture\ndef numbers():\n    return 2, 3\n\ndef test_add(numbers):\n    a, b = numbers\n    assert add(a, b) == 5\n</code></pre>"},{"location":"how-to-guides/pytest-inroduction-guide/#parameterization","title":"Parameterization","text":"<p>Parameterization allows you to run a test with multiple sets of inputs. Here\u2019s an example:</p> <pre><code># tests/test_calculator.py\n\nimport pytest\nfrom src.calculator import add\n\n@pytest.mark.parametrize(\"a, b, expected\", [\n    (2, 3, 5),\n    (-1, 1, 0),\n    (-1, -1, -2)\n])\ndef test_add(a, b, expected):\n    assert add(a, b) == expected\n</code></pre>"},{"location":"how-to-guides/pytest-inroduction-guide/#conclusion","title":"Conclusion","text":"<p>This guide introduced you to the basics of unit testing with Pytest. We covered the importance of unit testing, the benefits of using Pytest, and how to write and run your first unit test. With this foundation, you can start writing tests for your codebase, ensuring its reliability and maintainability. As you become more familiar with Pytest, you can explore its advanced features to further enhance your testing workflow. Happy testing!</p>"},{"location":"how-to-guides/python-docstrings-conventions/","title":"Docstring Guide Using Google Style","text":""},{"location":"how-to-guides/python-docstrings-conventions/#overview","title":"Overview","text":"<p>Docstrings are a crucial part of writing clear and maintainable code. This guide explains how to write docstrings following the Google style, providing examples for module-level, class-level, and method-level docstrings. Additionally, we will cover how to automate the creation of these docstrings using the VS Code extension autoDocstring.</p>"},{"location":"how-to-guides/python-docstrings-conventions/#module-level-docstring","title":"Module-Level Docstring","text":"<p>The module-level docstring is placed at the very top of the Python script. It provides an overview of the module's purpose and functionality.</p> <pre><code>\"\"\"\nThis module provides text processing utilities for NLP projects.\n\nThe utilities include functions for text cleaning, tokenization, and sentiment analysis.\n\"\"\"\n</code></pre>"},{"location":"how-to-guides/python-docstrings-conventions/#class-level-docstring","title":"Class-Level Docstring","text":"<p>The class-level docstring describes the class's purpose and provides an overview of its functionality.</p> <pre><code>class TextProcessor:\n    \"\"\"\n    A class used to perform text processing for NLP tasks.\n\n    This class includes methods for cleaning text, tokenizing sentences,\n    and calculating sentiment scores.\n\n    Attributes:\n        language (str): The language of the text to be processed.\n    \"\"\"\n</code></pre>"},{"location":"how-to-guides/python-docstrings-conventions/#method-level-docstrings","title":"Method-Level Docstrings","text":"<p>Each method should have a docstring explaining what the method does, its parameters, return values, any exceptions it raises, and providing an example if necessary.</p> <pre><code>    def __init__(self, language):\n        \"\"\"\n        Initializes the TextProcessor with a specified language.\n\n        Args:\n            language (str): The language of the text to be processed.\n\n        Raises:\n            ValueError: If the provided language is not supported.\n        \"\"\"\n        if language not in ['en', 'es', 'fr']:\n            raise ValueError(f\"Unsupported language: {language}\")\n        self.language = language\n\n    def clean_text(self, text):\n        \"\"\"\n        Cleans the input text by removing special characters and extra spaces.\n\n        Args:\n            text (str): The text to be cleaned.\n\n        Returns:\n            str: The cleaned text.\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def tokenize(self, text):\n        \"\"\"\n        Tokenizes the input text into a list of words.\n\n        Args:\n            text (str): The text to be tokenized.\n\n        Returns:\n            list: A list of words (tokens).\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def analyze_sentiment(self, text):\n        \"\"\"\n        Analyzes the sentiment of the input text.\n\n        Args:\n            text (str): The text to be analyzed.\n\n        Returns:\n            float: The sentiment score of the text.\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n</code></pre>"},{"location":"how-to-guides/python-docstrings-conventions/#automating-docstring-creation-with-autodocstring","title":"Automating Docstring Creation with autoDocstring","text":""},{"location":"how-to-guides/python-docstrings-conventions/#configuration-setup","title":"Configuration Setup","text":"<p>The following settings in <code>.vscode/settings.json</code> configure autoDocstring to use the Google style format for docstrings:</p> <pre><code>// #############################\n// ### Docstring ####\n// #############################\n// Use Google format for docstrings\n\"autoDocstring.docstringFormat\": \"google\",\n// Include Extended Summary section\n\"autoDocstring.includeExtendedSummary\": true,\n// Do not include function name at the start of docstrings\n\"autoDocstring.includeName\": false,\n</code></pre>"},{"location":"how-to-guides/python-docstrings-conventions/#using-autodocstring","title":"Using autoDocstring","text":"<ol> <li>Place your cursor inside the function or method where you want to add    a docstring.</li> <li>Trigger the autoDocstring generation by opening the Command Palette    (<code>Ctrl+Shift+P</code> or <code>Cmd+Shift+P</code>), typing <code>Generate Docstring</code>, and    selecting it.</li> <li>The extension will automatically generate a Google-style docstring    template for you.</li> </ol>"},{"location":"how-to-guides/python-docstrings-conventions/#example","title":"Example","text":"<p>Before using autoDocstring:</p> <pre><code>def clean_text(text):\n    # Implementation goes here\n    pass\n</code></pre> <p>After using autoDocstring:</p> <pre><code>def clean_text(text):\n    \"\"\"\n    Cleans the input text by removing special characters and extra spaces.\n\n    Args:\n        text (str): The text to be cleaned.\n\n    Returns:\n        str: The cleaned text.\n\n    Raises:\n        TypeError: If the input text is not a string.\n    \"\"\"\n    # Implementation goes here\n    pass\n</code></pre>"},{"location":"how-to-guides/python-docstrings-conventions/#full-example","title":"Full Example","text":"<p>Here is the complete example with all docstrings included:</p> <pre><code>\"\"\"\nThis module provides text processing utilities for NLP projects.\n\nThe utilities include functions for text cleaning, tokenization, and sentiment analysis.\n\"\"\"\n\nclass TextProcessor:\n    \"\"\"\n    A class used to perform text processing for NLP tasks.\n\n    This class includes methods for cleaning text, tokenizing sentences,\n    and calculating sentiment scores.\n\n    Attributes:\n        language (str): The language of the text to be processed.\n    \"\"\"\n\n    def __init__(self, language):\n        \"\"\"\n        Initializes the TextProcessor with a specified language.\n\n        Args:\n            language (str): The language of the text to be processed.\n\n        Raises:\n            ValueError: If the provided language is not supported.\n        \"\"\"\n        if language not in ['en', 'es', 'fr']:\n            raise ValueError(f\"Unsupported language: {language}\")\n        self.language = language\n\n    def clean_text(self, text):\n        \"\"\"\n        Cleans the input text by removing special characters and extra spaces.\n\n        Args:\n            text (str): The text to be cleaned.\n\n        Returns:\n            str: The cleaned text.\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def tokenize(self, text):\n        \"\"\"\n        Tokenizes the input text into a list of words.\n\n        Args:\n            text (str): The text to be tokenized.\n\n        Returns:\n            list: A list of words (tokens).\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def analyze_sentiment(self, text):\n        \"\"\"\n        Analyzes the sentiment of the input text.\n\n        Args:\n            text (str): The text to be analyzed.\n\n        Returns:\n            float: The sentiment score of the text.\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n</code></pre> <p>For more details, refer to the Google Python Style Guide.</p>"},{"location":"how-to-guides/python-docstrings-conventions/#requesting-google-style-docstrings-from-chatgpt","title":"Requesting Google-Style Docstrings from ChatGPT","text":"<p>A fast and easy way to create Google-style docstrings is by using ChatGPT. You can provide a simple prompt to request the creation of the docstring for any Python function.</p>"},{"location":"how-to-guides/python-docstrings-conventions/#example-prompt","title":"Example Prompt","text":"<p>Here\u2019s an example prompt you could use with ChatGPT to request a Google-style docstring following the settings in your <code>.vscode/settings.json</code> file:</p> <p>Prompt:</p> <pre><code>Create a Google-style docstring for the following Python function:\n\ndef add_numbers(a, b):\n    result = a + b\n    return result\n</code></pre>"},{"location":"how-to-guides/python-docstrings-conventions/#generated-docstring","title":"Generated Docstring","text":"<p>Using the prompt above, ChatGPT will generate a docstring similar to this:</p> <pre><code>def add_numbers(a, b):\n    \"\"\"\n    Adds two numbers together.\n\n    Args:\n        a (int): The first number.\n        b (int): The second number.\n\n    Returns:\n        int: The sum of the two numbers.\n\n    Raises:\n        TypeError: If either 'a' or 'b' is not an integer.\n    \"\"\"\n    result = a + b\n    return result\n</code></pre> <p>By following these guidelines and using the autoDocstring extension or a ChatGPT-like app, you can ensure that your Python code is well-documented and easy to understand for other developers.</p>"},{"location":"how-to-guides/python-docstrings-conventions/#references","title":"References","text":"<ul> <li>Documenting Python Code: A Complete   Guide</li> <li>Document Your Python Code and Projects With   ChatGPT</li> </ul>"},{"location":"how-to-guides/python-line-lenght-standards/","title":"Code and Comment Length Standards in Python Projects","text":""},{"location":"how-to-guides/python-line-lenght-standards/#overview","title":"Overview","text":"<p>Maintaining consistent code and comment lengths is crucial for readability and maintainability. This guide outlines the standards for line lengths in code and comments and explains how to configure and use Visual Studio Code (VS Code) to enforce these standards.</p>"},{"location":"how-to-guides/python-line-lenght-standards/#standards","title":"Standards","text":"<ul> <li>Comments: Limit comments to 72 characters per line.</li> <li>Code: Limit code lines to 79 characters.</li> </ul> <p>These standards help ensure that the codebase remains readable and manageable, particularly when reviewing code in different environments.</p>"},{"location":"how-to-guides/python-line-lenght-standards/#configuring-vs-code","title":"Configuring VS Code","text":"<p>To enforce these standards, we have pre-configured the following files in the repository:</p> <ul> <li><code>.vscode/settings.json</code></li> <li><code>.vscode/extensions.json</code></li> <li><code>pyproject.toml</code></li> </ul> <p>As a developer, you do not need to touch or modify these files. They are set up to ensure consistent standards across the project.</p>"},{"location":"how-to-guides/python-line-lenght-standards/#step-1-pre-configured-vs-code-settings","title":"Step 1: Pre-configured VS Code Settings","text":"<p>The <code>.vscode/settings.json</code> file is set to include rulers at 72 and 79 characters. This provides a visual cue for the character limits.</p> <pre><code>{\n    \"editor.rulers\": [\n        72,\n        79\n    ],\n    \"rewrap.wrappingColumn\": 72,\n    \"[python]\": {\n        \"editor.defaultFormatter\": \"ms-python.black-formatter\",\n        \"editor.formatOnSave\": false\n    },\n    \"rewrap.autoWrap.enabled\": true\n}\n</code></pre>"},{"location":"how-to-guides/python-line-lenght-standards/#step-2-pre-configured-extensions","title":"Step 2: Pre-configured Extensions","text":"<p>The <code>.vscode/extensions.json</code> file includes the necessary extensions like Black formatter and Ruff linter to enforce the standards.</p> <pre><code>{\n    \"recommendations\": [\n        \"ms-python.black-formatter\",\n        \"ms-python.python\",\n        \"stkb.rewrap\"\n    ]\n}\n</code></pre>"},{"location":"how-to-guides/python-line-lenght-standards/#step-3-pre-configured-pyprojecttoml","title":"Step 3: Pre-configured pyproject.toml","text":"<p>The <code>pyproject.toml</code> file is configured to use Black and Ruff, adhering to the line length standards. These libraries have been included under <code>[tool.poetry.dev-dependencies]</code> and this section should not be changed to maintain consistent formatting and linting across the project.</p> <pre><code>[tool.black]\nline-length = 79\n\n[tool.ruff]\nline-length = 79\n\n[tool.poetry.dev-dependencies]\nblack = { version = \"23.7.0\", extras = [\"jupyter\"] }\nruff = \"0.3.2\"\n# Other dependencies...\n</code></pre>"},{"location":"how-to-guides/python-line-lenght-standards/#using-vs-code-tools","title":"Using VS Code Tools","text":"<ol> <li>Visual Indicators: The rulers at 72 and 79 characters will help    you visually align your code and comments.</li> <li>Rewrap Extension: Use the Rewrap extension to format comments    quickly:</li> <li>Select the comment text.</li> <li>Press <code>Command + Shift + P</code> (to open the Command Palette) and      search for \"Rewrap comment/text\".</li> </ol>"},{"location":"how-to-guides/python-line-lenght-standards/#example-usage","title":"Example Usage","text":"<p>Before Rewrap:</p> <pre><code># This is a very long comment that exceeds the recommended 72 characters per line limit and should be wrapped.\ndef example_function():\n    pass\n</code></pre> <p>After Rewrap:</p> <pre><code># This is a very long comment that exceeds the recommended 72 characters\n# per line limit and should be wrapped.\ndef example_function():\n    pass\n</code></pre>"},{"location":"how-to-guides/python-line-lenght-standards/#conclusion","title":"Conclusion","text":"<p>By following these standards and using the pre-configured tools provided in the repository, you can maintain a clean and readable codebase. Consistent line lengths for code and comments improve readability, making it easier for team members to understand and review code. The configurations in <code>.vscode/settings.json</code>, <code>.vscode/extensions.json</code>, and <code>pyproject.toml</code> enforce these standards automatically, so developers can focus on writing quality code without worrying about configuration details. Additionally, the Python libraries Ruff and Black included in <code>[tool.poetry.dev-dependencies]</code> should not be changed to ensure consistent formatting and linting practices across the project.</p>"},{"location":"how-to-guides/python-oop-for-ml/","title":"Object-Oriented Programming (OOP) in Python for Machine Learning Projects","text":""},{"location":"how-to-guides/python-oop-for-ml/#overview","title":"Overview","text":"<p>Object-Oriented Programming (OOP) is a programming paradigm that uses objects and classes to structure code in a modular, reusable, and organized manner. Even in machine learning projects, where we often leverage powerful libraries like pandas, scikit-learn, and TensorFlow, adopting OOP principles can significantly enhance code maintainability, collaboration, and reproducibility.</p>"},{"location":"how-to-guides/python-oop-for-ml/#why-use-oop-in-machine-learning-projects","title":"Why Use OOP in Machine Learning Projects?","text":"<ol> <li>Modularity: Breaking down complex problems into smaller,    manageable pieces by using classes and methods makes the code more    modular.</li> <li>Reusability: Classes and methods can be reused across different    parts of the project or even in different projects.</li> <li>Maintainability: OOP helps in keeping the code organized, making    it easier to maintain and update.</li> <li>Collaboration: Clearly defined interfaces through classes and    methods allow multiple team members to work on different parts of the    project simultaneously without conflicts.</li> <li>Reproducibility: Encapsulating functionality in classes and    methods helps ensure that the code behaves consistently across    different runs, which is crucial for reproducibility in machine    learning experiments.</li> </ol>"},{"location":"how-to-guides/python-oop-for-ml/#example-of-oop-in-python","title":"Example of OOP in Python","text":"<p>Here is an example of how you might use OOP in a machine learning project. This example includes a simple data preprocessing pipeline using OOP principles and Google-style docstrings.</p> <pre><code>class DataPreprocessor:\n    \"\"\"\n    A class used to preprocess data for machine learning tasks.\n\n    This class includes methods for handling missing values, encoding categorical variables,\n    and scaling numerical features.\n\n    Attributes:\n        df (pd.DataFrame): The dataframe containing the data to be preprocessed.\n    \"\"\"\n\n    def __init__(self, df):\n        \"\"\"\n        Initializes the DataPreprocessor with the provided dataframe.\n\n        Args:\n            df (pd.DataFrame): The dataframe containing the data to be preprocessed.\n        \"\"\"\n        self.df = df\n\n    def handle_missing_values(self):\n        \"\"\"\n        Handles missing values in the dataframe by filling them with the median value.\n\n        Returns:\n            pd.DataFrame: The dataframe with missing values handled.\n        \"\"\"\n        self.df.fillna(self.df.median(), inplace=True)\n        return self.df\n\n    def encode_categorical_variables(self):\n        \"\"\"\n        Encodes categorical variables in the dataframe using one-hot encoding.\n\n        Returns:\n            pd.DataFrame: The dataframe with categorical variables encoded.\n        \"\"\"\n        self.df = pd.get_dummies(self.df)\n        return self.df\n\n    def scale_numerical_features(self):\n        \"\"\"\n        Scales numerical features in the dataframe using standard scaling.\n\n        Returns:\n            pd.DataFrame: The dataframe with numerical features scaled.\n        \"\"\"\n        from sklearn.preprocessing import StandardScaler\n        scaler = StandardScaler()\n        self.df[self.df.columns] = scaler.fit_transform(self.df)\n        return self.df\n\n    def preprocess(self):\n        \"\"\"\n        Executes the full preprocessing pipeline: handling missing values,\n        encoding categorical variables, and scaling numerical features.\n\n        Returns:\n            pd.DataFrame: The fully preprocessed dataframe.\n        \"\"\"\n        self.handle_missing_values()\n        self.encode_categorical_variables()\n        self.scale_numerical_features()\n        return self.df\n\n# Example usage\nimport pandas as pd\n\ndata = {\n    'age': [25, 30, None, 45],\n    'income': [50000, 60000, 70000, None],\n    'gender': ['Male', 'Female', 'Female', 'Male']\n}\ndf = pd.DataFrame(data)\n\npreprocessor = DataPreprocessor(df)\nprocessed_df = preprocessor.preprocess()\nprint(processed_df)\n</code></pre>"},{"location":"how-to-guides/python-oop-for-ml/#benefits-of-using-oop-in-machine-learning","title":"Benefits of Using OOP in Machine Learning","text":"<ol> <li>Separation of Concerns: By encapsulating different parts of the    preprocessing pipeline into methods, each method has a single    responsibility, making the code cleaner and more understandable.</li> <li>Extensibility: New preprocessing steps can be added easily by    creating new methods within the class.</li> <li>Testability: Each method can be tested independently, ensuring    that each step in the preprocessing pipeline works correctly.</li> </ol>"},{"location":"how-to-guides/python-oop-for-ml/#references","title":"References","text":"<ul> <li>Python OOP - Real   Python</li> <li>Python Classes - Real Python</li> <li>Object-Oriented Programming in Python -   DataCamp</li> <li>Python OOP Tutorial -   DataCamp</li> </ul> <p>By adopting OOP in your machine learning projects, you can enhance the readability, maintainability, and scalability of your code, making it easier to collaborate with others and ensure the reproducibility of your experiments.</p>"},{"location":"how-to-guides/templates/","title":"Templates for pull request, issue, feature and readme files","text":""},{"location":"how-to-guides/templates/#how-to-open-a-pull-request","title":"How to Open a Pull Request","text":"<p>Using ChatGPT to Write a Pull Request</p> <p>To create a clear and comprehensive pull request (PR) for your work on the Retrieval-Augmented Generation (RAG) Q&amp;A system, follow this simplified template. Use the provided ChatGPT prompt to generate the PR description automatically.</p> <p>PR Description Template:</p> <pre><code>**Title**: [Concise title of the PR]\n\n**Description**:\n- **JIRA Story**: [Link to the JIRA story]\n- **Summary**: Briefly describe the purpose of the PR and the changes made.\n- **Commit Messages**: List of commit messages for this branch.\n- **Context**: Any additional context or information that is relevant to understand the changes.\n- **Testing**: Summary of testing performed, including unit tests, integration tests, and manual testing.\n- **Documentation**: Mention any updates or additions to the documentation.\n</code></pre> <p>Example PR Description:</p> <pre><code>**Title**: Add User Authentication Feature\n\n**Description**:\n- **JIRA Story**: [DATA-123](https://jira.example.com/browse/DATA-123)\n- **Summary**: This PR adds a new user authentication feature, allowing users to register, log in, and recover their passwords. It includes both backend and frontend changes.\n- **Commit Messages**:\n  - feat(auth): add user registration [#DATA-123]\n  - feat(auth): implement login functionality [#DATA-123]\n  - feat(auth): add password recovery [#DATA-123]\n- **Context**: This change introduces new endpoints for authentication and updates the user model.\n- **Testing**: Unit tests, integration tests, and manual testing were performed to ensure the feature works as expected.\n- **Documentation**: Updated the API documentation and user guide to include the new authentication feature.\n</code></pre> <p>ChatGPT Prompt:</p> <pre><code>I need to create a pull request for my recent changes to the RAG Q&amp;A system. Here are the details:\n\n1. **JIRA Story**: [Enter JIRA story link]\n2. **Summary**: Briefly describe the purpose of the PR and the changes made.\n3. **Commit Messages**: [List of commit messages for this branch]\n4. **Context**: Any additional context or information relevant to the changes.\n5. **Testing**: Summary of testing performed.\n6. **Documentation**: Mention any updates or additions to the documentation.\n\nUsing this information, generate a comprehensive pull request description following the template below:\n\n**Title**: [Concise title of the PR]\n\n**Description**:\n- **JIRA Story**: [Link to the JIRA story]\n- **Summary**: Briefly describe the purpose of the PR and the changes made.\n- **Commit Messages**: List of commit messages for this branch.\n- **Context**: Any additional context or information that is relevant to understand the changes.\n- **Testing**: Summary of testing performed, including unit tests, integration tests, and manual testing.\n- **Documentation**: Mention any updates or additions to the documentation.\n</code></pre>"},{"location":"how-to-guides/templates/#pull-request-template","title":"Pull request template","text":"<p>The following is an example of a pull request template following a holistic view including methodology, code, data and documentation review by the coder and the reviewer.</p> .github/PULL_REQUEST_TEMPLATE/pull_request_template.md<pre><code>## Data Preparation Checklist\n- [ ] Data sources are properly cited\n- [ ] Data preprocessing and formatting are verified\n- [ ] Data quality is thoroughly assessed\n- [ ] Exploratory Data Analysis (EDA) is insightful and documented\n\n## Experiment Overview\n- [ ] Brief description of the experiment and its objectives\n- [ ] Links to the corresponding GitHub issue/Jira story\n- [ ] Summary of the experiment's findings/results\n- [ ] W&amp;B experiment page link for detailed metrics and visualizations\n\n## Feature Engineering Checklist\n- [ ] Relevant features are extracted and justified\n- [ ] Feature transformations and scaling methods are appropriate\n- [ ] Feature interactions are explored and utilized if beneficial\n- [ ] Feature importance analysis is conducted and findings are documented\n\n## Model Development Checklist\n- [ ] Data splitting strategy is justified and implemented correctly\n- [ ] Model training and validation approach is robust\n- [ ] Hyperparameter optimization is conducted using W&amp;B Sweeps (if applicable)\n- [ ] Model performance is benchmarked against baselines and documented\n- [ ] Model predictions are evaluated for reliability and bias\n\n## Code Quality Checklist\n- [ ] Code adheres to PEP 8 standards (or project-specific standards)\n- [ ] Code is modular, reusable, and well-commented\n- [ ] Code functionality is covered by tests (unit/integration)\n- [ ] Data validation checks are in place\n- [ ] End-to-end pipeline execution is verified\n- [ ] Dependency list is updated and conflicts are resolved\n\n## Documentation and Reporting Checklist\n- [ ] Purpose and design of the code are clearly articulated\n- [ ] Technical and business requirements are detailed\n- [ ] Documentation covers all aspects of the codebase\n- [ ] Execution steps are clearly outlined and reproducible\n- [ ] Model architecture and evaluation metrics are thoroughly documented\n\n## Collaboration and Review\n- [ ] Code is reviewed for clarity, efficiency, and adherence to best practices\n- [ ] Peer feedback is addressed constructively\n- [ ] Final review confirms that all checklist items are satisfied\n</code></pre>"},{"location":"how-to-guides/templates/#pull-request-template-for-bugs","title":"Pull request template for bugs","text":""},{"location":"how-to-guides/templates/#ask-issues","title":"Ask Issues","text":"<p>Ask issues are for capturing, scoping, and refining the value-based problems your team is trying to solve. They serve as a live definition of work for your projects and will be the anchor for coordinating the rest of the work you do.</p> <p>By having the definition of work be in an issue, data science teams can collaborate with their business partners and domain experts to refine and rescope the issue as they learn more about the problem.</p> <p>You should link to the other issues inside of the Ask issue. This will give people context around why a particular issue is being worked on.</p> <p>As your understanding of the problem evolves you should update your ask issue accordingly. In order to create clarity, you should be as specific as possible and resolve ambiguities by updating the Ask.</p> .github/ISSUE_TEMPLATE/ask_issue_template.md<pre><code>---\nname: Ask issue\nabout: Describe your ask issue\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n### Problem Statement\n\nThe problem statement is a high level description of what you're trying\nto solve and why. It should be clear what value solving this problem\nwill create and should also set a clear scope for what is and isn't\nincluded as part of the problem.\n\n!!! note\n    If there is a large project with several components, it may make sense\n    to create several Ask issues that link back to a parent Ask Issue.\n\n#### Desired Outcome\n\nThis describes what the end state should look like. If it helps, you can\nthink of this as a user story. \"I want [persona] to be able to do\n[action] so that [some outcome] can happen\". The goal here is to make\nsure that your model maps to a business process. The last thing you want\nis to build out a solution and not have a way to deploy it.\n\nThe most important part of this section is to capture what about the\nexisting process will change if your project is successful.\n\n#### Current State\n\nOne of the best ways to figure out what you need to build is by\nunderstanding the shortcomings of the existing process. You should also\ncapture why the current state needs to change. By focusing on this, you\ncan separate \"nice-to-have\" projects from \"need-to-have\" projects and\nmaximize your chances of get your project properly resourced.\n\n### Success Criteria\n\nIt's important to know what constitutes success early on in the project.\nThis prevents you from falling into the situation where you've built\nsomething that is good from a technical perspective but that the\nstakeholders reject for some reason. Set clear guidelines and acceptance\ncriteria. If it's not clear what those are, you should do some analysis\nto figure it out and then verify that with the stakeholders.\n\n#### Impact\n\nImpact is making it obvious what the value of your project will be once\ndone. The more concrete you can make this the better. If you are able to\nestimate the impact of your project in terms of clear metrics, you'll\nmake it easy for your stakeholders to make a go/no-go decision.\n\nYou should also try to quantify how much of an improvement is needed for\nthe project to be valuable. If a project needs accuracy that is not\nacheivable using SOTA techniques, it's good to know that before you\nstart working on it.\n\n#### Metrics\n\nMetrics take our impact and convert them into tangible metrics that we\ncan focus on while building our project. This can include both business\nmetrics as well as technical metrics.\n\nFor example, if the goal is to improve customer retention, you should\ndefine how churn is measured and how you intend measure the improvement.\nYou may also need to capture secondary metrics such as cost of keeping a\ncustomer.\n\nEventually, you'll need to convert your key metrics into technical ones.\nYou should be clear with which ones you're using. For example, if you're\ndoing a classification problem, you should be clear whether you're using\naccuracy, F-score, AUC, etc. and why.\n\nFinally, you may want to include counterbalance metrics. For example, if\nyour goal is to increase returns while minimizing risk.\n\n#### Constraints\n\nYou should capture any constraints your project needs to keep in mind.\nAsk yourself, what could keep my project from getting deployed. If you\nthink of something, that's a constraint. Some examples include:\n\n- If you need real-time predictions you need a model that can do fast\n  inference.\n- Certain data may not be acceptable for use in a model (e.g. privacy or\n  legal constraints).\n- Model must have interpretability metrics in order to be acted upon.\n\n### Solution Architecture\n\nIt's often useful to document your solution architecture. This can be\ndone using any tool (PowerPoint, Visio, hand-draw diagrams, etc.)\n\nIncluding a diagram can be very helpful in understanding how a solution\nwill be implemented. This helps you catch design issues early in the\nprocess while the cost to change them is still low.\n\nBy doing this throughout the project you'll also be able to see how your\nsolution evolved through time.\n\nFinally, it makes it easier for people who are new to a project to\nunderstand what's going on. This means there are lower barriers to\ncollaboration when you need to involve new teams.\n\n### Data Requirements\n\nYou should capture which datasets are needed to solve a problem. Include\nlinks to the data issues for each dataset (if there are multiple data\nissues for a dataset, use the most current version. GitHub will track\nchanges to the issue over time, so you can see if the linked issue\nchanged if necessary).\n\n#### Datasets\n\nFor each dataset, link to a data issue that describes what's in the\ndata. You can also link to the docs for a given dataset.\n\n&gt; Examples: Customer Transaction History (link to issue/docs)\n&gt; Customer demographics (link to issue/docs) Product inventories by\n&gt; store (link to issue/docs)\n\n### Understanding and Exploration\n\nThis section is for linking to explore issues that inform the problem.\nYou should also link to relevant docs that help frame the problem.\n\n### Approaches and Experiments\n\nThis section is for linking to your experiment issues. This section\nshould essentially be a running log of all the experiments attempted for\nthe ask. Experiments should be roughly chronological. Successful\nexperiments should be listed first.\n\n### See Also\n\n#### Background Info\n\nThis is useful for linking to related research, examples you found\nonline, etc. If something is relevant long term (i.e. after the ask is\nclosed), you may want to consider adding it to the docs.\n\n#### Related Projects\n\nSimilar to Background info. If there are related projects or repos, you\ncan link to them here. If something is relevant long term (i.e. after\nthe ask is closed), you may want to consider adding it to the docs.\n</code></pre>"},{"location":"how-to-guides/templates/#data-issues","title":"Data Issues","text":"<p>Data issues are for defining the requirements and acceptance criteria of a dataset needed for a problem. You should create one Data Issue per version of a dataset. Once a dataset is validated and accepted, you should publish that dataset as a version. For further changes and updates, you should create new Data Issues. This allows you to clearly mark milestones for when a dataset is done and creates clear expectations for the team on what's in their data.</p>"},{"location":"how-to-guides/templates/#data-acquisition","title":"Data Acquisition","text":"<p>Data Acquisition issues allow the data science team to collaborate with the team that owns the data to define which parts of the data they need inside the issue.</p> <p>They also enable a data review process to ensure that the data being loaded is what is needed for the problem at hand.</p> <p>Example</p> <ul> <li>Loading tables from a data warehouse to build models</li> <li>Loading customer transcript text files from the enterprise data lake</li> <li>Loading records from your company's CRM</li> </ul> <p>Note</p> <p>The more explicit you are about defining data requirements and writing validation tests, the less time you'll have to spend fixing your datasets later on.  Writing data validation tests is a great way to protect against breaking changes when you push updates to your datasets.</p> .github/ISSUE_TEMPLATE/data_acquisition_issue_template.md<pre><code>---\nname: Data aquisition issue\nabout: Describe your data aquisition\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n## Overview\n\nThe purpose of this issue is to track and document any issues or\nchallenges encountered while acquiring the data needed for the [Project\nName] project. This template can be used to report data quality issues,\nmissing data, or any other issues that may impact the success of the\nproject.\n\n## Data Source\n\nName and link to the data source(s)\n\n## Issue Description\n\nBriefly describe the data acquisition issue(s) you encountered,\nincluding details such as the affected variables or data files, the\nnature of the issue (e.g., missing data, inconsistent data, etc.), and\nany other relevant information.\n\n## Steps to Reproduce\n\nProvide step-by-step instructions for how to reproduce the issue, if\npossible.\n\n## Proposed Solution\n\nBriefly describe your proposed solution for resolving the issue, if\nknown.\n\n## Additional Information\n\nInclude any additional information or context that may be relevant to\nresolving the issue.\n\n## Labels\n\n- Data Acquisition\n- [Data Source] (e.g., API, CSV, etc.)\n- [Type of issue] (e.g., Missing Data, Inconsistent Data, etc.)\n</code></pre>"},{"location":"how-to-guides/templates/#dataset-creation","title":"Dataset Creation","text":"<p>In contrast to the data acquisition issues, data creation issues usually require a lot more experimentation and prototyping before a final dataset is ready to merge.</p> <p>The goal of this issue is to collaborate on creating the dataset you need. It will be completed once you've created and validated a data pipeline and created documentation for the new dataset.</p> .github/ISSUE_TEMPLATE/data_creation_issue_template.md<pre><code>---\nname: Data creation issue\nabout: Describe your data creation\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n## Problem Description\nDescribe the problem you are trying to solve with this data creation\neffort. What is the expected outcome and how will the data be used?\n\n## Data Requirements\nList the specific data requirements for this project. What\ncolumns/attributes do you need, what is the format (csv, json, etc.),\nand any constraints or limitations that need to be considered?\n\n## Potential Data Sources\nList any potential data sources that could be used for this project.\nWhat are the pros and cons of each source and why would you choose one\nover the others?\n\n## Data Collection Plan\nOutline the plan for collecting the necessary data. This should include\nthe steps you will take to collect the data, any tools or resources you\nwill use, and any potential challenges or roadblocks you anticipate.\n\n## Estimated Time and Resources\nEstimate the time and resources required to collect the data. What is\nthe expected timeline and what resources (e.g. people, equipment,\nbudget) will be needed?\n\n## Next Steps\nIndicate the next steps that need to be taken to start collecting the\ndata. Who is responsible for each step and what is the expected\ntimeline?\n\n## Additional Information\nInclude any additional information that would be helpful in\nunderstanding the data creation effort and why it is necessary.\n\n## Labels\n- data-creation\n- data-collection\n- project-planning\n</code></pre>"},{"location":"how-to-guides/templates/#explore-issues","title":"Explore Issues","text":"<p>A lot of data science is exploratory. The goal of this type of work is to increase your understanding of the data and the problem. Because of this, you really don't care about the code that much. The code is just an means to the end, which is getting more knowledge about the problem.</p> <p>Oftentimes, data exploration is only relevant while you're doing the work, or for providing context of why a decision was made.</p> <p>Traditionally, data science teams have three options here.</p> <ol> <li>Don't retain exploratory code in the repo.</li> <li>Retain all exploratory code in the repo.</li> <li>Selectively commit exploratory code to the repo.</li> </ol> <p>The biggest challenge with exploratory work is figuring out when you should merge it? If you merge it, how do you decide when you should deprecate it? How can you ensure your exploration code is reproducible? What does it mean to \"test\" exploration?</p> <p>Because it's often more work than it's worth to test exploration code, we should be hesitant to commit it to our main branch.</p> <p>Additionally, if we merge all of our exploration, we will quickly get notebook sprawl and our repo will be an unreadable, unmaintainable mess.</p> <p>As an alternative, we can create explore issues and pull requests where we describe and publish our work.</p> <p>Most of the time, it's sufficient to push our code to an explore branch and then close it without merging when we're done. The pull request will still contain all of our code if we need to view it later and by linking our explore issues to our other issues, we'll maintain a historical record without cluttering up our repo.</p> <p>Tip</p> <p>Push your exploratory code to an explore branch and then close it without merging when we're done.</p> <p>In the cases where a piece of exploration evolves into something we want to reuse or capture as part of our documentation then we can push changes to our repo to update the docs or add some code to our repository.</p>"},{"location":"how-to-guides/templates/#linking-explore-issues","title":"Linking Explore Issues","text":"<p>Explore issues should be linked to other issues so that they are given context. Often exploration without context of why it was done is not useful.</p> <p>Tip</p> <p>We should link the explore issues based on what issue prompted them.</p> <p>For example if you are exploring a new dataset to understand it's properties, you would link to an ask issue. If we're trying to validate assumptions or learn more about our target population, you would link to either an Ask or Explore issues. There are no hard rules here. The idea is you want to create a clear history and give other context about why an certain explore issue was created.</p> .github/ISSUE_TEMPLATE/explore_issue_template.md<pre><code>---\nname: Explore issue\nabout: Describe your exploration\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n## Problem Statement\n\nBriefly describe the problem that this exploration aims to solve or the\nquestion it aims to answer.\n\n## Data Source\n\nList the data sources that will be used for this exploration and a brief\ndescription of each.\n\n## Exploration Goals\n\nList the specific goals or questions that this exploration aims to\nanswer.\n\n## Requirements List\n\nAny specific requirements or constraints for this exploration.\n\n## Deliverables List\n\nThe final outputs or findings that you expect from this exploration.\n\n## Links to Other Issues\n\nLink the explore issue based on what issue prompted them.\n\n## Additional Information\n\nAdd any additional information or notes that may be relevant to this\nexploration.\n</code></pre> <p>How to link related issues on GitHub</p> <p>To link related issues in the same repository, you can type <code>#</code> followed by part of the issue title and then clicking the issue that you want to link.</p>"},{"location":"how-to-guides/templates/#experiment-issues","title":"Experiment Issues","text":"<p>We often won't solve our problem with the first approach we take. It often takes iterating through many versions before we find something that actually works.</p> <p>Traditionally, the experimentation phase is a place where data science teams struggle to be able to relay a sense of progress to others. They are trying many approaches, and in fact making progress, but all they can say is \"We're still trying to get this model working\".</p> <p>After the project is over, the various approaches taken usually are forgotten. When a team comes back to try to improve or fix a model later, they often end up wasting time retrying things that failed in the past simply because they don't know that they've already been tried.</p> <p>Experiment issues are designed to capture each distinct approach taken while trying to solve a problem. By doing so, teams can clearly communicate what they tried and learn from each others efforts. Additionally, they allow us to implement a model review process and decide in a more systematic way which approaches we want to productionalize. This allows us to catch problems early, - before we invest time in making our models production ready.</p> <p>The Experiment issues are the pivot point between the exploratory iterative part of the data science process and the more linear model deployment and MLOps portion of the process.</p> <p>Once we create a successful experiment, we can validate our approach with others and then elevate it to a Model issue and begin the MLOps cycle.</p>"},{"location":"how-to-guides/templates/#what-to-include","title":"What to include","text":"<p>Similar to Explore issues, experiment issues should contain a TLDR of what was tried and what the outcome was.</p> <p>All experiments should be linked to an Ask issue.  Additionally, if using a model and experimentation tracking system (like AzureML), they should link to the system of record for your experiments. This lets people have full insight into the full experiment lifecycle including what problem the experiment was attempting to solve, what was tried, what combinations of parameters were used, etc.</p> .github/ISSUE_TEMPLATE/experiment_issue_template.md<pre><code>### Enhanced ML Experiment Issue/Story Template\n\n**Instructions for Use**:\n- When creating a new issue or story for an ML experiment, copy and\n  paste this template into the description field.\n- Fill in each section with the details of your specific experiment.\n- Encourage team members to review and contribute to the experiment\n  planning process through comments or direct edits to the issue/story.\n\n---\n\nThis enhanced template is structured to ensure thorough preparation and\nclear communication of ML experiments, fostering a collaborative and\nresults-oriented approach to tackling complex problems.\n\n#### **Experiment Title**\n- A concise title that encapsulates the essence of the experiment,\n  making it easily identifiable.\n\n#### **Experiment Description**\n- **Background**: Provide context and the rationale behind this\n  experiment. Include any relevant previous work or literature.\n- **Objective**: Clearly define what this experiment aims to achieve.\n  Describe the problem it solves or the hypothesis it tests.\n- **Hypothesis**: State the expected outcome of the experiment and the\n  assumptions that are being tested.\n\n#### **Data Source**\n- Detail each data source to be used, including its origin, structure,\n  and any preprocessing steps applied. Mention the relevance of each\n  data source to the experiment.\n\n#### **Experiment Goals**\n- Enumerate the specific goals or questions the experiment aims to\n  address. This could include performance benchmarks, model comparisons,\n  or exploration of specific hypotheses.\n\n#### **Success Criteria**\n- Define clear, measurable criteria for what will constitute success for\n  this experiment. This could include target metrics, statistical\n  significance levels, or qualitative outcomes.\n\n#### **Requirements List**\n- List any technical, data, or resource requirements. Include hardware\n  specifications, software versions, and any necessary access\n  permissions.\n\n#### **Deliverables List**\n- Specify the expected outputs of the experiment. This could range from\n  model weights, performance reports, to insights or conclusions drawn\n  from the analysis.\n\n#### **Experiment Methodology**\n- **Approach**: Briefly outline the technical approach, including model\n  architectures, algorithms, and any novel techniques being tested.\n- **Metrics**: List the metrics by which the experiment will be\n  evaluated, explaining why each is chosen.\n- **Implementation Plan**: Provide an overview of the steps involved in\n  executing the experiment, including data preparation, model training,\n  and evaluation phases.\n\n#### **Timeline and Milestones**\n- Outline a tentative timeline for the experiment, including key\n  milestones and checkpoints.\n\n#### **Roles and Responsibilities**\n- Identify team members involved and their specific responsibilities\n  within the experiment.\n\n#### **Links to Other Issues/Stories**\n- Provide references to related issues or stories, including any that\n  prompted this experiment or are dependencies of it.\n\n#### **Additional Information**\n- Include any other relevant information, notes, or considerations that\n  could impact the experiment's design, execution, or analysis.\n\n#### **Feedback Section**\n- Invite team members to provide feedback on the experimental design,\n  methodology, or any other aspect. Encourage collaboration and open\n  discussion to refine and improve the experiment.\n</code></pre>"},{"location":"how-to-guides/templates/#model-issues","title":"Model Issues","text":"<p>Once you've completed an experiment and are ready to start the model development and deployment process, you should open a Model issue.</p> <p>You will use the Model issue to turn your experiment into production-ready, deployable code. This will include adding tests, parametrizing your models, etc.</p> <p>The Model issue marks the beginning of the MLOps process. As your team matures, you can create CI/CD processes and automation around the Model Issue and its associated PR.</p> <p>For example, once you've created your production model code with tests, parameters, etc, you can use a pull request to trigger a CI/CD pipeline to train, test, package, deploy, and monitor your model. Once your model monitoring detects that your model performance has dropped, you can automatically create a new model PR to retrain the model and mark it for review by the data science team before triggering an automated deployment process.</p> .github/ISSUE_TEMPLATE/model_issue_template.md<pre><code>---\nname: Model issue\nabout: Turn experimet into production-ready code\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\nOnce you've completed an experiment and are ready to start the model\ndevelopment and deployment process, you should open a Model issue.\n\nYou will use the Model issue to turn your experiment into\nproduction-ready, deployable code. This will include adding tests,\nparametrizing your models, etc.\n\nThe Model issue marks the beginning of the MLOps process. As your team\nmatures, you can create CI/CD processes and automation around the Model\nIssue and its associated PR.\n\nFor example, once you've created your production model code with tests,\nparameters, etc, you can use a pull request to trigger a CI/CD pipeline\nto train, test, package, deploy, and monitor your model. Once your model\nmonitoring detects that your model performance has dropped, you can\nautomatically create a new model PR to retrain the model and mark it for\nreview by the data science team before triggering an automated\ndeployment process.\n</code></pre>"},{"location":"how-to-guides/templates/#feature-request","title":"Feature Request","text":".github/ISSUE_TEMPLATE/feature_request.md<pre><code>---\nname: Feature request\nabout: Suggest an idea for this project\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n**Is your feature request related to a problem? Please describe.**\nA clear and concise description of what the problem is. Ex. I'm always frustrated when [...]\n\n**Describe the solution you'd like**\nA clear and concise description of what you want to happen.\n\n**Describe alternatives you've considered**\nA clear and concise description of any alternative solutions or features you've considered.\n\n**Additional context**\nAdd any other context or screenshots about the feature request here.\n</code></pre>"},{"location":"how-to-guides/templates/#bug-report","title":"Bug Report","text":".github/ISSUE_TEMPLATE/bug_report.md<pre><code>---\nname: Bug report\nabout: Create a report to help us improve\ntitle: ''\nlabels: ''\nassignees: ''\n\n---\n\n**Describe the bug**\nA clear and concise description of what the bug is.\n\n**To Reproduce**\nSteps to reproduce the behavior:\n1. Go to '...'\n2. Click on '....'\n3. Scroll down to '....'\n4. See error\n\n**Expected behavior**\nA clear and concise description of what you expected to happen.\n\n**Screenshots**\nIf applicable, add screenshots to help explain your problem.\n\n**Desktop (please complete the following information):**\n - Browser [e.g. chrome, safari]\n - Version [e.g. 22]\n\n**Additional context**\nAdd any other context about the problem here.\n</code></pre>"},{"location":"how-to-guides/templates/#readme-file-template","title":"Readme file template","text":""},{"location":"how-to-guides/toc-how-to-guides/","title":"How-To Guides","text":"<p>About This Section</p> <p>The How-To Guides section is a repository of in-depth, practical guides for specific tasks within Python projects and broader software development workflows. Each guide delivers concise, step-by-step directions designed to tackle particular challenges, ranging from initial setups to complex processes.</p>"},{"location":"how-to-guides/toc-how-to-guides/#purpose","title":"Purpose","text":"<ul> <li> Targeted Instruction: The guides are crafted to deliver straightforward, actionable steps for distinct development hurdles.</li> <li> Comprehensive Topic Coverage: Encompassing a spectrum of subjects, from fundamental configurations to intricate enhancements.</li> <li> Universal Applicability: Suited for developers of all experience levels, offering valuable insights for both beginners and veterans.</li> </ul>"},{"location":"how-to-guides/toc-how-to-guides/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Setting Up DVC for Efficient Data Management</li> <li>Project Scaffolding Standards</li> <li>File Naming Conventions</li> <li>Column Naming Conventions for ML/AI Projects</li> <li>Python Docstrings Conventions</li> <li>Best Practices for Using Git and Pushing to GitHub</li> <li>Code and Comment Length Standards in Python Projects</li> <li>Facilitating Team Communication with DVC</li> <li>Integrating DVC with VS Code for Enhanced Workflow</li> <li>Creating Git Branches Following Best Practices</li> <li>Standardizing Commit Messages in ML Projects</li> <li>Naming Data Folders in ML Projects for Better Organization</li> <li>Adopting Effective Data File Naming Conventions</li> <li>Automating the Creation of Metadata for Machine Learning Models</li> <li>Implementing GitHub Actions to Enforce Naming Conventions</li> <li>Using Cookiecutter for Project Setup Without Internet Access</li> <li>ML Experiments Life-Cycle with Weights &amp; Biases</li> <li>Templates for Issues/Stories and PRs</li> <li>Branching Strategies for ML Projects</li> <li>Python Scripting for Data Conversion</li> <li>Automating Backups for Weights &amp; Biases</li> <li>Configuring Jupyter Notebook Start-up Directory in VS Code</li> <li>Project Data Management Practices</li> <li>AI/ML Project Lifecycle with Git and GitHub</li> <li>Introduction to Doctest</li> <li>Code Review Best Practices</li> <li>Best Practices for Creating JIRA Stories for ML/AI Projects</li> <li>Moving from Jupyter Notebooks to Production Python Code</li> <li>Static Type Checking with Mypy</li> <li>Using Pre-Commit Hooks to Enforce Coding Standards</li> <li>Pytest Introduction Guide</li> <li>Using Configuration Files to Avoid Hardcoding Values</li> </ul> <p>Optimizing Your Workflow</p> <p>These guides are intentionally designed to function independently, providing the flexibility to directly engage with topics pertinent to your immediate challenges. As a compendium of efficient, practical solutions, they are an excellent resource for enhancing productivity and mastering various development tasks.</p>"},{"location":"how-to-guides/type-checking-mypy/","title":"Static Type Checking","text":"<p>Using ChatGPT to Assist with Static Typing</p> <p>Consider a Retrieval-Augmented Generation (RAG) Q&amp;A system. Here's a function without type annotations:</p> <pre><code>I have a Python function that processes user queries and retrieves \nrelevant documents in a RAG Q&amp;A system. The `query` parameter is a \nstring. The `retrieve_documents` function returns a list of \ndictionaries with `id` and `text` as strings. The `process_query` \nfunction should return a list of document texts. Here is the function:\n\ndef process_query(query, retrieve_documents):\n    documents = retrieve_documents(query)\n    return [doc['text'] for doc in documents]\n\nCan you add the appropriate type annotations to this function using \nMypy and explain your reasoning?\n</code></pre> <p>Detailed Mypy Configuration Guide</p> <p>For a detailed explanation and setup for Mypy configuration within VS Code and <code>pyproject.toml</code>, see the Mypy Configuration Guide.</p>"},{"location":"how-to-guides/type-checking-mypy/#example-output","title":"Example Output","text":"<p>ChatGPT might respond with:</p> <pre><code>from typing import Callable, List, Dict\n\ndef process_query(query: str, retrieve_documents: Callable[[str], List[Dict[str, str]]]) -&gt; List[str]:\n    documents = retrieve_documents(query)\n    return [doc['text'] for doc in documents]\n</code></pre>"},{"location":"how-to-guides/type-checking-mypy/#reasoning","title":"Reasoning:","text":"<ol> <li><code>query: str</code>: The <code>query</code> parameter is a string because it represents the user's question.</li> <li><code>retrieve_documents: Callable[[str], List[Dict[str, str]]]</code>: The <code>retrieve_documents</code> parameter is a callable function that takes a string (<code>query</code>) and returns a list of dictionaries, each containing an <code>id</code> and <code>text</code> as strings.</li> <li><code>-&gt; List[str]</code>: The <code>process_query</code> function returns a list of strings, which are the texts of the retrieved documents.</li> </ol>"},{"location":"how-to-guides/type-checking-mypy/#best-practices-for-using-chatgpt","title":"Best Practices for Using ChatGPT","text":"<ol> <li>Provide Context: More context leads to better type annotations. Include sample inputs or describe expected outputs.</li> <li>Ask for Explanations: If unsure about an annotation, ask ChatGPT for an explanation.</li> <li>Iterate: Start with a basic function and gradually add complexity to ensure consistent and accurate annotations.</li> </ol>"},{"location":"how-to-guides/type-checking-mypy/#running-mypy","title":"Running Mypy","text":"<p>To check your code with Mypy, run the following command in your terminal:</p> <pre><code>mypy your_script.py\n</code></pre>"},{"location":"how-to-guides/type-checking-mypy/#why-use-static-typing-with-python","title":"Why Use Static Typing with Python?","text":"<p>Python's dynamic typing allows for flexible and rapid development but comes with trade-offs. Understanding these trade-offs and the benefits of static typing can help you decide to use tools like Mypy in your Python projects.</p>"},{"location":"how-to-guides/type-checking-mypy/#dynamic-typing-in-python","title":"Dynamic Typing in Python","text":"<p>Python doesn't require explicit type declarations for variables or function return values. The interpreter determines the type at runtime based on the value assigned. This flexibility allows quick prototyping but can lead to subtle and hard-to-diagnose bugs.</p>"},{"location":"how-to-guides/type-checking-mypy/#duck-typing","title":"Duck Typing","text":"<p>Python uses \"duck typing,\" where an object's suitability is determined by the presence of certain methods and properties rather than its type. This makes code flexible but can make understanding and debugging difficult in larger codebases. For example:</p> <pre><code>def add(a, b):\n    return a + b\n\nresult = add(3, \"4\")  # This will cause a runtime error\nprint(result)\n</code></pre> <p>Without explicit type annotations, it's hard to know what types <code>a</code> and <code>b</code> should be, leading to potential runtime errors.</p>"},{"location":"how-to-guides/type-checking-mypy/#benefits-of-static-typing","title":"Benefits of Static Typing","text":"<p>Static typing addresses these issues by making type expectations explicit, providing several benefits:</p> <ul> <li>Improved Readability: Type annotations serve as documentation,   making it easier for others to understand the code.</li> <li>Early Error Detection: Tools like Mypy can detect type errors   before runtime.</li> <li>Ease of Refactoring: Knowing variable types makes refactoring   safer and more predictable.</li> <li>Enhanced IDE Support: IDEs use type annotations for better code   completion, navigation, and refactoring tools.</li> </ul>"},{"location":"how-to-guides/type-checking-mypy/#example-explicit-type-annotations","title":"Example: Explicit Type Annotations","text":""},{"location":"how-to-guides/type-checking-mypy/#before-without-type-checking","title":"Before: Without Type Checking","text":"<pre><code>def add(a, b):\n    return a + b\n\nresult = add(3, \"4\")  # This will cause a runtime error\nprint(result)\n</code></pre>"},{"location":"how-to-guides/type-checking-mypy/#after-with-type-checking","title":"After: With Type Checking","text":"<pre><code>def add(a: int, b: int) -&gt; int:\n    return a + b\n\nresult = add(3, 4)  # This will work\nprint(result)\n\n# Uncommenting the line below will cause a type checking error with mypy\n# result = add(3, \"4\")  # This will cause a mypy type error\n</code></pre> <p>Type annotations specify that <code>a</code> and <code>b</code> must be of type <code>int</code>. The type checker will catch any type errors before runtime.</p> <p>To check the code with <code>mypy</code>, save the \"after\" code in a file (e.g., <code>example.py</code>) and run:</p> <pre><code>mypy example.py\n</code></pre> <p>Mypy will report an error if you uncomment the line with the type mismatch:</p> <pre><code>example.py:10: error: Argument 2 to \"add\" has incompatible type \"str\"; expected \"int\"\n</code></pre> <p>This ensures that type errors are caught early during development, improving code robustness.</p>"},{"location":"how-to-guides/type-checking-mypy/#maintaining-type-annotations","title":"Maintaining Type Annotations","text":"<p>Type annotations are automatically checked for correctness by tools like Mypy, ensuring they remain accurate and up-to-date. This reduces the risk of outdated documentation and improves code quality.</p> <p>By incorporating static typing and using tools like Mypy, you can enhance the robustness and maintainability of your Python code, making it easier to understand, debug, and extend.</p>"},{"location":"how-to-guides/type-checking-mypy/#type-inference-using-mypy","title":"Type Inference Using Mypy","text":""},{"location":"how-to-guides/type-checking-mypy/#introduction","title":"Introduction","text":"<p>Type inference lets the compiler or interpreter deduce types without explicit annotations. In Python, mypy helps ensure type correctness by using type inference.</p>"},{"location":"how-to-guides/type-checking-mypy/#what-is-type-inference","title":"What is Type Inference?","text":"<p>Type inference allows the type checker to deduce variable and expression types based on assigned values and operations, reducing the need for explicit annotations and keeping the code clean and concise.</p>"},{"location":"how-to-guides/type-checking-mypy/#how-mypy-performs-type-inference","title":"How Mypy Performs Type Inference","text":""},{"location":"how-to-guides/type-checking-mypy/#variable-type-inference","title":"Variable Type Inference","text":"<p>Mypy can infer the type of a variable based on the value assigned to it. For example:</p> <pre><code>x = 10  # mypy infers x as int\ny = \"Hello, world!\"  # mypy infers y as str\nz = 3.14  # mypy infers z as float\n</code></pre> <p>In this example, <code>mypy</code> infers that <code>x</code> is of type <code>int</code>, <code>y</code> is of type <code>str</code>, and <code>z</code> is of type <code>float</code> based on the assigned values.</p>"},{"location":"how-to-guides/type-checking-mypy/#function-return-type-inference","title":"Function Return Type Inference","text":"<p>Mypy can also infer the return type of a function based on the return statement:</p> <pre><code>def add(a: int, b: int) -&gt; int:\n    return a + b\n\nresult = add(3, 4)  # mypy infers result as int\nprint(result)\n\n# Uncommenting the line below will cause a type checking error with mypy\n# result = add(3, \"4\")  # This will cause a mypy type error\n</code></pre> <p>Here, <code>mypy</code> infers that the <code>add</code> function returns an <code>int</code> because it adds two integers. Consequently, the variable <code>result</code> is inferred to be of type <code>int</code>.</p>"},{"location":"how-to-guides/type-checking-mypy/#collection-type-inference","title":"Collection Type Inference","text":"<p>When working with collections like lists and dictionaries, <code>mypy</code> can infer the types of the elements:</p> <pre><code>numbers = [1, 2, 3, 4]  # mypy infers numbers as List[int]\nnames = [\"Alice\", \"Bob\", \"Charlie\"]  # mypy infers names as List[str]\n\nperson = {\n    \"name\": \"Marcos\",\n    \"age\": 42\n}  # mypy infers person as Dict[str, Union[str, int]]\n</code></pre> <p>In these examples, <code>mypy</code> infers the types of the lists and dictionaries based on their contents. If you later try to add elements of a different type to these collections, mypy will flag it as an error, ensuring type consistency throughout your codebase.</p>"},{"location":"how-to-guides/type-checking-mypy/#example-complete-type-inference","title":"Example: Complete Type Inference","text":"<pre><code># example.py\n\ndef add(a: int, b: int):\n    return a + b\n\ndef greet(name: str):\n    return f\"Hello, {name}!\"\n\nx = 10  # mypy infers x as int\ny = 20  # mypy infers y as int\nz = add(x, y)  # mypy infers z as int\n\nmessage = greet(\"Marcos\")  # mypy infers message as str\n\nnumbers = [1, 2, 3, 4]  # mypy infers numbers as List[int]\nnames = [\"Alice\", \"Bob\", \"Charlie\"]  # mypy infers names as List[str]\n\nperson = {\n    \"name\": \"Marcos\",\n    \"age\": 42\n}  # mypy infers person as Dict[str, Union[str, int]]\n</code></pre> <p>To check the type inference with <code>mypy</code>, run:</p> <pre><code>mypy example.py\n</code></pre>"},{"location":"how-to-guides/type-checking-mypy/#benefits-of-type-inference","title":"Benefits of Type Inference","text":"<ol> <li>Conciseness: Reduces the need for explicit type annotations,    making the code more concise.</li> <li>Readability: Enhances code readability by inferring types based    on context.</li> <li>Safety: Ensures type correctness, reducing runtime errors.</li> </ol>"},{"location":"how-to-guides/type-checking-mypy/#guide-to-using-union-and-optionals-with-mypy","title":"Guide to Using Union and Optionals with Mypy","text":""},{"location":"how-to-guides/type-checking-mypy/#introduction_1","title":"Introduction","text":"<p>Two important concepts in type annotations are <code>Union</code> and <code>Optional</code>. These types enable developers to specify multiple possible types for a variable or function return type, making the code more flexible and expressive.</p>"},{"location":"how-to-guides/type-checking-mypy/#union","title":"Union","text":"<p>The <code>Union</code> type allows a variable to have multiple possible types. This is useful when a variable or function parameter can accept different types.</p>"},{"location":"how-to-guides/type-checking-mypy/#syntax","title":"Syntax","text":"<pre><code>from typing import Union\n\ndef process(value: Union[int, str]) -&gt; None:\n    if isinstance(value, int):\n        print(f\"Processing an integer: {value}\")\n    elif isinstance(value, str):\n        print(f\"Processing a string: {value}\")\n\n# Example usage\nprocess(42)\nprocess(\"Hello\")\n</code></pre> <p>In this example, the <code>process</code> function can accept both <code>int</code> and <code>str</code> types for the <code>value</code> parameter. <code>mypy</code> will check that the code correctly handles both types.</p>"},{"location":"how-to-guides/type-checking-mypy/#example","title":"Example","text":"<pre><code>from typing import Union\n\ndef describe(value: Union[int, float, str]) -&gt; str:\n    if isinstance(value, int):\n        return f\"Integer: {value}\"\n    elif isinstance(value, float):\n        return f\"Float: {value}\"\n    elif isinstance(value, str):\n        return f\"String: {value}\"\n\n# Example usage\nprint(describe(10))     # Integer: 10\nprint(describe(3.14))   # Float: 3.14\nprint(describe(\"hello\"))  # String: hello\n</code></pre> <p>In this example, the <code>describe</code> function can handle <code>int</code>, <code>float</code>, and <code>str</code> types, providing a description for each.</p>"},{"location":"how-to-guides/type-checking-mypy/#optional","title":"Optional","text":"<p>The <code>Optional</code> type is a shorthand for a <code>Union</code> with <code>None</code>. It indicates that a variable can have a specific type or be <code>None</code>.</p>"},{"location":"how-to-guides/type-checking-mypy/#syntax_1","title":"Syntax","text":"<pre><code>from typing import Optional\n\ndef greet(name: Optional[str]) -&gt; str:\n    if name is None:\n        return \"Hello, there!\"\n    else:\n        return f\"Hello, {name}!\"\n\n# Example usage\nprint(greet(None))        # Hello, there!\nprint(greet(\"Marcos\"))    # Hello, Marcos!\n</code></pre> <p>In this example, the <code>greet</code> function can accept either a <code>str</code> or <code>None</code> for the <code>name</code> parameter.</p>"},{"location":"how-to-guides/type-checking-mypy/#example_1","title":"Example","text":"<pre><code>from typing import Optional\n\ndef find_user(user_id: int) -&gt; Optional[str]:\n    users = {1: \"Alice\", 2: \"Bob\"}\n    return users.get(user_id)\n\n# Example usage\nprint(find_user(1))  # Alice\nprint(find_user(3))  # None\n</code></pre> <p>In this example, the <code>find_user</code> function returns a <code>str</code> if the user is found, otherwise it returns <code>None</code>.</p>"},{"location":"how-to-guides/type-checking-mypy/#combining-union-and-optional","title":"Combining Union and Optional","text":"<p>You can combine <code>Union</code> and <code>Optional</code> to specify complex types.</p>"},{"location":"how-to-guides/type-checking-mypy/#example_2","title":"Example","text":"<pre><code>from typing import Union, Optional\n\ndef read_value(value: Union[int, float, Optional[str]]) -&gt; str:\n    if value is None:\n        return \"No value\"\n    elif isinstance(value, int):\n        return f\"Integer value: {value}\"\n    elif isinstance(value, float):\n        return f\"Float value: {value}\"\n    elif isinstance(value, str):\n        return f\"String value: {value}\"\n\n# Example usage\nprint(read_value(10))      # Integer value: 10\nprint(read_value(3.14))    # Float value: 3.14\nprint(read_value(\"text\"))  # String value: text\nprint(read_value(None))    # No value\n</code></pre> <p>In this example, the <code>read_value</code> function can handle <code>int</code>, <code>float</code>, <code>str</code>, and <code>None</code>.</p> <p>Sure! Here is the updated section for Overloads with the corrected example:</p>"},{"location":"how-to-guides/type-checking-mypy/#guide-to-using-overloads-and-generics-with-mypy","title":"Guide to Using Overloads and Generics with Mypy","text":""},{"location":"how-to-guides/type-checking-mypy/#introduction_2","title":"Introduction","text":"<p>Two advanced features in mypy are <code>Overloads</code> and <code>Generics</code>, which provide more flexibility and expressiveness in type annotations.</p>"},{"location":"how-to-guides/type-checking-mypy/#overloads","title":"Overloads","text":"<p>Function overloading allows you to define multiple signatures for a function, enabling different behaviors based on input types. The <code>overload</code> decorator from the <code>typing</code> module is used to achieve this in Python.</p>"},{"location":"how-to-guides/type-checking-mypy/#syntax_2","title":"Syntax","text":"<pre><code>from typing import overload\n\n@overload\ndef process(value: int) -&gt; int:\n    ...\n\n@overload\ndef process(value: str) -&gt; str:\n    ...\n\ndef process(value):\n    if isinstance(value, int):\n        return value * 2\n    elif isinstance(value, str):\n        return value.upper()\n\n# Example usage\nprint(process(10))    # 20\nprint(process(\"hello\"))  # HELLO\n</code></pre> <p>In this example, the <code>process</code> function has two overloaded signatures: one for <code>int</code> and one for <code>str</code>. The implementation combines both cases and returns different results based on the input type.</p>"},{"location":"how-to-guides/type-checking-mypy/#example_3","title":"Example","text":"<pre><code>from typing import overload, Union\n\n@overload\ndef add(a: int, b: int) -&gt; int:\n    ...\n\n@overload\ndef add(a: float, b: float) -&gt; float:\n    ...\n\n@overload\ndef add(a: str, b: str) -&gt; str:\n    ...\n\ndef add(a: Union[int, float, str], b: Union[int, float, str]) -&gt; Union[int, float, str]:\n    if isinstance(a, int) and isinstance(b, int):\n        return a + b\n    elif isinstance(a, float) and isinstance(b, float):\n        return a + b\n    elif isinstance(a, str) and isinstance(b, str):\n        return a + b\n    else:\n        raise TypeError(\"Unsupported types\")\n\n# Example usage\nprint(add(1, 2))        # 3\nprint(add(1.5, 2.5))    # 4.0\nprint(add(\"Hello, \", \"World!\"))  # Hello, World!\n</code></pre> <p>Here, the <code>add</code> function is overloaded to handle <code>int</code>, <code>float</code>, and <code>str</code> types, each performing the appropriate addition operation. The actual implementation uses type checks to ensure that the parameters are of the correct type, and raises a <code>TypeError</code> if they are not.</p> <p>Mypy will validate that the <code>add</code> function calls match one of the overloaded signatures and ensure type consistency. This ensures that the <code>add</code> function will correctly handle integers, floats, and strings and that <code>mypy</code> can verify the type correctness.</p>"},{"location":"how-to-guides/type-checking-mypy/#generics","title":"Generics","text":"<p>Generics allow you to write functions and classes that can operate on a variety of types while maintaining type safety. The <code>TypeVar</code> class from the <code>typing</code> module is used to define generic types.</p>"},{"location":"how-to-guides/type-checking-mypy/#syntax_3","title":"Syntax","text":"<pre><code>from typing import TypeVar, Generic, List\n\nT = TypeVar('T')\n\ndef get_first_element(elements: List[T]) -&gt; T:\n    return elements[0]\n\n# Example usage\nprint(get_first_element([1, 2, 3]))         # 1\nprint(get_first_element([\"a\", \"b\", \"c\"]))   # \"a\"\n</code></pre> <p>In this example, the <code>get_first_element</code> function is generic and can operate on a list of any type, returning an element of the same type.</p>"},{"location":"how-to-guides/type-checking-mypy/#generic-classes","title":"Generic Classes","text":"<p>You can also create generic classes using <code>Generic</code>.</p> <pre><code>from typing import TypeVar, Generic\n\nT = TypeVar('T')\n\nclass Box(Generic[T]):\n    def __init__(self, content: T) -&gt; None:\n        self.content = content\n\n    def get_content(self) -&gt; T:\n        return self.content\n\n# Example usage\nint_box = Box(123)\nstr_box = Box(\"Hello\")\n\nprint(int_box.get_content())  # 123\nprint(str_box.get_content())  # Hello\n</code></pre> <p>In this example, the <code>Box</code> class is generic and can store content of any type.</p>"},{"location":"how-to-guides/type-checking-mypy/#example-with-multiple-type-variables","title":"Example with Multiple Type Variables","text":"<pre><code>from typing import TypeVar, Generic\n\nT = TypeVar('T')\nU = TypeVar('U')\n\nclass Pair(Generic[T, U]):\n    def __init__(self, first: T, second: U) -&gt; None:\n        self.first = first\n        self.second = second\n\n    def get_first(self) -&gt; T:\n        return self.first\n\n    def get_second(self) -&gt; U:\n        return self.second\n\n# Example usage\npair = Pair(1, \"one\")\nprint(pair.get_first())   # 1\nprint(pair.get_second())  # one\n</code></pre> <p>Here, the <code>Pair</code> class uses two type variables <code>T</code> and <code>U</code>, allowing it to store and return two values of different types.</p>"},{"location":"how-to-guides/using-code-tags/","title":"Guide to Using Code Tags in ML/AI Projects","text":""},{"location":"how-to-guides/using-code-tags/#overview","title":"Overview","text":"<p>Code tags are an essential tool for maintaining readability and manageability in your codebase. They provide clear markers for important comments, issues, and reminders, enhancing collaboration and ensuring critical information is not overlooked. This guide explains why and how to use code tags in your ML/AI projects, leveraging the VS Code extensions CodeTags and Todo Highlight.</p>"},{"location":"how-to-guides/using-code-tags/#why-use-code-tags","title":"Why Use Code Tags?","text":"<ol> <li>Improve Readability: Highlight important comments, making them    stand out in the code.</li> <li>Facilitate Collaboration: Help team members quickly identify    issues, requests, and important notes.</li> <li>Enhance Documentation: Provide clear and consistent documentation    within the code.</li> <li>Ensure Accountability: Track who is responsible for various parts    of the code.</li> <li>Increase Efficiency: Quickly locate and address critical areas in    the codebase.</li> </ol>"},{"location":"how-to-guides/using-code-tags/#setting-up-vs-code","title":"Setting Up VS Code","text":"<p>To get started, ensure you have the following extensions installed:</p> <ul> <li>CodeTags</li> <li>Todo   Highlight</li> </ul> <p>The configuration for these extensions is located in your <code>.vscode/settings.json</code> file. This configuration includes specific keywords for code tags, their styling, and which file types to include or exclude. Here is a brief overview:</p> <ul> <li>Keywords: Defines various tags like <code>BUG</code>, <code>TODO</code>, <code>RFE</code>, etc.,   with specific colors and background settings.</li> <li>Include/Exclude: Specifies which file types and directories should   be included or excluded from highlighting.</li> </ul> <p>Example settings:</p> <pre><code>// #############################\n// ### CODETAGS ###\n// #############################\n\"todohighlight.keywords\": [\n    // Keywords and styling configurations...\n],\n\"codetags.custom\": [\n    // Custom tags and their descriptions...\n],\n\"todohighlight.include\": [\n    \"**/*.js\",\n    \"**/*.jsx\",\n    \"**/*.ts\",\n    \"**/*.tsx\",\n    \"**/*.html\",\n    \"**/*.php\",\n    \"**/*.css\",\n    \"**/*.scss\",\n    \"**/*.py\"\n],\n\"todohighlight.exclude\": [\n    \"**/node_modules/**\",\n    \"**/bower_components/**\",\n    \"**/dist/**\",\n    \"**/build/**\",\n    \"**/.vscode/**\",\n    \"**/.github/**\",\n    \"**/_output/**\",\n    \"**/*.min.*\",\n    \"**/*.map\",\n    \"**/.next/**\"\n]\n</code></pre>"},{"location":"how-to-guides/using-code-tags/#using-code-tags-in-your-project","title":"Using Code Tags in Your Project","text":"<p>Here are some common code tags and their intended use in ML/AI projects:</p> <ul> <li>BUG: To mark sections of code that have known bugs.</li> <li>NOBUG: To indicate well-known issues that will not be addressed.</li> <li>RFE: To mark requests for enhancements or features to be added.</li> <li>IDEA: To highlight ideas or potential improvements.</li> <li>???: To mark areas of the code that need further investigation.</li> <li>!!!: To indicate urgent issues that need immediate attention.</li> <li>PORT: To note portability issues or workarounds.</li> <li>CAVEAT: To point out non-intuitive implementation details.</li> <li>FAQ: To highlight frequently asked questions or important notes.</li> <li>GLOSS: To add glossaries or explanations of terms.</li> <li>SEE: To provide pointers to other parts of the code or external   resources.</li> <li>TODOC: To indicate areas that need documentation.</li> <li>CRED: To give credit for external help or contributions.</li> <li>STAT: To indicate the status or maturity of a file.</li> <li>RVD: To mark files or sections that have been reviewed.</li> <li>NOTE: To add general notes or important reminders.</li> <li>HACK: To mark quick and dirty fixes or hacks.</li> <li>TODO: To indicate tasks that need to be done.</li> </ul>"},{"location":"how-to-guides/using-code-tags/#example-usage","title":"Example Usage","text":"<p>Here are some examples of how to use these code tags in your Python code:</p> <pre><code># TODO: Implement the function to process the data\ndef process_data(data):\n    \"\"\"\n    Processes the input data.\n\n    Args:\n        data (list): The data to be processed.\n\n    Returns:\n        list: The processed data.\n    \"\"\"\n    pass\n\n# BUG: This function does not handle empty lists properly\ndef calculate_average(numbers):\n    \"\"\"\n    Calculates the average of a list of numbers.\n\n    Args:\n        numbers (list): A list of numbers.\n\n    Returns:\n        float: The average of the numbers.\n\n    Raises:\n        ValueError: If the input list is empty.\n    \"\"\"\n    if not numbers:\n        raise ValueError(\"Cannot calculate average of an empty list.\")\n    return sum(numbers) / len(numbers)\n\n# !!!: This section needs immediate attention to fix performance issues\ndef perform_heavy_computation(data):\n    \"\"\"\n    Performs a heavy computation on the input data.\n\n    Args:\n        data (list): The data to be processed.\n\n    Returns:\n        list: The result of the computation.\n    \"\"\"\n    # Placeholder for the heavy computation logic\n    result = [x * x for x in data]  # Optimize this line for better performance\n    return result\n\n# SEE: For more details on the algorithm, refer to the research paper (link).\n</code></pre>"},{"location":"how-to-guides/using-code-tags/#automation-with-codetags-and-todo-highlight","title":"Automation with CodeTags and Todo Highlight","text":"<p>By configuring the <code>todohighlight.keywords</code>, <code>todohighlight.include</code>, and <code>todohighlight.exclude</code> settings in your <code>.vscode/settings.json</code> file, you can automate the highlighting of important code tags, making it easier to manage and navigate your codebase.</p>"},{"location":"how-to-guides/using-code-tags/#conclusion","title":"Conclusion","text":"<p>Using code tags effectively helps maintain a clean, readable, and manageable codebase. By following this guide and using the provided VS Code extensions, you can enhance collaboration, improve documentation, and ensure critical areas of your code are highlighted and addressed promptly.</p>"},{"location":"how-to-guides/using-code-tags/#references","title":"References","text":"<ul> <li>Codetags and TODO Comments</li> <li>PEP 350 \u2013 Codetags</li> <li>Never Forget Anything Before, After and While Coding</li> </ul>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/","title":"Using the Todo Tree Extension with Code Tags in ML/AI Projects","text":""},{"location":"how-to-guides/using-todo-tree-with-code-tags/#overview","title":"Overview","text":"<p>The Todo Tree extension for VS Code helps you visualize and manage your code tags effectively. When combined with the previously discussed code tag extensions, this tool can greatly enhance your ability to track and organize TODO comments and other code tags in your machine learning or AI projects.</p>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/#why-use-the-todo-tree-extension","title":"Why Use the Todo Tree Extension?","text":"<p>The Todo Tree extension provides a powerful way to:</p> <ul> <li>Visualize Code Tags: See all TODO comments and other custom tags in a convenient tree view.</li> <li>Navigate Quickly: Jump to specific tags in your code directly from the tree view.</li> <li>Manage Tasks Efficiently: Keep track of what needs to be done, what issues exist, and other important notes directly within your codebase.</li> </ul>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/#configuring-todo-tree-with-code-tags","title":"Configuring Todo Tree with Code Tags","text":"<p>To integrate the Todo Tree extension with your existing code tags setup, follow these steps:</p> <ol> <li> <p>Install the Todo Tree Extension: If you haven't already, install the Todo Tree extension from the VS Code Marketplace.</p> <pre><code>Open VS Code, go to the Extensions view (Ctrl+Shift+X), and search for \"Todo Tree\". Click \"Install\".\n</code></pre> </li> <li> <p>Configure Todo Tree: Update your <code>.vscode/settings.json</code> file to include the relevant tags and settings for the Todo Tree extension.</p> <pre><code>// #############################\n// ### Todo Tree ###\n// #############################\n\"todo-tree.tree.showScanModeButton\": true,\n\"todo-tree.tree.showCountsInTree\": true,\n\"todo-tree.general.tags\": [\n    \"BUG:\",\n    \"NOBUG:\",\n    \"RFE:\",\n    \"IDEA:\",\n    \"???:\",\n    \"!!!:\",\n    \"PORT:\",\n    \"CAVEAT:\",\n    \"FAQ:\",\n    \"GLOSS:\",\n    \"SEE:\",\n    \"TODOC:\",\n    \"CRED:\",\n    \"STAT:\",\n    \"RVD:\",\n    \"NOTE:\",\n    \"HACK:\",\n    \"TODO:\"\n],\n\"todo-tree.regex.regex\": \"((//|#|&lt;!--|;|/\\\\*|^)\\\\s*($TAGS)|^\\\\s*- \\\\[ \\\\])\",\n\"todo-tree.regex.regexCaseSensitive\": false,\n\"todo-tree.general.tagGroups\": {\n    \"Bugs\": [\"BUG:\", \"NOBUG:\"],\n    \"Enhancements\": [\"RFE:\", \"IDEA:\"],\n    \"Questions\": [\"???:\", \"FAQ:\", \"SEE:\"],\n    \"Important\": [\"!!!:\", \"CAVEAT:\", \"NOTE:\"],\n    \"Other\": [\"PORT:\", \"GLOSS:\", \"TODOC:\", \"CRED:\", \"STAT:\", \"RVD:\", \"HACK:\", \"TODO:\"]\n}\n</code></pre> </li> <li> <p>File Inclusion and Exclusion: Make sure the Todo Tree extension includes and excludes the appropriate files by adding these settings to your <code>.vscode/settings.json</code> file.</p> <pre><code>\"todohighlight.include\": [\n    \"**/*.js\",\n    \"**/*.jsx\",\n    \"**/*.ts\",\n    \"**/*.tsx\",\n    \"**/*.html\",\n    \"**/*.php\",\n    \"**/*.css\",\n    \"**/*.scss\",\n    \"**/*.py\"\n],\n\"todohighlight.exclude\": [\n    \"**/node_modules/**\",\n    \"**/bower_components/**\",\n    \"**/dist/**\",\n    \"**/build/**\",\n    \"**/.vscode/**\",\n    \"**/.github/**\",\n    \"**/_output/**\",\n    \"**/*.min.*\",\n    \"**/*.map\",\n    \"**/.next/**\"\n],\n\"todohighlight.defaultStyle\": {},\n</code></pre> </li> <li> <p>Using Todo Tree: Once configured, you can use the Todo Tree extension to view and manage your code tags:</p> <ul> <li>Open Todo Tree: Open the Todo Tree view by clicking on the Todo Tree icon in the Activity Bar on the side of VS Code.</li> <li>View Tags: You will see a tree structure listing all your tags under their respective groups.</li> <li>Navigate to Tags: Click on any tag in the tree to jump to its location in your code.</li> </ul> </li> </ol>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/#example-usage","title":"Example Usage","text":""},{"location":"how-to-guides/using-todo-tree-with-code-tags/#before-adding-tags","title":"Before Adding Tags","text":"<pre><code># This function calculates the sum of two numbers\ndef add(a, b):\n    return a + b\n\n# TODO: Add error handling for non-numeric inputs\n</code></pre>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/#after-adding-tags","title":"After Adding Tags","text":"<pre><code># This function calculates the sum of two numbers\ndef add(a, b):\n    return a + b\n\n# TODO: Add error handling for non-numeric inputs\n# BUG: Fix the issue with negative numbers\n# IDEA: Optimize this function for large datasets\n# NOTE: This function is part of the core module\n</code></pre>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/#visualizing-tags-with-todo-tree","title":"Visualizing Tags with Todo Tree","text":"<p>Here is an example of how the Todo Tree extension helps visualize code tags within VS Code:</p> <p></p> <p>In this screenshot, you can see how various tags are highlighted and organized in a tree structure. This allows you to quickly identify and navigate to specific tags within your codebase.</p>"},{"location":"how-to-guides/using-todo-tree-with-code-tags/#conclusion","title":"Conclusion","text":"<p>Using the Todo Tree extension in conjunction with your custom code tags setup allows for efficient management of tasks, bugs, ideas, and notes within your codebase. This setup helps ensure that important items are not overlooked and that your project remains well-organized and maintainable.</p> <p>For more detailed information and advanced configuration options, refer to the Todo Tree extension documentation.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/","title":"ML Experiments Life-Cycle with Weights &amp; Biases","text":"<p>Manage the life-cycle of your AI experiments effectively using Weights &amp; Biases by following this structured guide. Ensure reproducibility, scalability, and collaboration through each phase of your experiment.</p> <p>Getting Started</p> <p>Before you begin documenting your experiment, familiarize yourself with the goals and hypotheses of your experiment. This will enable you to create clear and effective documentation.</p> <pre><code>---\ntitle: ML Experiment Life-Cycle with W&amp;B\n---\nflowchart TB\n    A(1. Create Issue/Story) --&gt; B(2. Branching for the Experiment)\n    B --&gt; C(3. Coding the Experiment)\n    C --&gt; D(4. Configuring W&amp;B)\n    D --&gt; |\"Commit Changes \u26a0\ufe0f\"| E(5. Running the Experiment)\n    E --&gt; F(6. Analyzing Results)\n    F --&gt; G(7. Refining the Experiment)\n    G --&gt; |Iterate| C\n    G --&gt; H(8. Document)\n    H --&gt; |Create a New Experiment| A</code></pre>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#how-to-create-a-github-issue-or-a-jira-story-for-your-experiment","title":"How to Create a GitHub Issue or a Jira Story for Your Experiment","text":"<p>Documenting your experiment's intent by opening a GitHub issue or Jira story is the first crucial step in the experiment life-cycle.</p> <p>Create an Issue/Story for Each Family of Experiments</p> <p>This ensures consistency and traceability within your  project.</p> <p>Use a Template</p> <p>Select the following a pre-defined template that fits experiment documentation: Experiment Issue/Story Template</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#preparation","title":"Preparation","text":"<ol> <li>Understand the Experiment:    Before you start documenting, ensure you have a clear understanding    of the experiment's goals and the methods you'll use.</li> </ol>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#documentation-steps","title":"Documentation Steps","text":"<ol> <li> <p>Create a New Issue/Story: Navigate to your project's GitHub    repository or Jira board and initiate a new issue or story.</p> </li> <li> <p>Use a Template: If available use a pre-defined template that    fits experiment documentation.</p> </li> <li> <p>Title Your Issue/Story: Start with a succinct and descriptive    title, prefixed with 'experiment/' followed by a brief identifier and    issue number or Jira key.</p> </li> <li> <p>Describe Your Experiment: Provide a detailed description of the    experiment's background, objectives, and hypothesis.</p> </li> <li> <p>List Your Goals: Clearly outline the specific goals or questions    your experiment aims to address.</p> </li> <li> <p>Link to Resources: Include links to any relevant documents, code    repositories, or previous experiments.</p> </li> <li> <p>Ensure Visibility: Make sure all potential stakeholders have    visibility and encourage team members to comment.</p> </li> <li> <p>Submit the Documentation: Once all details are filled in, submit    your issue or story.</p> </li> </ol> <p>To reformat the guide for branch naming and management using the MkDocs Material theme, you would utilize a variety of the theme's features to enhance the presentation and usability of the document. Here's how you could write the markdown with Material for MkDocs features:</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#how-to-create-a-new-experiment-branch","title":"How to Create a New Experiment Branch","text":"<p>This guide will provide a structured approach to creating a consistent branch naming convention for your experiment.</p> <p>Create a New Branch for Each Family of Experiments.</p> <p>This ensures consistency and traceability within your project.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#create-a-naming-convention","title":"Create a Naming Convention","text":"<p>Structure your branch names to include the category, description, and issue or Jira key.</p> <pre><code>experiment/&lt;description&gt;-&lt;issue_number_or_jira_key&gt;\n</code></pre> <p>For example:</p> <pre><code>experiment/rag-fusion-GH15\n</code></pre> <p>Naming Convention</p> <p>Choose names that are short yet descriptive to convey the branch's purpose at a glance and provide context for the work being  done.</p> <p>Best Practices for Experiment Branch Naming Conventions</p> <p>For detailed insights and guidelines on establishing effective naming conventions for your experiment branches, please refer to the following resource: Experiment Branch Naming Convention.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#how-to-develop-python-code-for-ml-experiments","title":"How to Develop Python Code for ML Experiments","text":"<p>Developing Python code for machine learning (ML) experiments is a critical phase where your experimental concepts are transformed into executable models and analyses. This how-to guide outlines a structured approach to code development, focusing on integrating advanced ML components like LangChain RAG Fusion into your projects.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#preparing-for-development","title":"Preparing for Development","text":"<p>Before diving into coding, it's essential to align your development work with the experiment's goals and requirements.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#review-experiment-documentation","title":"Review Experiment Documentation","text":"<ul> <li>Objective: Ensure your development efforts are guided by the   experiment's defined goals and success criteria.</li> <li>Action: Revisit the experiment's description, goals, and success   criteria detailed in the GitHub issue or Jira story.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#conduct-technical-research","title":"Conduct Technical Research","text":"<ul> <li>Objective: Gain a deep understanding of the technologies and   components you plan to integrate, such as LangChain RAG Fusion.</li> <li>Action: Review technical documentation, research papers, and   source code related to the technologies in use.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#implementing-your-experiment","title":"Implementing Your Experiment","text":"<p>Follow these steps to implement your experiment within your Python codebase.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#data-preparation","title":"Data Preparation","text":"<ul> <li>Objective: Prepare your data to meet the input requirements of   your ML components.</li> <li>Action:</li> <li>Write scripts for data preprocessing.</li> <li>Ensure data formats are compatible with LangChain RAG Fusion.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#model-integration","title":"Model Integration","text":"<ul> <li>Objective: Seamlessly integrate the LangChain RAG Fusion component   into your ML pipeline.</li> <li>Action:</li> <li>Adapt existing code to accommodate the new component.</li> <li>Define new interfaces or integrate third-party libraries as      necessary.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#configuration-and-parameters","title":"Configuration and Parameters","text":"<p>At this stage, identify your experiment's hyperparameters and hardcode them into your Python code. This decision is crucial for the experiment's setup.</p> <p>Identify Hyperparameters</p> <p>Determine your experiment's hyperparameters now. We'll introduce dynamic parameter configuration with Weights &amp; Biases (W&amp;B) in the next section.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#logging-and-monitoring","title":"Logging and Monitoring","text":"<p>Understanding what metrics and outputs to log is essential. The detailed setup for logging and monitoring will be covered next, using W&amp;B.</p> <p>Logging Setup Ahead</p> <p>Prepare for logging by identifying key metrics. Dynamic logging and monitoring with W&amp;B will be detailed in the following section.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#ensuring-code-quality-and-reliability","title":"Ensuring Code Quality and Reliability","text":"<p>Maintain high standards of code quality and reliability throughout the development process.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#write-unit-tests","title":"Write Unit Tests","text":"<ul> <li>Objective: Verify that individual components of your code operate   correctly.</li> <li>Action:</li> <li>Create unit tests for critical functions and modules.</li> <li>Use a testing framework like pytest to automate testing.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#conduct-integration-testing","title":"Conduct Integration Testing","text":"<ul> <li>Objective: Ensure that different components of your system work   together as intended.</li> <li>Action:</li> <li>Design and run integration tests after integrating new components.</li> <li>Pay special attention to the interaction between LangChain RAG      Fusion and other parts of your system.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#perform-experiment-runs","title":"Perform Experiment Runs","text":"<ul> <li>Objective: Validate the overall setup and functionality of your   experiment.</li> <li>Action:</li> <li>Conduct initial small-scale runs to test the experimental setup.</li> <li>Adjust code and configurations based on initial outcomes.</li> </ul> <p>Continuous Improvement</p> <p>Treat the development process as iterative. Use insights from testing and initial experiment runs to continuously refine and improve your code.</p> <p>By following this guide, you'll develop Python code for your ML experiments that is not only aligned with your scientific goals but also adheres to best practices in software engineering and ML research.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#how-to-configure-and-track-ml-experiments-with-weights-biases","title":"How to Configure and Track ML Experiments with Weights &amp; Biases","text":"<p>Configuring your machine learning experiments with Weights &amp; Biases (W&amp;B) is crucial for effective tracking, logging, and optimization. Follow this guide to set up W&amp;B for managing and automating your experiments.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#setting-up-wb-for-your-experiments","title":"Setting Up W&amp;B for Your Experiments","text":""},{"location":"how-to-guides/wandb-experiment-tracking-rag/#create-a-sweep-configuration","title":"Create a Sweep Configuration","text":"<ol> <li>Objective: Define the experiment's hyperparameters and their    search space.</li> <li>Action: Write a <code>sweep_config.yaml</code> file detailing parameters and    search strategies (grid, random, Bayesian).</li> <li>Version Control: Ensure this file and other configuration files    are under version control for reproducibility.</li> </ol>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#adapt-your-code-for-wb-integration","title":"Adapt Your Code for W&amp;B Integration","text":"<p>Integrating W&amp;B into your Python code allows dynamic parameter adjustment and comprehensive metric logging.</p> <ol> <li>Dynamic Parameters:</li> <li>Modify code to fetch and use parameters from W&amp;B dynamically.</li> <li> <p>Utilize the W&amp;B API to implement this functionality.</p> </li> <li> <p>Logging Metrics:</p> </li> <li>Enhance your code to log training and evaluation metrics to W&amp;B.</li> <li> <p>Ensure both outcome metrics and intermediate metrics are logged for      in-depth analysis.</p> </li> <li> <p>Automate Hyperparameter Search with Sweeps:</p> </li> <li>Set up W&amp;B Sweeps in your code for automated hyperparameter      optimization.</li> <li>Handle sweep initialization, metric reporting, and parameter      updates within your code.</li> </ol> <p>Double-Check Experiment Reproducibility</p> <p>Ensure you're logging all necessary components to guarantee experiment reproducibility. This includes:</p> <ul> <li>All Metrics: Verify that every relevant metric is being captured.</li> <li>Modified Python Scripts: Keep track of any changes in your Python scripts.</li> <li>Configuration Files: Ensure all configuration files are logged and version-controlled.</li> <li>Notes and Labels: Add descriptive notes and labels to your logged data for clarity.</li> </ul> <p>Reproducibility is key to validating and sharing your ML experiments. Missing data can lead to incomplete insights and challenges in replicating results. Always review your logging setup to ensure comprehensive coverage of your experiment's artifacts.</p> <p>Best Practice: Use W&amp;B's API Documentation</p> <p>Refer to W&amp;B's API documentation for detailed instructions on implementing these features in your code.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#experiment-tracking-best-practices","title":"Experiment Tracking Best Practices","text":"<p>Maximize the benefits of W&amp;B for your ML experiments with these tracking best practices.</p> <ol> <li>Comprehensive Logging:</li> <li> <p>Log every relevant piece of experiment data, including      configurations, results, and binaries.</p> </li> <li> <p>Iterative Improvement:</p> </li> <li> <p>Use logged data to iteratively refine your experiments, aiming for      continuous improvement.</p> </li> <li> <p>Collaboration:</p> </li> <li>Share experiment results on W&amp;B's platform for collaborative      analysis and discussion.</li> </ol> <p>Collaboration is Key</p> <p>Encourage team engagement by sharing insights and discussing experiment results through W&amp;B.</p> <p>Integrating W&amp;B into your ML experiment workflow enhances tracking, optimization, and collaboration. By following this guide, you'll establish a solid foundation for managing your experiments, leading to more efficient and impactful research outcomes.</p> <p>To reformat the section on committing code changes before running experiments using the MkDocs Material theme, you would use Markdown enhanced with Material theme's features such as admonitions, code blocks, and tips. Here's how the reformatted section could look:</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#committing-code-changes-before-running-ml-experiments","title":"Committing Code Changes Before Running ML Experiments","text":"<p>Ensuring that your code changes are committed before running machine learning experiments is critical for maintaining reproducibility and facilitating collaboration.</p> <p>Reproducibility &amp; Collaboration</p> <p>Committing changes before experiments tie W&amp;B runs to a specific code version, crucial for tracing experiment conditions and results.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#importance-of-pre-experiment-commits","title":"Importance of Pre-Experiment Commits","text":"<ul> <li>Reproducibility: Committing changes before running experiments   allows you to associate each W&amp;B experiment run with the exact version   of the code used. This is crucial for reproducibility, as it enables   you and others to revisit and understand the conditions under which an   experiment was conducted.</li> <li>Tracking and Collaboration: By committing before experiments, you   ensure that the git SHA (a unique identifier for each commit) is   captured by W&amp;B. This allows team members to easily track which code   changes correspond to which experiment results, facilitating   collaboration and review.</li> </ul>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#steps-for-committing-code-changes","title":"Steps for Committing Code Changes","text":"<ol> <li>Review Your Changes: Before committing, review your changes to    ensure that they are complete and aligned with the experiment's    objectives. This includes new code, modifications, configuration    files, and any documentation updates.</li> <li>Stage Your Changes: Use <code>git add</code> to stage the files you intend    to commit. This step allows you to select exactly which changes you    want to include in your commit.</li> <li>Commit with a Meaningful Message: Commit your staged changes    using <code>git commit</code>, providing a descriptive message that clearly    summarizes the changes made and their purpose. This message is    invaluable for historical context and understanding the evolution of    your project.</li> <li>Push to Remote Repository: Push your commit to the remote    repository to ensure it is shared with your team and integrated into    the project's shared history. This step is critical for remote    collaboration and backup.</li> </ol> <p>Committing code changes before running experiments is a critical practice that underpins the reliability and integrity of your machine learning projects. It ensures that every experiment conducted with W&amp;B is traceable to a specific code state, thereby enhancing reproducibility, facilitating collaboration, and supporting rigorous scientific inquiry. By embedding this practice into your workflow, you establish a robust foundation for conducting and sharing machine learning experiments.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#how-to-run-experiments-and-manage-outcomes-with-weights-biases","title":"How to Run Experiments and Manage Outcomes with Weights &amp; Biases","text":"<p>Running experiments with Weights &amp; Biases (W&amp;B) and managing the outcomes efficiently is crucial for the success of machine learning projects. This guide provides a structured approach to executing experiments and handling the results, whether integrating successful outcomes into production or documenting learnings from unsuccessful attempts.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#initiating-experiments-with-wb","title":"Initiating Experiments with W&amp;B","text":""},{"location":"how-to-guides/wandb-experiment-tracking-rag/#start-your-wb-runs","title":"Start Your W&amp;B Runs","text":"<ol> <li>Objective: Log parameters, metrics, and outcomes of your experiment.</li> <li>Action: Ensure your code initializes a W&amp;B run at the start of your experiment script.</li> </ol>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#automate-with-wb-sweeps","title":"Automate with W&amp;B Sweeps","text":"<ol> <li>Objective: Optimize hyperparameters and manage multiple configurations efficiently.</li> <li>Action: Set up W&amp;B Sweeps in your <code>sweep_config.yaml</code> and initiate them for automated parameter exploration.</li> </ol>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#monitor-and-analyze-results","title":"Monitor and Analyze Results","text":"<ol> <li>Objective: Keep track of experiment progress and analyze data for insights.</li> <li>Action: Use the W&amp;B dashboard for real-time monitoring and result analysis.</li> </ol>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#experiment-branch-strategy","title":"Experiment Branch Strategy","text":""},{"location":"how-to-guides/wandb-experiment-tracking-rag/#for-successful-experiments","title":"For Successful Experiments","text":"<pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base', 'gitGraph': {'rotateCommitLabel': true} } }%%\ngitGraph TB:\n    commit tag: \"Initial commit\"\n    branch experiment\n    checkout experiment\n    commit tag: \"Experiment changes\"\n    commit tag: \"Interate Experiment\"\n    commit tag: \"Successful approach found\"\n    checkout main\n    commit tag: \"Changes on master\"\n    merge experiment tag: \"Merge successful experiment into master\"\n    commit tag: \"Changes on master\"</code></pre> <ol> <li>Opening Pull Requests: Prepare and open a PR for successful experiment branches, clearly documenting the experiment and its findings.</li> <li>Peer Review: Undergo a review process, providing links to W&amp;B pages for detailed metrics.</li> <li>Documentation: Record the experiment's methodology and findings in the project's main documentation.</li> </ol>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#for-unsuccessful-experiments","title":"For Unsuccessful Experiments","text":"<pre><code>%%{init: { 'logLevel': 'debug', 'theme': 'base' } }%%\ngitGraph TB:\n  commit tag: \"Initial commit\"\n  branch experiment\n  checkout experiment\n  commit tag: \"First Commit on experiment\"\n  commit tag: \"Iterate Experiment\"\n  commit tag: \"Failed Experiment\"</code></pre> <ol> <li>Keep Branches Open: Retain the experimental branch open for documentation and to avoid repeating unsuccessful experiments.</li> <li>Analysis and Documentation: Document the approach, results, and learnings from the experiment in the repository.</li> </ol> <p>Comprehensive Guide to Branch Strategy</p> <p>For an in-depth exploration of best practices and strategies for managing branches within your projects, consult our dedicated guide: Branch Strategy.</p>"},{"location":"how-to-guides/wandb-experiment-tracking-rag/#best-practices","title":"Best Practices","text":"<ul> <li>Batch Running and Automation: Use scripts or CI/CD pipelines for automating experiments, especially when leveraging W&amp;B Sweeps.</li> <li>Iterative Improvement: Apply insights from all experiments to refine future experimental designs.</li> <li>Collaboration and Sharing: Promote a collaborative environment by sharing results and insights within the team and using W&amp;B for discussions.</li> </ul>"},{"location":"tutorials/black-formatter/","title":"Using Black Code Formatter in Your Project","text":"<p>Running Black Formatter</p> <p>To format your Python script with Black, run the following command in your terminal:</p> <pre><code>black sample_script.py\n</code></pre> <p>This command will reformat your <code>sample_script.py</code> file according to Black's style guidelines.</p> <p>Black is an uncompromising code formatter. It ensures that your Python code adheres to a consistent, readable style. The setup you have thanks to the Cookiecutter template makes it even more convenient to use Black in various contexts: Python scripts, Jupyter notebooks, and during git operations. Below are the key points and examples to help you understand how to use Black in these scenarios.</p>"},{"location":"tutorials/black-formatter/#vs-code-integration","title":"VS Code Integration","text":"<p>When you're writing code in VS Code, Black can manually format your code. Here's how:</p> <p>Note</p> <p>In this Cookiecutter template, we've configured Black to be executed manually rather than the default automatic approach upon saving. This decision offers users greater precision and control, especially in Jupyter notebooks where maintaining context and flow is crucial.</p>"},{"location":"tutorials/black-formatter/#formatting-python-scripts-and-jupyter-notebooks-with-black","title":"Formatting Python scripts and Jupyter Notebooks with Black","text":"<p>When working with Python scripts and Jupyter notebooks in VS Code, you can manually format it by pressing <code>Shift + Opt + F</code> or right-click and select <code>Format Document</code>, <code>Format Cell</code> or <code>Format Notebook</code>.</p> <p>Black Formatting in Action</p> <p>Before Formatting: <pre><code>def calculate(a,b):result=a+b; return result\n</code></pre></p> <p>After Formatting:  <pre><code>def calculate(a, b):\n    result = a + b\n    return result\n</code></pre></p> <p>VS Code Output Window Log</p> <p>When Black formats a script or a notebook, the VS Code output window for the Black Formatter provides a detailed log of the operation.</p> <p>Here's an example of what you might see:</p> <pre><code>2023-09-20 12:38:00.934 [info] Sending request 'textDocument/formatting - (5)'.\n2023-09-20 12:38:00.936 [info] Received notification 'window/logMessage'.\n2023-09-20 12:38:00.936 [info] Running: /Users/markeyser/Library/Caches/pypoetry/virtualenvs/cookiecutter_rag-zDtOoUN1-py3.11/bin/python -m black --stdin-filename /Users/markeyser/Projects/ccookiecutter-genai-ml-ai/checkformatter.py\n2023-09-20 12:38:00.938 [info] CWD formatter: /Users/markeyser/Projects/ccookiecutter-genai-ml-ai\n2023-09-20 12:38:00.938 [info] reformatted /Users/markeyser/Projects/ccookiecutter-genai-ml-ai/checkformatter.py\nAll done! \u2728 \ud83c\udf70 \u2728\n1 file reformatted.\n2023-09-20 12:38:00.940 [info] Sending notification 'textDocument/didChange'.\n</code></pre> <p>This log provides insights into the formatting process, including the file being formatted, the Black command being executed, and the result of the formatting operation.</p>"},{"location":"tutorials/black-formatter/#command-line-formatting","title":"Command Line Formatting","text":"<p>For those who prefer using the terminal or need to format multiple files at once, Black can be run directly from the command line.</p> <p>Formatting Commands</p> <ul> <li> <p>For Python scripts, run:   <pre><code>black your_script.py\n</code></pre></p> </li> <li> <p>For Jupyter notebooks, run:    <pre><code>black your_notebook.ipynb\n</code></pre></p> </li> </ul> <p>Formatting with Black from the Terminal</p> <p>Suppose you've written a Python script named <code>sample_script.py</code> with the following content:</p> <pre><code>def calculate(a,b):result=a+b; return result\n</code></pre> <p>To format <code>sample_script.py</code> using Black, navigate to the directory containing the script and run:</p> <pre><code>black sample_script.py\n</code></pre> <p>Terminal Output:</p> <pre><code>reformatted sample_script.py\nAll done! \u2728 \ud83c\udf70 \u2728\n1 file reformatted.\n</code></pre> <p>Formatted Code:</p> <p>After running the command, your <code>sample_script.py</code> will be updated to:</p> <pre><code>def calculate(a, b):\n    result = a + b\n    return result\n</code></pre>"},{"location":"tutorials/black-formatter/#pre-commit-hooks-in-git","title":"Pre-commit Hooks in Git","text":"<p>Thanks to the pre-configured hooks, any attempt to commit unformatted Python code will trigger Black to automatically format the code for you.</p> <p>Git Commit with Black</p> <p>Suppose you've written the following code in <code>sample_script.py</code>:</p> <pre><code>def calculate(a,b):result=a+b; return result\n</code></pre> <p>You decide to commit your changes:</p> <pre><code>git add sample_script.py\ngit commit -m \"Add example function\"\n</code></pre> <p>The pre-commit hook will detect the unformatted code and output a message:</p> <pre><code>black....................................................................Failed\n- hook id: black\n- files were modified by this hook\nreformatted sample_script.py\n\nAll done! \u2728 \ud83c\udf70 \u2728\n1 file reformatted.\n</code></pre> <p>Your commit will be halted, but Black will have automatically formatted your file. You can simply run the commit command again, and this time it will succeed with the newly formatted code.</p>"},{"location":"tutorials/black-formatter/#automated-formatting-with-github-actions","title":"Automated Formatting with GitHub Actions","text":"<p>Upon utilizing the pre-commit hooks for Black, our repository takes an extra step to ensure code consistency. We've integrated a GitHub Action, defined in <code>.github/workflows/black.yml</code>, that automatically checks and formats the code whenever new changes are introduced, be it through a push or a pull request.</p> <p>This GitHub Action serves a dual purpose:</p> <ol> <li> <p>Automated Code Review: Before any code merges into the main    branch, the action ensures it adheres to our predefined formatting    standards. This guarantees a consistent code style across the    repository, irrespective of the number of contributors or their    individual coding environments.</p> </li> <li> <p>Auto-Fixing Formatting Issues: If the action detects any code    that doesn't align with our formatting standards, it doesn't just    stop at notifying the discrepancy. Instead, it takes a proactive step    by automatically formatting the code using Black. Once formatted, it    commits these changes back to the repository. This automation    minimizes manual interventions, offering a smoother development    experience. However, a side effect to be aware of is the potential    addition of extra commits to the git history. While this might make    the commit log appear denser, we believe the trade-off is worth the    convenience and consistency it brings to the development process.</p> </li> </ol> <p>When the GitHub Action runs, you might see an output similar to the following in the action's log:</p> <pre><code>reformatted sample_script.py\nAll done! \u2728 \ud83c\udf70 \u2728\n1 file reformatted.\n</code></pre> <p>This output confirms that the action has successfully reformatted the code to adhere to our standards.</p>"},{"location":"tutorials/black-formatter/#skipping-formatting-using-black","title":"Skipping Formatting Using Black","text":"<p>If for some reason you need a section of your code to remain unformatted by Black (for example, due to alignment preferences or specific readability concerns), you can tell Black to skip that section.</p> <p>Black Directives for Skipping Formatting</p> <p>To skip formatting for a particular section, you can wrap that section of code between <code># fmt: off</code> and <code># fmt: on</code> comments.</p> <p>Example:</p> <pre><code>def transformation_matrix():\n    # fmt: off\n    matrix = [\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1]\n    ]\n    # fmt: on\n    return matrix\n</code></pre> <p>When you run Black on this snippet, the matrix remains untouched, while the rest of your code (if there were any other parts) would still get formatted.</p> <p>If you were to format the above with Black without using the comments, it might put the entire matrix on a single line if it's short enough, or it might space things out differently.</p> <pre><code>def transformation_matrix():\n    matrix = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n    return matrix\n</code></pre>"},{"location":"tutorials/black-formatter/#formatting-multiple-python-files","title":"Formatting Multiple Python Files","text":"<p>Formatting multiple Python files with Black can be efficiently handled in a few ways, depending on your project's structure and your personal workflow. Here are some approaches:</p> <p>Format Entire Directory: If all your Python files are in a specific    directory (or directories), you can format them all at once by    running Black on the directory. For example:</p> <pre><code>black path/to/directory/\n</code></pre> <p>Format Specific File Types: If you have a mix of file types and only    want to format the Python files, you can use a command like:</p> <pre><code>find . -type f -name \"*.py\" -exec black {} +\n</code></pre> <p>This command uses <code>find</code> to locate all <code>.py</code> files and <code>black</code> to    format each one.</p> <p>Integrate with Version Control: If you're using a version control    system like Git, you can format only the Python files that have    changed. This can be particularly useful in a collaborative    environment. For example, to format all Python files that have been    modified but not yet committed:</p> <pre><code>git diff --name-only --staged | grep '\\.py$' | xargs black\n</code></pre> <p>Choose the approach that best fits your workflow and project structure. Automation and integration with your development tools can significantly streamline the formatting process.</p> <p>Branch Protection with Black</p> <p>To ensure code quality, we need to protect the main branch with branch protection rules. This setup requires that all code must pass Black formatting checks before being merged into the main branch. While you can push code to your feature branches even if it fails formatting, merging into main is blocked until all formatting errors are resolved. This approach mimics pre-commit hooks, maintaining a clean and compliant main branch. Developers should run Black locally and fix issues before pushing code for review.</p>"},{"location":"tutorials/black-formatter/#conclusion","title":"Conclusion","text":"<p>Black ensures consistency and readability across your codebase. With the integrations provided by the Cookiecutter template, it's easier than ever to maintain a clean code style, whether you're working in Python scripts, Jupyter notebooks, or committing changes via git. Remember, a consistent code style not only makes your codebase look professional but also reduces cognitive load, making it easier for you and others to understand and collaborate on the project.</p>"},{"location":"tutorials/black-formatter/#further-reading-and-resources","title":"Further Reading and Resources","text":"<p>For a deeper dive into the functionalities and best practices of using Black, the following resource is highly recommended:</p> <ul> <li>Official Black Documentation</li> </ul> <p>The official documentation provides a comprehensive guide to Black's features, configuration options, and integration methods. It's an excellent resource for understanding the philosophy behind Black and how to make the most out of this powerful code formatter. Whether you're new to Black or looking to explore its more advanced features, the official documentation is a valuable resource.</p>"},{"location":"tutorials/creating-managing-metadata-documentation/","title":"Creating Effective Metadata for Your Data Projects","text":""},{"location":"tutorials/creating-managing-metadata-documentation/#step-by-step-tutorial-on-crafting-metadata","title":"Step-by-Step Tutorial on Crafting Metadata","text":"<ol> <li> <p>Understanding Metadata: Begin by understanding what metadata is and why it's crucial for your data project.</p> </li> <li> <p>Identify Metadata Elements: List all essential elements that should be included in your metadata (e.g., data source, format, collection date, modifications).</p> </li> <li> <p>Choose a Format: Decide on a metadata format (like JSON, XML) that suits your project needs and is compatible with your tools.</p> </li> <li> <p>Creating Metadata Structure: Develop a template or structure for your metadata, ensuring it's organized and comprehensive.</p> </li> <li> <p>Filling in the Details: Populate your metadata template with details specific to your data. This could include data source, collection methods, preprocessing steps, etc.</p> </li> <li> <p>Validation: Validate the metadata to ensure all necessary information is included and correctly formatted.</p> </li> <li> <p>Integration with Data: Link your metadata with the actual data set, ensuring they are easily associated with each other.</p> </li> <li> <p>Version Control: Apply version control to your metadata, just as you would with your data.</p> </li> <li> <p>Regular Updates: As your data evolves, make sure to update the metadata accordingly.</p> </li> <li> <p>Review and Refinement: Regularly review and refine your metadata, seeking input from team members or stakeholders.</p> </li> </ol>"},{"location":"tutorials/creating-managing-metadata-documentation/#example-creating-json-metadata","title":"Example: Creating JSON Metadata","text":"<pre><code>{\n  \"file_name\": \"example_dataset.csv\",\n  \"collection_date\": \"2024-02-15\",\n  \"data_source\": \"Online Survey\",\n  \"format\": \"CSV\",\n  \"columns\": [\n    {\"name\": \"ID\", \"type\": \"Integer\", \"description\": \"Respondent ID\"},\n    {\"name\": \"Age\", \"type\": \"Integer\", \"description\": \"Respondent Age\"},\n    ...\n  ],\n  \"modifications\": [\n    {\"step\": \"Anonymization\", \"description\": \"Removed personal identifiers\"}\n  ],\n  \"version\": \"1.0\",\n  \"notes\": \"Data collected for market research purposes\"\n}\n</code></pre>"},{"location":"tutorials/creating-managing-metadata-documentation/#managing-data-documentation-a-comprehensive-guide","title":"Managing Data Documentation: A Comprehensive Guide","text":""},{"location":"tutorials/creating-managing-metadata-documentation/#creating-and-maintaining-data-documentation","title":"Creating and Maintaining Data Documentation","text":"<ol> <li> <p>Understanding the Importance: Recognize the role of thorough data documentation in data projects.</p> </li> <li> <p>Documenting Data Attributes: Detail every attribute of your data set, including its source, format, and any preprocessing done.</p> </li> <li> <p>Writing Guidelines: Develop a standardized approach or template for documenting data, ensuring consistency.</p> </li> <li> <p>Data Dictionary: Create a comprehensive data dictionary, outlining each column, its type, and purpose.</p> </li> <li> <p>Recording Preprocessing Steps: Document every step of data preprocessing, explaining the rationale and methods used.</p> </li> <li> <p>Versioning Documentation: Keep track of different versions of your data documentation, just like your data.</p> </li> <li> <p>Accessibility and Sharing: Ensure your data documentation is easily accessible to all team members and stakeholders.</p> </li> <li> <p>Regular Updates: Update the documentation as your data or methodologies evolve.</p> </li> <li> <p>Peer Reviews: Regularly review your data documentation with peers for accuracy and comprehensiveness.</p> </li> <li> <p>Best Practices: Stay updated with best practices in data documentation and incorporate them into your processes.</p> </li> </ol>"},{"location":"tutorials/data-version-control/","title":"Data Version control","text":""},{"location":"tutorials/data-version-control/#using-dvc-for-data-version-control","title":"Using DVC for Data Version Control","text":""},{"location":"tutorials/data-version-control/#overview","title":"Overview","text":"<p>In the realm of data science and machine learning, managing and versioning large datasets effectively is crucial for maintaining the integrity and reproducibility of projects. Data Version Control (DVC) is an essential tool that provides robust mechanisms for data versioning. By integrating DVC into our workflow, we gain a high level of control over our data assets, ensuring that our datasets are managed in a consistent, traceable, and efficient manner.</p> <p>DVC is designed to complement the traditional code version control systems, allowing us to handle large data files and datasets with ease. It enables our team to track changes in data, manage different versions of datasets, and link specific data versions to our experiments and model versions. This capability is vital for ensuring that our projects are reproducible and that we can easily revert or branch data as needed, just like we do with our code.</p>"},{"location":"tutorials/data-version-control/#why-dvc","title":"Why DVC?","text":"<p>DVC stands out as a tool specifically tailored for data management in several key ways:</p> <ul> <li>Versioning Large Datasets: It provides a Git-like interface for   data files, enabling us to track changes and maintain a history of   modifications.</li> <li>Reproducibility: By binding specific versions of data to our   project stages, DVC ensures that we can always replicate our results   with the exact data used in any given experiment.</li> <li>Collaboration: With DVC, sharing data changes becomes more   manageable, allowing for synchronized and consistent data usage across   the team.</li> </ul> <p>Integrating DVC into our data management strategy solidifies our commitment to maintaining best practices in data handling, ensuring that our projects are not only innovative but also methodically sound and reproducible.</p>"},{"location":"tutorials/data-version-control/#setting-up-dvc-for-data-version-control","title":"Setting Up DVC for Data Version Control","text":"<p>Integrating Data Version Control (DVC) into our workflow is a straightforward process. DVC is versatile and supports various storage options including cloud services like Azure and AWS, as well as local storage. Here\u2019s how you can set it up:</p>"},{"location":"tutorials/data-version-control/#step-1-initialize-dvc-in-your-project","title":"Step 1: Initialize DVC in Your Project","text":"<p>Navigate to your project repository and initialize DVC:</p> <pre><code>dvc init\n</code></pre> <p>This command creates a <code>.dvc</code> directory, signifying that DVC is now tracking the data in this repository.</p>"},{"location":"tutorials/data-version-control/#step-2-set-up-remote-storage","title":"Step 2: Set Up Remote Storage","text":"<p>DVC supports various storage backends. Depending on where you want to host your data (Azure, AWS, or locally), the setup will differ slightly.</p> <ul> <li> <p>For Azure Blob Storage:   <pre><code>dvc remote add -d myremote azure://mycontainer/path\ndvc remote modify myremote connection_string 'myconnectionstring'\n</code></pre></p> </li> <li> <p>For AWS S3:   <pre><code>dvc remote add -d myremote s3://mybucket/path\n# Configure AWS credentials via environment variables, AWS credentials file, or IAM roles.\n</code></pre></p> </li> <li> <p>For Local Storage:   <pre><code>dvc remote add -d myremote /path/to/local/storage\n</code></pre></p> </li> </ul> <p>In these commands, <code>myremote</code> is the name of the remote storage, which you can change to something more descriptive of your specific setup.</p>"},{"location":"tutorials/data-version-control/#step-3-add-data-to-dvc","title":"Step 3: Add Data to DVC","text":"<p>Add your data files or directories to DVC. This tells DVC to track the specified files or directories:</p> <pre><code>dvc add data/dataset.csv\n</code></pre>"},{"location":"tutorials/data-version-control/#step-4-commit-changes-to-version-control","title":"Step 4: Commit Changes to Version Control","text":"<p>After adding files or directories to DVC, commit the changes to both DVC and Git. This synchronizes the data tracking with your version control system:</p> <pre><code>git add data/dataset.csv.dvc data/.gitignore\ngit commit -m \"Add dataset with DVC\"\n</code></pre>"},{"location":"tutorials/data-version-control/#step-5-push-data-to-remote-storage","title":"Step 5: Push Data to Remote Storage","text":"<p>Finally, push your data to the configured remote storage. This ensures that your data is backed up and can be shared with others:</p> <pre><code>dvc push\n</code></pre>"},{"location":"tutorials/data-version-control/#local-data-updates-with-dvc-an-example","title":"Local Data Updates with DVC: An Example","text":""},{"location":"tutorials/data-version-control/#scenario","title":"Scenario","text":"<p>Imagine you have a dataset file named <code>dataset.csv</code> that you're already tracking with DVC. You've made some updates to this file - perhaps you've added more data, removed some entries, or modified existing data.</p>"},{"location":"tutorials/data-version-control/#how-dvc-detects-changes","title":"How DVC Detects Changes","text":"<ol> <li>DVC's Checksum Mechanism:</li> <li>DVC uses a checksum (a type of hash value) to track the version of      each file. When you first run <code>dvc add dataset.csv</code>, DVC calculates      and stores this file's checksum in the corresponding <code>.dvc</code> file.</li> <li> <p>When you modify <code>dataset.csv</code>, its checksum changes. DVC detects      that the current checksum of the file doesn't match the one stored      in the <code>.dvc</code> file, indicating that the file has been altered.</p> </li> <li> <p>Communicating Changes to the User:</p> </li> <li>To see which files have changed, you can use the command <code>dvc      status</code>. This command compares the current checksums of files with      those stored in <code>.dvc</code> files.</li> <li>If there are changes, DVC will list the files that have been      modified. For example, it might show <code>dataset.csv</code> as 'modified.'</li> </ol>"},{"location":"tutorials/data-version-control/#example-commands-to-track-new-data-version","title":"Example Commands to Track New Data Version","text":"<ol> <li>Add the Updated File to DVC:</li> <li>Run <code>dvc add dataset.csv</code> again. This updates the <code>.dvc</code> file with      the new checksum for the modified <code>dataset.csv</code>.</li> <li> <p>This command is crucial as it tells DVC to track this new version      of the file.</p> </li> <li> <p>Commit the Changes to Version Control:</p> </li> <li> <p>After updating the <code>.dvc</code> file, commit these changes to your Git      repository:      <pre><code>git add dataset.csv.dvc\ngit commit -m \"Update dataset.csv with new data\"\n</code></pre></p> </li> <li> <p>Push the Changes:</p> </li> <li>Use <code>dvc push</code> to upload the new version of <code>dataset.csv</code> to your      remote DVC storage. This ensures that the updated data is backed up      and available for others.</li> </ol>"},{"location":"tutorials/data-version-control/#the-purpose-of-this-process","title":"The Purpose of This Process","text":"<ul> <li>Reproducibility: By tracking every change to your data, you ensure   that every stage of your project can be reproduced, using the exact   version of the data that was used originally.</li> <li>Collaboration: This process makes sure that all team members are   aware of the data changes and can access the exact same data versions,   leading to consistent results across different environments.</li> <li>Traceability: Keeping track of data changes allows you to trace   back to any point in the project\u2019s history, understand what data was   used, and why certain decisions were made.</li> </ul>"},{"location":"tutorials/data-version-control/#conclusion","title":"Conclusion","text":"<p>In summary, DVC\u2019s mechanism of detecting changes through checksums, combined with its integration with Git, allows you to effectively version your data. This ensures that your datasets are consistently managed and that changes are transparent and traceable across your team. The process of updating data, running <code>dvc add</code>, committing changes, and pushing to remote storage is essential for maintaining the integrity and reproducibility of your data science projects.</p> <p>The process for handling data updates with DVC is similar for both local and cloud-based data, but there are some key differences to consider, particularly in how changes are detected and synchronized.</p>"},{"location":"tutorials/data-version-control/#cloud-data-updates-with-dvc-the-process","title":"Cloud Data Updates with DVC: The Process","text":""},{"location":"tutorials/data-version-control/#scenario_1","title":"Scenario","text":"<p>Suppose your data is hosted on a cloud storage service (like AWS S3, Azure Blob Storage, etc.), and you've updated the dataset directly in the cloud, bypassing your local machine.</p>"},{"location":"tutorials/data-version-control/#detecting-changes-in-cloud-data","title":"Detecting Changes in Cloud Data","text":"<ol> <li>Local vs. Cloud State:</li> <li>DVC does not automatically detect changes made directly in the      cloud storage. Instead, it relies on the local <code>.dvc</code> files to      track the data state.</li> <li> <p>To understand what's changed in the cloud, you need to manually      synchronize your local DVC environment with the cloud.</p> </li> <li> <p>Synchronizing Local and Cloud:</p> </li> <li>Run <code>dvc pull</code>. This command checks the remote storage for any      changes or new versions of the files and downloads them to your      local machine.</li> <li>When you execute <code>dvc pull</code>, DVC compares the checksums from the      cloud with those stored locally in your <code>.dvc</code> files. If there are      differences, DVC retrieves the updated files.</li> </ol>"},{"location":"tutorials/data-version-control/#updating-cloud-data-in-dvc","title":"Updating Cloud Data in DVC","text":"<ol> <li>Pull the Updated Data:</li> <li> <p>After detecting changes in the cloud, use <code>dvc pull</code> to update your      local copy with the latest version from the cloud.</p> </li> <li> <p>Track and Commit the Changes Locally:</p> </li> <li> <p>If you want to version these updates (which is a best practice),      you should <code>dvc add</code> the updated files and commit the changes to      your Git repository, just like with local updates:      <pre><code>dvc add dataset.csv\ngit add dataset.csv.dvc\ngit commit -m \"Update dataset.csv with cloud changes\"\n</code></pre></p> </li> <li> <p>Push the Changes to Remote Storage:</p> </li> <li>In case there are any further local changes or if you need to      synchronize the <code>.dvc</code> files, use <code>dvc push</code>.</li> </ol>"},{"location":"tutorials/data-version-control/#the-purpose-of-this-process-for-cloud-data","title":"The Purpose of This Process for Cloud Data","text":"<ul> <li>Consistency and Reproducibility: Even when data is stored and   updated in the cloud, it\u2019s crucial to maintain consistency and   reproducibility by tracking these changes through DVC.</li> <li>Collaborative Work: This process ensures that all team members are   working with the latest version of the data, regardless of where it is   stored or updated.</li> <li>Version Control Integration: By integrating DVC with cloud storage   and your local Git repository, you maintain a robust version control   system for your data.</li> </ul>"},{"location":"tutorials/data-version-control/#key-differences-from-local-data","title":"Key Differences from Local Data","text":"<ul> <li>Manual Synchronization Required: Unlike local data changes, which   you can detect by running <code>dvc status</code>, changes in the cloud require a   manual <code>dvc pull</code> to synchronize.</li> <li>Local Tracking of Cloud Changes: It's important to track cloud   changes locally using DVC and Git to maintain version control and   project history.</li> </ul>"},{"location":"tutorials/data-version-control/#conclusion_1","title":"Conclusion","text":"<p>While the core principles of data versioning with DVC remain the same, cloud data updates involve an additional step of manually synchronizing your local environment with the cloud. This ensures that your data management practices are consistent, transparent, and aligned with the collaborative needs of your project.</p>"},{"location":"tutorials/data-version-control/#communication-of-data-version-in-a-collaborative-setting","title":"Communication of Data Version in a Collaborative Setting","text":"<p>When working collaboratively on a project using DVC and GitHub, communication and transparency about data and model versions are key. Here's how your colleagues can be made aware of the specific data version used in a particular branch of the project for training a machine learning model:</p> <ol> <li>Commit Messages and Pull Requests:</li> <li>When you commit changes (including DVC-tracked files) to a branch      and push to GitHub, write clear and descriptive commit messages.      For example: \"Update dataset.csv to version 2.1 for model      training\".</li> <li> <p>If you're using Pull Requests (PRs), ensure the description clearly      states the changes in the data and the rationale behind these      changes. This helps team members understand the context of the      update.</p> </li> <li> <p>DVC Files in Git:</p> </li> <li>DVC tracks data changes using <code>.dvc</code> files which are committed to      Git. These files contain metadata about the data file versions.</li> <li> <p>When you push these changes to GitHub, your colleagues can see the      updated <code>.dvc</code> files in the commits or PRs. By examining these      files, they can identify which version of the data you used.</p> </li> <li> <p>DVC Pull to Sync Data:</p> </li> <li>After pulling the latest changes from GitHub, your colleagues      should run <code>dvc pull</code> to synchronize their local data with the      version tracked in the branch.</li> <li> <p>This ensures that they are working with the exact same data version      that you used for training your model.</p> </li> <li> <p>Automated Notifications:</p> </li> <li>Utilize features in GitHub like notifications, watching a      repository, or setting up automated alerts to inform team members      of new commits or PRs.</li> <li> <p>Team members who are closely following the project can stay updated      about changes in real-time.</p> </li> <li> <p>Project Documentation:</p> </li> <li> <p>Maintain a section in your project's documentation or README file      about the data versions being used. Update this section whenever a      significant change is made to the dataset.</p> </li> <li> <p>Regular Team Meetings or Updates:</p> </li> <li>In regular team meetings or through periodic updates (like emails      or team chat channels), summarize key changes in the project,      including updates to data and models.</li> </ol>"},{"location":"tutorials/data-version-control/#using-branches-to-manage-data-versions","title":"Using Branches to Manage Data Versions","text":"<ul> <li>Branch-Specific Data Versions: In Git, different branches can be   used to experiment with different data versions or model   configurations. This allows you to isolate changes in a controlled   environment.</li> <li>Communicate Branch Purpose: Clearly communicate the purpose of   each branch in your project documentation or in the branch name   itself, e.g., <code>feature/new-data-set-2.1</code>.</li> </ul>"},{"location":"tutorials/data-version-control/#collaborative-tools-and-integrations","title":"Collaborative Tools and Integrations","text":"<ul> <li>Integration with Slack or Other Communication Tools: Consider   integrating GitHub with tools like Slack to automatically post updates   about commits, PRs, and branch changes.</li> <li>Code Review Practices: Encourage thorough code reviews, where   reviewers not only look at code changes but also at updates in data   (as indicated by changes in <code>.dvc</code> files).</li> </ul>"},{"location":"tutorials/data-version-control/#workflow-for-collaborative-data-updates-using-dvc-and-github","title":"Workflow for Collaborative Data Updates Using DVC and GitHub","text":"<ol> <li>Local Data Update:</li> <li> <p>You update your dataset locally. This could be any modification      like adding new data, editing existing data, etc.</p> </li> <li> <p>Tracking Changes with DVC:</p> </li> <li> <p>Use DVC to track these changes. Run <code>dvc add &lt;file_or_directory&gt;</code>      to update the DVC tracking for the changed data. This updates the      <code>.dvc</code> file corresponding to the data, which now reflects the new      state of your dataset.</p> </li> <li> <p>Committing and Pushing to GitHub:</p> </li> <li>Commit the changes to your Git repository, including the updated      <code>.dvc</code> file and any other modified files. Your commit message      should clearly describe the changes made.</li> <li> <p>Push these commits to a branch in your GitHub repository.</p> </li> <li> <p>Team Member's Syncing Process:</p> </li> <li>Your colleagues, upon learning about the update (via pull request,      notification, or any other communication channel), will first pull      the latest changes from the GitHub repository. This includes the      updated <code>.dvc</code> files but not the actual data.</li> <li>They then run <code>dvc pull</code> in their local environment. This command      fetches the updated data files from the DVC remote storage and      replaces the older version of the data in their local workspace      with the version you just pushed.</li> </ol>"},{"location":"tutorials/data-version-control/#what-happens-during-the-sync","title":"What Happens During the Sync?","text":"<ul> <li>DVC Remote Storage: The updated data resides in the DVC remote   storage (which could be on cloud services like AWS S3, Azure Blob   Storage, or a local server). When you push changes with DVC, the   updated data is transferred to this remote storage.</li> <li>Rewriting Local Data: When your colleagues run <code>dvc pull</code>, DVC   fetches the specific version of the data associated with the latest   commit from the remote storage. Their local data files are then   updated (rewritten) to match this version.</li> </ul>"},{"location":"tutorials/data-version-control/#ensuring-consistency","title":"Ensuring Consistency","text":"<ul> <li>Version Control of Data: The <code>.dvc</code> files in your Git repository   ensure that everyone is aware of which version of the data should be   used with the current codebase.</li> <li>No Direct Data Overwrite in Git: It's important to note that the   data files themselves are not stored or directly overwritten in the   Git repository. The synchronization of the actual data files happens   through DVC commands (<code>dvc push</code> and <code>dvc pull</code>), while Git handles   the version control through <code>.dvc</code> files.</li> </ul>"},{"location":"tutorials/data-version-control/#conclusion_2","title":"Conclusion","text":"<p>In summary, by using DVC in conjunction with Git, you can update datasets on your local machine and track these changes. When you push these changes to GitHub, your colleagues can then update their local copies to match your data version, ensuring everyone in the team is working with the same dataset. This process is crucial for maintaining consistency, reproducibility, and effective collaboration in data-driven projects.</p> <pre><code>graph TB\n    A[Local Data Update] --&gt;|DVC add| B[Track Changes with DVC]\n    B --&gt; |Git commit &amp; push| C[Push to GitHub]\n    C --&gt;|Colleague Git pull| D[Team Member Git Sync]\n    D --&gt;|DVC pull| E[Team Member DVC Sync]\n    E --&gt; F[Local Data Updated to New Version]\n\n    style A fill:#f9f,stroke:#333,stroke-width:2px,color:black\n    style B fill:#bbf,stroke:#333,stroke-width:2px,color:black\n    style C fill:#f96,stroke:#333,stroke-width:2px,color:black\n    style D fill:#9f6,stroke:#333,stroke-width:2px,color:black\n    style E fill:#6f9,stroke:#333,stroke-width:2px,color:black\n    style F fill:#9ff,stroke:#333,stroke-width:2px,color:black</code></pre>"},{"location":"tutorials/data-version-control/#how-to-read-the-diagram","title":"How to Read the Diagram","text":"<ul> <li>Local Data Update: This is where you make changes to your dataset   locally.</li> <li>Track Changes with DVC: After updating the data, you use <code>dvc add</code>   to track these changes.</li> <li>Push to GitHub: Commit the updated <code>.dvc</code> file and push these   changes to GitHub.</li> <li>Team Member Git Sync: Your colleagues pull the latest changes from   the GitHub repository.</li> <li>Team Member DVC Sync: They run <code>dvc pull</code> to synchronize their   local data with the updated version stored in DVC remote storage.</li> <li>Local Data Updated to New Version: At this point, your colleagues   have the same data version as you do.</li> </ul>"},{"location":"tutorials/data-version-control/#utilizing-the-dvc-extension-in-vs-code-for-data-management","title":"Utilizing the DVC Extension in VS Code for Data Management","text":"<p>Once the DVC extension is installed in VS Code, managing your data becomes more streamlined and integrated within your development environment. Here\u2019s how you can effectively use the extension for data management:</p> <ol> <li> <p>View DVC Tracked Files: Access the DVC extension's sidebar in VS    Code to see a list of all DVC-tracked files and stages in your    project. This offers a clear overview of your data files and their    status.</p> </li> <li> <p>Execute DVC Commands: Directly interact with your data through    the VS Code interface. Right-click on the DVC-tracked files or stages    to run common DVC commands such as <code>dvc pull</code>, <code>dvc push</code>, or <code>dvc    repro</code>. This feature simplifies executing DVC commands and reduces    the need to switch between the terminal and the code editor.</p> </li> <li> <p>Monitor Pipeline Status: The extension provides a visualization    of your DVC pipelines. You can track the progress and status of    different stages in your data pipelines, making it easier to identify    and resolve issues.</p> </li> <li> <p>Manage Experiments: If you are using DVC for machine learning    experiments, the extension allows you to browse, compare, and manage    these experiments directly within VS Code.</p> </li> </ol> <p>By incorporating these features into your workflow, the DVC extension for VS Code enhances your efficiency in managing data versions and pipelines, contributing to a more productive and organized project environment.</p>"},{"location":"tutorials/data/","title":"How to organize project's data","text":"<p>Warning</p> <p>It is not a best practice to save sensitive or large project data on GitHub. GitHub is designed for source code management and version control, and while it does allow for limited file storage, it's not designed for storing large binary files or sensitive information.</p>"},{"location":"tutorials/data/#data-files","title":"Data files","text":"<p>Best Practice</p> <p>Store project's data on the remote Data Hub Linux server.</p> <p>Creating separate folders for raw, external, interim, and processed data is the best practice for organizing and managing data in a project. This approach helps to maintain a clear and organized structure for the data, and can also help to ensure that the different stages of data processing are clearly defined and documented.</p>"},{"location":"tutorials/data/#folder-structure-and-naming-convention","title":"Folder Structure and Naming Convention","text":"<p>The data folder structure should be organized into distinct categories, each serving a specific purpose in the data processing pipeline. Here\u2019s the recommended structure:</p> <pre><code>data\n\u251c\u2500\u2500 raw                   # Original, immutable data dump\n\u251c\u2500\u2500 external              # Data from third-party sources\n\u251c\u2500\u2500 interim               # Intermediate data, partially processed\n\u251c\u2500\u2500 processed             # Fully processed data, ready for analysis\n\u2514\u2500\u2500 features              # Engineered features ready for model training\n</code></pre>"},{"location":"tutorials/data/#explanation-of-categories","title":"Explanation of Categories","text":"<ul> <li>Raw: </li> <li> <p>Contains the original datasets. Data in this folder is immutable      and serves as a backup for all processing steps.</p> </li> <li> <p>External: </p> </li> <li> <p>For data sourced from outside the original dataset. This includes      third-party data, external APIs, or any supplementary data.</p> </li> <li> <p>Interim: </p> </li> <li> <p>Holds data that is in the process of being cleaned or transformed.      Useful for datasets that are not final but are needed in      intermediate stages.</p> </li> <li> <p>Processed: </p> </li> <li> <p>Contains the final version of the dataset, which is ready for      analysis or modeling. Data in this folder is cleaned, formatted,      and pre-processed.</p> </li> <li> <p>Features: </p> </li> <li>Dedicated to storing feature sets that are used in machine learning      models. This includes transformed data, new features, and selected      features.</li> </ul> <p>Having a clear and organized structure for the data can help to ensure that it is properly managed throughout the project and that any necessary modifications or transformations can be easily traced and recorded. Additionally, it can also make it easier for other team members or stakeholders to understand and access the data as needed.</p>"},{"location":"tutorials/data/#centralized-data-folders-whether-to-use-them-or-not","title":"Centralized data folders: whether to use them or not","text":"<p>The answer to this question depends on the specific requirements of the project and the organization's data management policies.</p> <p>In some cases, it may be appropriate for data scientists to keep a copy of the project's data on their local machines for ease of access and local processing. This can be especially useful when working with large data sets that may not be feasible to store and transfer over the network.</p> <p>However, in most cases, it is a best practice to use a centralized shared data folder that is stored on a networked storage system or a cloud-based data storage service. This allows all team members to access the data, and ensures that the data is stored in a secure and organized manner. Additionally, using a centralized data folder can help to avoid duplication and ensure that everyone is working with the same version of the data.</p> <p>Regardless of the approach used, it's important to establish clear guidelines and protocols for data management and access, and to ensure that data security and privacy concerns are properly addressed. This may involve implementing access controls, encryption, and backup and recovery measures.</p>"},{"location":"tutorials/data/#using-vs-code-on-a-project-with-a-centralized-shared-data-folder","title":"Using VS Code on a project with a centralized shared data folder","text":"<p>Step 4 of the Getting Started guide involves creating a shared data directory for the project. To make the data accessible and visible in VS Code, symbolic links need to be created.</p> <p>What is a symbolic link?</p> <p>The purpose of a symbolic link (symlink) in Linux is to create a shortcut or alias to a file or directory. A symlink acts as a reference to the original file, allowing multiple paths to access the same data. This can be useful for creating shortcuts to frequently used files, linking to files in different directories, and creating backups of important files.</p> <p>Here's a shell script that creates symbolic links for the specified folders:</p> symlink_data_folders.sh<pre><code>folders=(raw external interim processed model_output)\n\nfor folder in \"${folders[@]}\"; do\n  ln -s /path/to/original/\"$folder\" /path/to/link/\"$folder\"\ndone\n</code></pre> <p>Where the path to the original folder corresponds to the project's shared data folder:</p> <pre><code>/nyl/data/tenants/insurance/cdsamktg/mediamix/data\n</code></pre> <p>And the link folder corresponds to the location of the project's data folder on the cloned Git repository on server user space:</p> <pre><code>/nyl/data/home/t93kqi0/projects/Media-Mix/data/\"$folder\"\n</code></pre> symlink_data_folders.sh<pre><code>folders=(raw external interim processed model_output)\n\nfor folder in \"${folders[@]}\"; do\n  ln -s /nyl/data/tenants/insurance/cdsamktg/mediamix/data/\"$folder\" \\\n  /nyl/data/home/t93kqi0/projects/Media-Mix/data/\"$folder\"\ndone\n</code></pre> <p>Warning</p> <p>The link <code>data</code> folder must be empty before running the <code>symlink_data_folders.sh</code> script.</p> <p>Now all shared data in each sub-directory of the <code>data</code> shared folder must be visible from the VS Code Explorer pane.</p>"},{"location":"tutorials/data/#metadata-and-documentation","title":"Metadata and Documentation","text":"<p>Discussing the importance of Metadata and Documentation in the context of your data science project is crucial. Here's a detailed breakdown:</p>"},{"location":"tutorials/data/#importance-of-metadata-and-documentation","title":"Importance of Metadata and Documentation","text":"<ol> <li> <p>Contextual Understanding: Metadata provides context to the data.    It includes details like the source of the data, when and how it was    collected, its format, and any changes it has undergone. This    information is essential for anyone trying to understand or use the    data effectively.</p> </li> <li> <p>Reproducibility and Traceability: Proper documentation ensures    that data processing steps can be replicated and understood by    others. This is particularly important in data science, where    reproducibility is a key aspect of project credibility and    validation.</p> </li> <li> <p>Data Quality Insights: Metadata can include information about    data quality, such as accuracy, completeness, and consistency. This    is valuable for assessing the reliability of the data and    understanding its limitations.</p> </li> <li> <p>Compliance and Auditing: In many industries, there are regulatory    requirements for data management. Detailed metadata and documentation    help in complying with these regulations and make audits more    manageable.</p> </li> </ol>"},{"location":"tutorials/data/#recommendations-for-effective-metadata-and-documentation","title":"Recommendations for Effective Metadata and Documentation","text":"<ol> <li> <p>Standardized Format: Adopt a standardized format or template for    metadata and documentation. This could be a structured file like JSON    or XML for metadata, and markdown or structured text files for    documentation.</p> </li> <li> <p>Automated Generation: Where possible, automate the generation of    metadata. For instance, when data is imported or processed, scripts    can automatically capture and record key information.</p> </li> <li> <p>Versioning of Data and Metadata: Just like the data, its metadata    should also be version-controlled. This is important as the data    evolves over time due to various processing steps.</p> </li> <li> <p>Inclusion of Key Elements: Ensure that metadata includes    essential elements like data source, date of acquisition, data    format, any preprocessing steps applied, data schema (if applicable),    and information on confidentiality or sensitivity.</p> </li> <li> <p>Accessibility: Store metadata and documentation in an easily    accessible location. It should be clearly linked to the corresponding    data.</p> </li> <li> <p>Training and Guidelines: Provide team members with training or    guidelines on how to create and maintain proper documentation and    metadata. This ensures consistency and compliance with established    standards.</p> </li> <li> <p>Regular Updates: Just as data changes, so should its    documentation. It's important to update documentation whenever the    data or its handling procedures change.</p> </li> <li> <p>Use of Tools: Leverage tools that support metadata management and    documentation. For instance, data cataloging tools can be very    helpful in larger organizations for maintaining a central repository    of metadata.</p> </li> <li> <p>Collaboration and Reviews: Encourage regular reviews of metadata    and documentation by different team members. This not only improves    the quality but also ensures that the data remains understandable and    usable by everyone.</p> </li> <li> <p>Integration with Data Pipeline: Integrate metadata generation     and documentation updates into your data processing pipelines. This     ensures that changes in data are automatically reflected in the     metadata and documentation.</p> </li> </ol> <p>In summary, comprehensive and up-to-date metadata and documentation are fundamental to the effective management, use, and understanding of data in any data science project. They facilitate better collaboration, ensure compliance with standards and regulations, and significantly contribute to the overall integrity and usability of the data.</p>"},{"location":"tutorials/data/#example-of-metadata-creation","title":"Example of Metadata Creation","text":"<p>Using a JSON file to store metadata for a CSV file is an efficient way to keep track of important information about your data. Below is an example of how this can be done, along with a method to automate the process.</p>"},{"location":"tutorials/data/#example-json-metadata-for-a-csv-file","title":"Example: JSON Metadata for a CSV File","text":"<p>Suppose you have a CSV file named <code>sales_data.csv</code>. The metadata for this file could include information such as the source of the data, the date of creation, the number of rows and columns, column names, and any preprocessing steps applied.</p> <p>Here's an example of what the JSON metadata might look like:</p> <pre><code>{\n  \"file_name\": \"sales_data.csv\",\n  \"creation_date\": \"2024-01-22\",\n  \"source\": \"Internal Sales System\",\n  \"number_of_rows\": 1200,\n  \"number_of_columns\": 5,\n  \"columns\": [\n    {\"name\": \"Date\", \"type\": \"Date\", \"description\": \"Date of sale\"},\n    {\"name\": \"Product_ID\", \"type\": \"String\", \"description\": \"Unique identifier for the product\"},\n    {\"name\": \"Quantity\", \"type\": \"Integer\", \"description\": \"Number of products sold\"},\n    {\"name\": \"Price\", \"type\": \"Float\", \"description\": \"Sale price per unit\"},\n    {\"name\": \"Total_Sales\", \"type\": \"Float\", \"description\": \"Total sales amount\"}\n  ],\n  \"preprocessing\": [\n    {\"step\": \"Data Cleaning\", \"description\": \"Removed null values and corrected data formats\"},\n    {\"step\": \"Normalization\", \"description\": \"Normalized the Price column using min-max scaling\"}\n  ],\n  \"notes\": \"Data updated monthly. Last update included Q4 2023 sales data.\"\n}\n</code></pre>"},{"location":"tutorials/data/#automating-the-metadata-generation","title":"Automating the Metadata Generation","text":"<p>To automate the process of generating this metadata, you can use a script in Python. This script will:</p> <ol> <li>Read the CSV file.</li> <li>Extract relevant information such as the number of rows and columns,    column names, etc.</li> <li>Generate and save the metadata in a JSON file.</li> </ol> <p>Here's a simple Python script to achieve this:</p> <pre><code>import json\nimport pandas as pd\nfrom datetime import datetime\n\ndef generate_metadata(csv_file_path):\n    # Read the CSV file\n    df = pd.read_csv(csv_file_path)\n\n    # Extracting information\n    file_name = csv_file_path.split('/')[-1]\n    creation_date = datetime.now().strftime(\"%Y-%m-%d\")\n    number_of_rows = df.shape[0]\n    number_of_columns = df.shape[1]\n    columns = [{\"name\": col, \"type\": str(df[col].dtype)} for col in df.columns]\n\n    # Metadata dictionary\n    metadata = {\n        \"file_name\": file_name,\n        \"creation_date\": creation_date,\n        \"source\": \"Specify the data source\",\n        \"number_of_rows\": number_of_rows,\n        \"number_of_columns\": number_of_columns,\n        \"columns\": columns,\n        \"preprocessing\": [],  # Add any preprocessing steps manually or through code\n        \"notes\": \"Add any additional notes here\"\n    }\n\n    # Saving metadata to a JSON file\n    with open(file_name.replace('.csv', '_metadata.json'), 'w') as json_file:\n        json.dump(metadata, json_file, indent=4)\n\n# Example usage\ngenerate_metadata('path/to/your/sales_data.csv')\n</code></pre>"},{"location":"tutorials/data/#notes","title":"Notes:","text":"<ul> <li>The <code>preprocessing</code> section is left empty in the script. This is   because preprocessing steps can vary widely and might need to be added   manually or captured through additional scripting logic based on your   specific data pipeline.</li> <li>The script assumes a basic understanding of the data's source and   nature. Adjustments may be required based on the specific context of   your data.</li> <li>The script uses the <code>pandas</code> library for data handling. Ensure this   library is installed in your Python environment (<code>pip install   pandas</code>).</li> </ul> <p>This automated approach can save significant time and reduce errors in metadata creation, especially for large datasets or frequent data updates.</p>"},{"location":"tutorials/data/#data-documentation-example","title":"Data Documentation Example","text":""},{"location":"tutorials/data/#overview","title":"Overview","text":"<p>This document describes the <code>sales_data.csv</code> dataset, which contains records of sales transactions for a retail company. The dataset is updated monthly and includes detailed information about each sale, including the date, product details, and sale amounts.</p>"},{"location":"tutorials/data/#file-details","title":"File Details","text":"<p>The file is named <code>sales_data_documentation.md</code> which clearly indicates its purpose and content.</p> <ul> <li>File Name: <code>sales_data.csv</code></li> <li>Creation Date: 2024-01-22</li> <li>Last Updated: 2024-01-22</li> <li>Total Records: 1200</li> <li>Source: Internal Sales System</li> </ul>"},{"location":"tutorials/data/#data-dictionary","title":"Data Dictionary","text":"Column Name Data Type Description Date Date Date of sale (format: YYYY-MM-DD) Product_ID String Unique identifier for the product Quantity Integer Number of products sold Price Float Sale price per unit (in USD) Total_Sales Float Total sales amount (in USD)"},{"location":"tutorials/data/#preprocessing-steps","title":"Preprocessing Steps","text":"<ol> <li>Data Cleaning</li> <li>Null values in the <code>Price</code> and <code>Quantity</code> columns were removed.</li> <li> <p>Date formats were standardized to <code>YYYY-MM-DD</code>.</p> </li> <li> <p>Normalization</p> </li> <li>The <code>Price</code> column was normalized using min-max scaling.</li> </ol>"},{"location":"tutorials/data/#usage-notes","title":"Usage Notes","text":"<ul> <li>The dataset is intended for internal use only.</li> <li>Data is confidential and should not be shared outside the organization   without proper authorization.</li> <li>For any discrepancies or data requests, contact the Data Management   Team.</li> </ul>"},{"location":"tutorials/data/#versioning","title":"Versioning","text":"<ul> <li>Current Version: 1.2</li> <li>Previous Versions:</li> <li>1.1 - Included Q3 2023 sales data.</li> <li>1.0 - Initial dataset creation.</li> </ul>"},{"location":"tutorials/dvc-cloud/","title":"Tutorial: Managing Cloud Data Updates with DVC","text":""},{"location":"tutorials/dvc-cloud/#introduction","title":"Introduction","text":"<p>Learning Objectives</p> <p>Learn to manage and sync data updates in the cloud using Data Version Control (DVC). This tutorial is ideal for scenarios involving direct data updates in cloud storage services like AWS S3 or Azure Blob Storage.</p>"},{"location":"tutorials/dvc-cloud/#prerequisites","title":"Prerequisites","text":"<ul> <li> Familiarity with DVC and cloud storage.</li> <li> DVC setup for your cloud storage service.</li> </ul>"},{"location":"tutorials/dvc-cloud/#scenario-updates-in-cloud-storage","title":"Scenario: Updates in Cloud Storage","text":"<p>Consider a situation where your dataset in a cloud storage service is updated directly in the cloud, without involving your local machine.</p>"},{"location":"tutorials/dvc-cloud/#the-challenge-of-cloud-data-updates","title":"The Challenge of Cloud Data Updates","text":"<ol> <li>Local vs. Cloud State:</li> <li>DVC's limitation in automatically detecting cloud changes.</li> <li>The need for manual synchronization between local and cloud states.</li> </ol>"},{"location":"tutorials/dvc-cloud/#step-by-step-guide-to-syncing-cloud-data","title":"Step-by-Step Guide to Syncing Cloud Data","text":""},{"location":"tutorials/dvc-cloud/#1-synchronizing-local-and-cloud-states","title":"1. Synchronizing Local and Cloud States","text":"<p>Syncing with DVC Pull</p> <p>Run <code>dvc pull</code> to fetch and update your local workspace with the latest file versions from the cloud storage.</p> <pre><code>dvc pull\n</code></pre> <p>DVC will compare and download any updated files based on checksum differences.</p>"},{"location":"tutorials/dvc-cloud/#2-local-tracking-of-cloud-updates","title":"2. Local Tracking of Cloud Updates","text":"<ol> <li>Pull Updated Data:</li> <li> <p>Ensure your local workspace reflects the latest cloud version.</p> </li> <li> <p>Track and Commit Changes Locally:</p> </li> <li> <p>Version the cloud updates using DVC and Git.</p> <pre><code>dvc add dataset.csv\ngit add dataset.csv.dvc\ngit commit -m \"Update dataset.csv with cloud changes\"\n</code></pre> </li> <li> <p>Push Local Changes to Remote Storage:</p> </li> <li> <p>Sync any additional local changes or <code>.dvc</code> files to the cloud.</p> <pre><code>dvc push\n</code></pre> </li> </ol>"},{"location":"tutorials/dvc-cloud/#understanding-the-process","title":"Understanding the Process","text":"<p>Purpose of Cloud Data Syncing</p> <ul> <li>Consistency and Reproducibility: Tracks and versions changes even in cloud storage.</li> <li>Collaborative Work: Ensures all team members have the latest data version.</li> <li>Version Control Integration: Combines DVC with cloud storage for effective data management.</li> </ul>"},{"location":"tutorials/dvc-cloud/#comparing-with-local-data-handling","title":"Comparing with Local Data Handling","text":"<p>Key Differences from Local Data</p> <ul> <li>Manual Synchronization: Active steps are required to align cloud and local data states.</li> <li>Local Tracking of Cloud Changes: Crucial for comprehensive version control and historical tracking.</li> </ul>"},{"location":"tutorials/dvc-cloud/#conclusion","title":"Conclusion","text":"<p>Key Takeaways</p> <p>Cloud data updates with DVC introduce a necessary step of manual synchronization. This tutorial equips you with the knowledge to ensure your data management practices are consistent, transparent, and collaborative, irrespective of where the data is stored or updated.</p>"},{"location":"tutorials/dvc-collaboration/","title":"Tutorial: Workflow for Collaborative Data Updates Using DVC and GitHub","text":""},{"location":"tutorials/dvc-collaboration/#introduction","title":"Introduction","text":"<p>Learning Objectives</p> <p>Master the workflow for managing collaborative data updates in projects using Data Version Control (DVC) and GitHub. This tutorial is designed to provide a clear guide for teams to maintain consistency and synchronization in their data-driven projects.</p>"},{"location":"tutorials/dvc-collaboration/#prerequisites","title":"Prerequisites","text":"<ul> <li> Basic knowledge of DVC and GitHub.</li> <li> A project set up with DVC and connected to a GitHub repository.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#collaborative-update-scenario","title":"Collaborative Update Scenario","text":"<p>Imagine a collaborative environment where dataset updates need to be synchronized across a team's local environments.</p>"},{"location":"tutorials/dvc-collaboration/#step-by-step-workflow-guide","title":"Step-by-Step Workflow Guide","text":""},{"location":"tutorials/dvc-collaboration/#1-local-data-update","title":"1. Local Data Update","text":"<p>Updating Data Locally</p> <ul> <li>Action: Make changes to your dataset locally, such as adding or editing data.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#2-tracking-changes-with-dvc","title":"2. Tracking Changes with DVC","text":"<p>Using DVC for Tracking</p> <ul> <li>Command: Run <code>dvc add &lt;file_or_directory&gt;</code> to track changes.   <pre><code>dvc add data/dataset.csv\n</code></pre></li> <li>Result: The <code>.dvc</code> file is updated to reflect the new dataset state.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#3-committing-and-pushing-to-github","title":"3. Committing and Pushing to GitHub","text":"<p>Syncing with GitHub</p> <ul> <li>Commit and Push: Update your Git repository with the new data version.   <pre><code>git add data/dataset.csv.dvc\ngit commit -m \"Update dataset.csv with new data\"\ngit push\n</code></pre></li> <li>Outcome: Changes are now in the GitHub repository.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#4-team-syncing-process","title":"4. Team Syncing Process","text":"<p>Team Members' Actions</p> <ul> <li>Git Pull: Team members pull the latest changes.   <pre><code>git pull\n</code></pre></li> <li>DVC Pull: Synchronize the local data with the updated version in DVC remote storage.   <pre><code>dvc pull\n</code></pre></li> <li>Consistency Achieved: Everyone works with the same data version.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#sync-mechanism-explained","title":"Sync Mechanism Explained","text":"<ul> <li>:octicons-cloud-upload-24: DVC Remote Storage: Stores the updated data, accessible to all team members.</li> <li> Local Data Sync: <code>dvc pull</code> ensures local data matches the remote version.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#ensuring-data-consistency","title":"Ensuring Data Consistency","text":"<ul> <li> Version Control with <code>.dvc</code> Files: Crucial for indicating the current data version.</li> <li> Data Synchronization: Managed through DVC commands, while Git handles <code>.dvc</code> file version control.</li> </ul>"},{"location":"tutorials/dvc-collaboration/#conclusion","title":"Conclusion","text":"<p>Key Takeaway</p> <p>Following this workflow allows teams to efficiently collaborate on data-driven projects, ensuring data consistency, reproducibility, and effective teamwork. It's an essential practice for maintaining project integrity and collaborative efficiency.</p>"},{"location":"tutorials/dvc-local/","title":"Tutorial: Managing Local Data Updates with DVC","text":""},{"location":"tutorials/dvc-local/#introduction","title":"Introduction","text":"<p>About this Tutorial</p> <p>In this tutorial, you'll learn the practical steps to manage and track changes to datasets locally using Data Version Control (DVC). We'll cover a real-world scenario of updating a dataset and tracking these changes with DVC.</p>"},{"location":"tutorials/dvc-local/#prerequisites","title":"Prerequisites","text":"<ul> <li> Basic understanding of version control with Git.</li> <li> DVC installed and initialized in your project.</li> </ul>"},{"location":"tutorials/dvc-local/#scenario-updating-your-dataset","title":"Scenario: Updating Your Dataset","text":"<p>Imagine you've made updates to a dataset file named <code>dataset.csv</code> that's already being tracked by DVC. Your task now is to manage these updates efficiently.</p>"},{"location":"tutorials/dvc-local/#step-by-step-guide-to-manage-local-data-updates","title":"Step-by-Step Guide to Manage Local Data Updates","text":""},{"location":"tutorials/dvc-local/#1-detecting-changes-with-dvc","title":"1. Detecting Changes with DVC","text":"<p>How DVC Detects Changes</p> <ul> <li>Checksum Mechanism: DVC uses checksums (hash values) to track file versions. A change in the file leads to a change in its checksum, signaling an update.</li> </ul>"},{"location":"tutorials/dvc-local/#2-updating-the-data-version","title":"2. Updating the Data Version","text":""},{"location":"tutorials/dvc-local/#tracking-the-updated-file","title":"Tracking the Updated File","text":"<ol> <li> <p>Run the DVC Add Command:    <pre><code>dvc add dataset.csv\n</code></pre>    This command updates the <code>.dvc</code> file with the new checksum for <code>dataset.csv</code>.</p> </li> <li> <p>Commit the Changes in Git:    <pre><code>git add dataset.csv.dvc\ngit commit -m \"Update dataset.csv with new data\"\n</code></pre></p> </li> <li> <p>Push the Updated Data:    <pre><code>dvc push\n</code></pre>    This step uploads the new version to your remote DVC storage.</p> </li> </ol>"},{"location":"tutorials/dvc-local/#understanding-the-purpose-of-this-process","title":"Understanding the Purpose of This Process","text":"<p>Key Benefits</p> <ul> <li>Reproducibility: Ensures stages of your project can be reproduced with the exact data versions.</li> <li>Collaboration: Maintains consistency in data across team environments.</li> <li>Traceability: Allows tracking of data changes throughout the project's history.</li> </ul>"},{"location":"tutorials/dvc-local/#conclusion","title":"Conclusion","text":"<p>Wrapping Up</p> <p>You've learned how DVC's integration with Git facilitates effective data versioning, ensuring consistent dataset management and traceable changes across your team. While focused on local data updates, remember that cloud-based updates might follow a slightly different process.</p>"},{"location":"tutorials/gitignore/","title":"Understanding the <code>.gitignore</code> file","text":"<p>Should you commit <code>.gitignore</code> to git repositories?</p> <p>Yes, <code>.gitignore</code> files should be commited.</p>"},{"location":"tutorials/gitignore/#introduction","title":"Introduction","text":"<p>When you make commits in a git repository, you choose which files to stage and commit by using <code>git add FILENAME</code> and then git commit. But what if there are some files that you never want to commit? It's too easy to accidentally commit them (especially if you use <code>git add .</code> to stage all files in the current directory). That's where a <code>.gitignore</code> file comes in handy. It lets Git know that it should ignore certain files and not track them.</p> <p>Info</p> <p>The purpose of gitignore files is to ensure that certain files not tracked by Git remain untracked.</p>"},{"location":"tutorials/gitignore/#how-gitignore-works","title":"How <code>.gitignore</code> works","text":"<p>Here's how it works. A <code>.gitignore</code> file is a plain text file where each line contains a pattern for files/directories to ignore. Generally, this is placed in the root folder of the repository.</p>"},{"location":"tutorials/gitignore/#literal-file-names","title":"Literal File Names","text":"<p>The easiest pattern is a literal file name, for example:</p> .gitignore<pre><code>.DS_Store\n</code></pre> <p>This will ignore any files named <code>.DS_Store</code>, which is a common file on macOS.</p>"},{"location":"tutorials/gitignore/#directories","title":"Directories","text":"<p>You can ignore entire directories, just by including their paths and putting a <code>/</code> on the end:</p> .gitignore<pre><code>node_modules/\nlogs/\n</code></pre> <p>If you leave the slash off of the end, it will match both files and directories with that name.</p>"},{"location":"tutorials/gitignore/#wildcard","title":"Wildcard","text":"<p>The <code>*</code> matches 0 or more characters (except the <code>/</code>). So, for example, <code>*.log</code> matches any file ending with the <code>.log</code> extension.</p> <p>Another example is <code>*~</code>, which matches any file ending with <code>~</code>, such as <code>index.html~</code></p> <p>You can also use the <code>?</code>, which matches any one character except for the <code>/</code>.</p>"},{"location":"tutorials/gitignore/#negation","title":"Negation","text":"<p>You can use a prefix of <code>!</code> to negate a file that would be ignored.</p> .gitignore<pre><code>*.log\n!example.log\n</code></pre> <p>In this example, <code>example.log</code> is not ignored, even though all other files ending with <code>.log</code> are ignored.</p> <p>But be aware, you can't negate a file inside of an ignored directory:</p> .gitignore<pre><code>logs/\n!logs/example.log\n</code></pre> <p>Due to performance reasons, git will still ignore <code>logs/example.log</code> here because the entire logs directory is ignored.</p>"},{"location":"tutorials/gitignore/#double-asterisk","title":"Double Asterisk","text":"<p><code>**</code>  can be used to match any number of directories.</p> <ul> <li> <p><code>**/logs</code>  matches all files or directories named logs (same as the   pattern  <code>logs</code>)</p> </li> <li> <p><code>**/logs/*.log</code>  matches all files ending with  <code>.log</code>  in a logs   directory</p> </li> <li> <p><code>logs/**/*.log</code>  matches all files ending with  <code>.log</code>  in the logs   directory and any of its subdirectories</p> </li> </ul> <p><code>**</code>  can also be used to match all files inside of a directory, so for example  <code>logs/**</code>  matches all files inside of logs.</p>"},{"location":"tutorials/gitignore/#comments","title":"Comments","text":"<p>Any lines that start with  <code>#</code>  are comments:</p> .gitignore<pre><code>1# macOS Files\n2.DS_Store\n</code></pre>"},{"location":"tutorials/gitignore/#automatically-create-gitignore-file-from-vs-code","title":"Automatically create <code>.gitignore</code> file from VS Code","text":"<p>The <code>gitignore</code> VS Code extension included in the list of recommended extensions for this repository automates the creation of <code>.gitignore</code> files.</p>"},{"location":"tutorials/gitignore/#usage","title":"Usage","text":"<p>Start command palette (with <code>Ctrl</code>+<code>Shift</code>t+<code>P</code> or <code>F1</code>) and start typing<code>Add gitignore</code>. Then select a programming language from the list.</p> <p>It is possible to append multiple programming languages like Python and R. Add first Python and then, if you try to add R you will see these options:</p> .gitignore<pre><code>Append Append to existing .gitignore file\nOverwrite Overwrite existing .gitignore file\n</code></pre> <p>Choose append and you will see the following message:</p> .gitignore<pre><code>Appended Python.gitignore to the existing .gitignore in the project root\n</code></pre> <p>References</p> <ul> <li>git   documentation</li> <li>How to Use a <code>.gitignore</code>   File</li> </ul>"},{"location":"tutorials/hydra-config/","title":"Manage Configuration Files with Hydra","text":""},{"location":"tutorials/hydra-config/#introduction-and-motivation","title":"Introduction and Motivation","text":"<p>Welcome to this tutorial on Hydra, a Python framework designed for elegantly configuring complex applications. If you've ever found yourself in a situation where you have a Python script filled with both code and configuration parameters, you know how messy it can get. Changing a single value often means digging into the source code, making it difficult to manage and maintain your application.</p> <p>Hydra offers a solution to this problem by separating the code and configuration parameters into different files. This separation makes it easier to manage complex applications, especially in data science projects where configurations can get complicated quickly. With Hydra, your Python scripts become cleaner, more modular, and easier to understand.</p> <p>In this guide, we'll walk you through how to use Hydra in both Python scripts and Jupyter notebooks to manage your application's configuration elegantly.</p>"},{"location":"tutorials/hydra-config/#using-hydra-in-a-python-script","title":"Using Hydra in a Python Script","text":"<p>To use Hydra in a Python script, you need to import the <code>hydra</code> package and use the <code>@hydra.main()</code> decorator. This decorator will automatically load and validate the configuration file specified.</p>"},{"location":"tutorials/hydra-config/#example-configuration-file","title":"Example Configuration File","text":"<p>Create a YAML configuration file named <code>main.yaml</code> inside a folder named <code>config</code>:</p> <pre><code># config/main.yaml\n\ndb:\n  driver: mysql\n  user: omry\n  password: secret\n</code></pre>"},{"location":"tutorials/hydra-config/#example-python-script","title":"Example Python Script","text":"<p>Here's an example Python script that uses Hydra to read the configuration file and perform some actions based on it:</p> <pre><code>from omegaconf import DictConfig, OmegaConf\nimport hydra\n\ndef display_config(cfg: DictConfig) -&gt; None:\n    \"\"\"Display the entire configuration.\"\"\"\n    print(OmegaConf.to_yaml(cfg))\n\ndef display_driver(cfg: DictConfig) -&gt; None:\n    \"\"\"Display the database driver.\"\"\"\n    print(f\"Database driver is: {cfg.db.driver}\")\n\n@hydra.main(version_base=None, config_path=\"../../config\", config_name=\"main\")\ndef my_app(cfg: DictConfig) -&gt; None:\n    display_config(cfg)\n    display_driver(cfg)\n\n    # Get the SQL driver from the config\n    sql_driver = cfg.db.driver\n\n    # Now you can use sql_driver in your code\n    # For example, you might pass it to a function that initializes a\n    # database connection\n    initialize_database(sql_driver)\n\ndef initialize_database(driver: str) -&gt; None:\n    print(f\"Initializing database with driver: {driver}\")\n    # Your database initialization code here\n\nif __name__ == \"__main__\":\n    my_app()\n</code></pre>"},{"location":"tutorials/hydra-config/#example-output","title":"Example Output","text":"<p>When you run the script, you should see output similar to the following:</p> <pre><code>db:\n  driver: mysql\n  user: omry\n  password: secret\n\nDatabase driver is: mysql\nInitializing database with driver: mysql\n</code></pre>"},{"location":"tutorials/hydra-config/#using-hydra-in-a-jupyter-notebook","title":"Using Hydra in a Jupyter Notebook","text":"<p>To use Hydra in a Jupyter notebook, you can import the <code>omegaconf</code> package and manually load the configuration file using the <code>OmegaConf.load()</code> function.</p>"},{"location":"tutorials/hydra-config/#example-jupyter-notebook","title":"Example Jupyter Notebook","text":"<p>Here's how you can read the same configuration file in a Jupyter notebook:</p> <pre><code># notebooks/app.ipynb\n\nimport omegaconf\n\n# Load the config file\nconfig = omegaconf.OmegaConf.load(\"../config/main.yaml\")\n\n# Print the config\nprint(config)\n</code></pre>"},{"location":"tutorials/hydra-config/#output","title":"Output:","text":"<pre><code>{'db': {'driver': 'mysql', 'user': 'omry', 'password': 'secret'}}\n</code></pre> <pre><code>driver = config['db']['driver']\nprint(driver)\n</code></pre>"},{"location":"tutorials/hydra-config/#output_1","title":"Output:","text":"<pre><code>mysql\n</code></pre> <p>This guide should give you a good starting point for managing complex configurations in Python using Hydra. Whether you're working in a Python script or a Jupyter notebook, Hydra and OmegaConf offer powerful tools for this task.</p>"},{"location":"tutorials/hydra-config/#further-reading","title":"Further Reading","text":"<p>For more information on Hydra and its capabilities, you can refer to the following resources:</p> <ul> <li>Hydra Official Documentation</li> <li>OmegaConf Documentation</li> </ul>"},{"location":"tutorials/markdown-ml-model-documentation/","title":"Using Markdown for Machine Learning Model Documentation","text":"<p>Using Markdown files for Model Documentation</p> <p>This tutorial covers the process of documenting machine learning models using Markdown. It demonstrates how to create a comprehensive and readable documentation file, covering aspects like model training, parameters, and metadata.</p>"},{"location":"tutorials/markdown-ml-model-documentation/#overview","title":"Overview","text":"<ul> <li>Markdown provides a simple yet effective way to document machine learning models and their metadata.</li> <li>This tutorial uses the example of a linear regression model trained on the Boston housing dataset.</li> </ul>"},{"location":"tutorials/markdown-ml-model-documentation/#creating-your-markdown-documentation","title":"Creating Your Markdown Documentation","text":""},{"location":"tutorials/markdown-ml-model-documentation/#step-1-document-overview","title":"Step 1: Document Overview","text":"<p>Start with an overview of what your model does and the dataset used. <pre><code># Linear Regression Model Documentation\n\n## Overview\nThis document describes the process of training a Linear Regression model using the Boston housing dataset. It details the generation of a metadata file capturing essential information about the model parameters, performance metrics, and data characteristics.\n</code></pre></p>"},{"location":"tutorials/markdown-ml-model-documentation/#step-2-model-training-description","title":"Step 2: Model Training Description","text":"<p>Describe how the model is trained, including dataset details and model parameters. <pre><code>## Model Training\nThe model is trained using the Boston housing dataset from Scikit-Learn. We perform a basic train-test split, train a Linear Regression model using the training data, and then evaluate its performance on the test data.\n\n### Data\n- **Dataset Used**: Boston Housing Dataset\n- **Features**: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT\n- **Target**: Housing Price\n</code></pre></p>"},{"location":"tutorials/markdown-ml-model-documentation/#step-3-detailing-model-parameters","title":"Step 3: Detailing Model Parameters","text":"<p>List the parameters used in the model training. <pre><code>## Model Parameters\nThe model is trained with the following parameters:\n- **fit_intercept**: true\n- **normalize**: false\n- **copy_X**: true\n- **n_jobs**: null\n\nThese parameters are the default settings for Scikit-Learn's LinearRegression model.\n</code></pre></p>"},{"location":"tutorials/markdown-ml-model-documentation/#step-4-performance-metrics","title":"Step 4: Performance Metrics","text":"<p>Explain the performance metrics used to evaluate the model. <pre><code>## Performance Metrics\nThe model's performance is evaluated using the following metrics:\n- **Mean Squared Error (MSE)**\n- **R-squared (R2)**\n\nThe values for these metrics are computed based on the model's predictions on the test set.\n</code></pre></p>"},{"location":"tutorials/markdown-ml-model-documentation/#step-5-metadata-file-description","title":"Step 5: Metadata File Description","text":"<p>Describe the metadata file generated along with the model. <pre><code>## Metadata File\nAlongside the model, a metadata file (`service_sage_v1.2.0_linearReg_20240123_metadata.json`) is generated. This JSON file includes:\n- **Model Name**: Linear Regression\n- **Timestamp**: Date when the model is trained and metadata is generated.\n- **Model Parameters**: A list of parameters used to train the model.\n- **Performance Metrics**: MSE and R2 values calculated from the test set.\n- **Data Description**: Brief description of the dataset used.\n- **Feature Names**: List of feature names from the dataset.\n- **Target Name**: Name of the target variable.\n</code></pre></p>"},{"location":"tutorials/markdown-ml-model-documentation/#step-6-file-generation-process","title":"Step 6: File Generation Process","text":"<p>Conclude with details about how the model and metadata files are generated. <pre><code>## File Generation\nThe model is saved as a pickle file (`service_sage_v1.2.0_linearReg_20240123.pkl`), and the metadata is stored in a JSON file (`service_sage_v1.2.0_linearReg_20240123_metadata.json`). These files provide a snapshot of the model at the time of training, along with relevant information about its performance and configuration.\n</code></pre></p>"},{"location":"tutorials/markdown-ml-model-documentation/#conclusion","title":"Conclusion","text":"<p>This Markdown file offers a concise yet comprehensive overview of the model training and metadata generation process, suitable for inclusion in project documentation or a repository readme.</p>"},{"location":"tutorials/mkdocs-docs/","title":"Project Documentation With MkDocs","text":"<p>MkDocs is a powerful tool for building project documentation. With the Materials theme, it becomes even more robust, offering a plethora of features to make your documentation stand out. This tutorial focuses on key commands and features to help you build and deploy your documentation effectively.</p>"},{"location":"tutorials/mkdocs-docs/#local-development-with-mkdocs","title":"Local Development with MkDocs","text":"<p>When you're working on documentation in VS Code, MkDocs provides commands to build and serve your site locally. Here's how:</p> <p>Note</p> <p>This tutorial assumes that all necessary libraries and extensions are already installed as per the provided <code>pyproject.toml</code> and <code>mkdocs.yml</code> files.</p>"},{"location":"tutorials/mkdocs-docs/#building-and-serving-your-documentation","title":"Building and Serving Your Documentation","text":"<p>To build your documentation locally, you can run the following command:</p> <pre><code>mkdocs build\n</code></pre> <p>This command will generate a <code>site</code> folder containing the HTML files for your documentation.</p> <p>To serve your documentation and view it in a web browser, use:</p> <pre><code>mkdocs serve\n</code></pre> <p>Local Development in Action</p> <p>Before Running Commands: Your <code>docs</code> folder contains Markdown files.</p> <p>After Running Commands:  You'll have a <code>site</code> folder containing HTML files, and you can view your documentation at <code>http://127.0.0.1:8000/</code>.</p> <p>Stopping the Local MkDocs Server</p> <p>To stop the local MkDocs server, simply press <code>Ctrl + C</code> in the terminal window where <code>mkdocs serve</code> is running. This will shut down the site and free up the port it was using.</p> <p>Alternative Commands for Unix-like Systems</p> <p>If you want to run the MkDocs server on an alternative port or need to kill the process occupying port 8000, you can use the following commands from the Makefile:</p> <ul> <li> <p>Start MkDocs on an Alternative Port:    <pre><code>make docs_serve_alt_port\n</code></pre>   This command starts the MkDocs server on port 8001.</p> </li> <li> <p>Kill Process on Port 8000:    <pre><code>make docs_kill_port\n</code></pre>   This command kills any process that is occupying port 8000.</p> </li> </ul> <p>These commands are useful if you encounter port conflicts or need to run multiple instances of the MkDocs server. They should work on Unix-like systems, including both macOS and Linux.</p>"},{"location":"tutorials/mkdocs-docs/#deploying-to-github-pages","title":"Deploying to GitHub Pages","text":""},{"location":"tutorials/mkdocs-docs/#setting-up-github-pages-on-github","title":"Setting Up GitHub Pages on GitHub","text":"<p>Before deploying your documentation, you'll need to configure GitHub Pages.</p> <p>Steps to Configure GitHub Pages</p> <ol> <li>Make sure the <code>gh-pages</code> branch exists: Create it by running <code>git checkout -b gh-pages</code> if it doesn't exist.</li> <li>Navigate to your GitHub repository: Go to the repository where your MkDocs project is hosted.</li> <li>Access Repository Settings: Click on the \"Settings\" tab.</li> <li>Go to Pages Settings: In the sidebar, click on \"Pages.\"</li> <li>Select Publishing Source: Choose <code>gh-pages</code> from the branch dropdown menu.</li> <li>Select Folder: Choose <code>docs</code> from the folder dropdown menu.</li> <li>Save Changes: Click \"Save.\"</li> </ol>"},{"location":"tutorials/mkdocs-docs/#deploying-your-documentation","title":"Deploying Your Documentation","text":"<p>To deploy your documentation to GitHub Pages, run:</p> <pre><code>mkdocs gh-deploy\n</code></pre> <p>Deployment in Action</p> <p>After running the command, your site will be available at <code>https://&lt;your-github-username&gt;.github.io/&lt;repository-name&gt;/</code>.</p>"},{"location":"tutorials/mkdocs-docs/#understanding-mkdocsyml","title":"Understanding <code>mkdocs.yml</code>","text":"<p>Your <code>mkdocs.yml</code> file is the backbone of your MkDocs project. It specifies the theme, plugins, and other settings.</p> <p>Key Components of <code>mkdocs.yml</code></p> <ul> <li><code>theme</code>: Specifies the theme, in this case, Materials.</li> <li><code>markdown_extensions</code>: Lists the Markdown extensions used.</li> <li><code>plugins</code>: Lists the plugins used in the project.</li> </ul>"},{"location":"tutorials/mkdocs-docs/#conclusion","title":"Conclusion","text":"<p>MkDocs with the Materials theme offers a robust solution for building and deploying beautiful, functional documentation. Whether you're working locally in VS Code or deploying to GitHub Pages, the process is streamlined and efficient.</p>"},{"location":"tutorials/mkdocs-docs/#further-reading-and-resources","title":"Further Reading and Resources","text":"<p>For a deeper understanding of MkDocs and its various features, the following resources are highly recommended:</p> <ul> <li>Official MkDocs Documentation</li> <li>Materials for MkDocs</li> <li>Python Project Documentation with MkDocs - Real Python</li> </ul> <p>These resources provide comprehensive guides to MkDocs' features, configuration options, and methods for extending its capabilities. They are valuable for both newcomers and those looking to explore more advanced features.</p>"},{"location":"tutorials/model-persistence/","title":"Model output","text":"<p>The best practice for storing binary files for model persistence in machine learning projects is to use a binary file format such as HDF5, PyTorch's <code>.pt</code> file format, Pickle or Joblib. This allows you to save the model weights and architecture in a compact and efficient manner, which can then be loaded and used for prediction or further training.</p> <p>Here are some additional best practices for storing binary files:</p> <ul> <li>Store binary files in a separate directory: Create a separate directory (<code>models</code>) within your project structure to store binary files, to keep your code and data organized.</li> <li>Use version control: Use version control tools like Git to manage changes to binary files, so that you can revert to previous versions if necessary.</li> <li>Use cloud/on-premises storage: Consider using the Data Hub remote server storage solutions to store binary files, so that they can be easily shared and accessed by multiple team members or systems.</li> <li>Store metadata: Store metadata along with the binary file, such as the training data used to create the model, the parameters used during training, and the accuracy metrics obtained. This information can be useful for understanding and reproducing the results.</li> </ul> <p>Warning</p> <p>If you are Git-tracking the model data, do not include the <code>model_output</code> directory in the centralized repository. This directory must only be placed in the user's data directory.</p> <p>By following these best practices, you can ensure that your binary files are stored and managed in a secure, organized, and accessible manner, making it easier to collaborate on and maintain your machine learning projects.</p>"},{"location":"tutorials/naming-conventions/","title":"Naming Conventions for Project Assets","text":"<p>In the intricate world of Data Science and Machine Learning (ML), the ability to navigate through vast amounts of code, data files, and documentation efficiently is pivotal to the success and maintainability of projects. Proper naming conventions are more than a mere aspect of stylistic preference; they form the backbone of project organization and play a critical role in ensuring that team members and automated systems can quickly understand and manage various project components.</p> <p>This document presents a comprehensive set of guidelines designed to bring order and clarity to the diverse assets of data science and ML projects. From the way we name our GitHub repositories to the structure of our data directories, each convention has been thoughtfully crafted to align with industry best practices and the unique demands of ML workflows.</p> <p>We delve into the specifics of naming branches, which enables contributors to discern the purpose of each branch at a glance, thereby streamlining collaboration and review processes. We also cover the conventions for data files, including the systematic versioning and timestamping that allow for the efficient tracking of dataset iterations and history.</p> <p>Model persistence files, the artifacts of our computational efforts, are given a structured naming framework that facilitates version control and model identification across different stages of a project. Furthermore, we extend these principles to the naming of Jupyter notebooks and scripts, recognizing their role as both developmental tools and as part of the project's documentation.</p> <p>In addition to defining these standards, we are committed to enforcing them through automated means. This document outlines how GitHub Actions can be leveraged to ensure adherence to these conventions, providing examples and setup instructions for integrating these checks into your continuous integration and deployment pipelines.</p> <p>By adhering to the naming conventions and enforcement strategies outlined in this document, teams can achieve a level of operational harmony that not only optimizes current project workflows but also paves the way for future scalability and knowledge transfer.</p>"},{"location":"tutorials/naming-conventions/#github-repository-naming-conventions-for-data-science-projects","title":"GitHub Repository Naming Conventions for Data Science Projects","text":""},{"location":"tutorials/naming-conventions/#overview","title":"Overview","text":"<p>Choosing the right naming convention for GitHub repositories in data science projects is crucial for clarity, organization, and ease of navigation. A well-defined naming convention helps team members and stakeholders to quickly understand the scope and purpose of a repository at a glance. This section outlines the guidelines for naming GitHub repositories related to data science projects.</p>"},{"location":"tutorials/naming-conventions/#naming-convention-structure","title":"Naming Convention Structure","text":"<p>Repositories should be named following this format:</p> <pre><code>&lt;prefix&gt;-&lt;descriptive-name&gt;[-&lt;optional-version&gt;]\n</code></pre>"},{"location":"tutorials/naming-conventions/#components","title":"Components","text":"<ul> <li>Prefix: A concise identifier related to the project's domain or   main technology.</li> <li>Descriptive Name: A clear and specific description of the   repository's content or purpose.</li> <li>Optional Version: A version number, if applicable, to distinguish   between different iterations or stages of the project.</li> </ul>"},{"location":"tutorials/naming-conventions/#guidelines","title":"Guidelines","text":"<ol> <li>Choose an Appropriate Prefix</li> <li>The prefix should represent the key area or technology of the      project, like <code>ml</code> for machine learning, <code>nlp</code> for natural language      processing, <code>cv</code> for computer vision, etc.</li> <li> <p>This helps in categorizing and quickly identifying the project's      domain.</p> </li> <li> <p>Be Clear and Specific</p> </li> <li>Use descriptive and meaningful terms that accurately reflect the      primary focus or functionality of the repository.</li> <li> <p>Avoid vague or overly broad terms that do not convey the specific      purpose of the repository.</p> </li> <li> <p>Include Versioning Where Necessary</p> </li> <li>For projects that have multiple versions or stages, include a      version number at the end of the repository name.</li> <li> <p>This is useful for tracking development progress and      differentiating between major project phases.</p> </li> <li> <p>Maintain Consistency</p> </li> <li>Keep all repository names in lowercase and use hyphens (<code>-</code>) to      separate words. This enhances readability and avoids issues with      URL encoding.</li> </ol>"},{"location":"tutorials/naming-conventions/#examples","title":"Examples","text":"<ul> <li><code>ml-predictive-modeling</code></li> <li><code>nlp-chatbot-interface</code></li> <li><code>cv-facial-recognition-v1</code></li> <li><code>ds-data-cleaning-tools</code></li> </ul>"},{"location":"tutorials/naming-conventions/#conclusion","title":"Conclusion","text":"<p>Adopting these naming conventions for GitHub repositories in data science projects promotes a structured and systematic approach to repository management. It ensures that the repository names are informative, organized, and aligned with the project's objectives and technical domain.</p>"},{"location":"tutorials/naming-conventions/#git-branch-naming-standards-for-ml-projects","title":"Git Branch Naming Standards for ML Projects","text":""},{"location":"tutorials/naming-conventions/#overview_1","title":"Overview","text":"<p>For machine learning projects, the clarity of the Git repository is non-negotiable. A consistent approach to branch naming is fundamental to this clarity. It enables rapid identification of the branch's purpose, streamlining collaboration and making navigation within the repository more intuitive.</p>"},{"location":"tutorials/naming-conventions/#naming-convention-structure_1","title":"Naming Convention Structure","text":"<p>Branch names must be constructed as follows:</p> <pre><code>&lt;category&gt;/&lt;description&gt;-&lt;issue_number_or_jira_key&gt;\n</code></pre>"},{"location":"tutorials/naming-conventions/#components_1","title":"Components","text":"<ul> <li>Category: A concise keyword that delineates the branch based on   the nature of the work being conducted.</li> <li>Description: A succinct, yet informative descriptor of the   specific task, feature, or focus of the branch.</li> <li>Issue Number or Jira Key: This is a mandatory inclusion that   connects the branch to a corresponding task or issue in your project   management tool, be it GitHub or Jira.</li> </ul>"},{"location":"tutorials/naming-conventions/#categories","title":"Categories","text":"<p>Categories provide immediate context regarding the branch's domain of work. Standard categories include:</p> Category Description <code>feature</code> New feature development or enhancements <code>bugfix</code> Targeted branches for bug resolution <code>data</code> Data management activities, like acquisition or processing <code>experiment</code> Exploratory or experimental development <code>model</code> Model creation, testing, or deployment <code>docs</code> Documentation creation or updates <code>refactor</code> Code restructuring to improve performance without altering functionality <code>test</code> Test development or modification <code>chore</code> Routine tasks or minor updates"},{"location":"tutorials/naming-conventions/#examples_1","title":"Examples","text":"<p>Below are examples of branch names that adhere to our standards:</p> <ul> <li><code>feature/user-authentication-DATA123</code></li> <li><code>data/dataset-enhancement-GH15</code></li> <li><code>model/performance-improvement-DATA22</code></li> <li><code>bugfix/data-loading-error-GH45</code></li> <li><code>docs/api-documentation-update</code></li> <li><code>refactor/code-optimization-DATA78</code></li> <li><code>test/new-model-tests-GH27</code></li> </ul>"},{"location":"tutorials/naming-conventions/#guidelines_1","title":"Guidelines","text":"<ul> <li>Maintain lowercase lettering in all branch names for uniformity.</li> <li>Use hyphens to separate words within the branch name, ensuring   readability.</li> <li>Keep branch names brief, yet descriptive enough to clearly articulate   their intent at a glance.</li> <li>Including the issue number or Jira key in the branch name is not   optional; it is a required practice for traceability and coherence.</li> </ul>"},{"location":"tutorials/naming-conventions/#conclusion_1","title":"Conclusion","text":"<p>Adhering to these branch naming conventions is imperative for the organization and accessibility of our ML project repositories. It supports our team in promptly discerning the intent of each branch, enhancing effective collaboration and project oversight.</p>"},{"location":"tutorials/naming-conventions/#git-commit-message-standards-for-ml-projects","title":"Git Commit Message Standards for ML Projects","text":""},{"location":"tutorials/naming-conventions/#introduction","title":"Introduction","text":"<p>Clear and informative commit messages are a cornerstone of effective team collaboration in machine learning projects. To enhance this aspect of our workflow, we incorporate tools that facilitate the creation of standardized commit messages and link them to their respective task management entries. This section details the standards and tools to assist with writing commit messages that include GitHub issue numbers or Jira keys.</p>"},{"location":"tutorials/naming-conventions/#commit-message-structure","title":"Commit Message Structure","text":"<p>Commit messages should adhere to the following format:</p> <pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;subject&gt; [#issue_number | #jira_key]\n\n&lt;body&gt;\n\n&lt;footer&gt;\n</code></pre>"},{"location":"tutorials/naming-conventions/#components_2","title":"Components","text":"<ul> <li>Type: The category of change you're committing.</li> <li>Scope: The particular area of the codebase affected by the   changes.</li> <li>Subject: A brief description of the changes, including the issue   tracker reference.</li> <li>Body: An in-depth explanation of the changes and the reasoning   behind them.</li> <li>Footer: Any additional notes or references.</li> </ul>"},{"location":"tutorials/naming-conventions/#commit-types","title":"Commit Types","text":"<p>Consistency in the types of changes committed is key for readability and organization. Here's a table of types to use in commit messages, similar to those used for branch naming:</p> Type Description <code>feat</code> Introducing new features or enhancements <code>fix</code> Bug fixes <code>data</code> Modifications in data processing or management <code>experiment</code> Changes to experimental or exploratory code <code>model</code> Changes related to model development, testing, or deployment <code>docs</code> Documentation additions or updates <code>refactor</code> Code changes that enhance performance without altering functionality <code>test</code> Test writing or fixing <code>chore</code> Routine tasks or updates not impacting the production code"},{"location":"tutorials/naming-conventions/#best-practices","title":"Best Practices","text":"<ol> <li>Make it Concise: The subject line should be succinct yet     descriptive. It\u2019s the first line of communication and often what     people will see in their notifications or logs.</li> <li>Use the Imperative Mood: Write your commit message as if you are     giving an order or instruction. For example, \u201cfix\u201d instead of     \u201cfixed\u201d or \u201cfixes\u201d.</li> <li>Capitalize the Subject Line: Start the subject line with a     capital letter.</li> <li>Do Not End the Subject Line with a Period: The subject line is a     title or headline; it doesn\u2019t need a period at the end.</li> <li>Separate Subject from Body with a Blank Line: This helps various     tools (e.g., log, shortlog) to correctly identify the subject from     the rest of the content.</li> <li>Use the Body to Explain the \"What\" and \"Why\": Not just the     \"how\". The code itself explains \u201chow\u201d a change has been made; the     commit message should explain what and why.</li> <li>Reference Issues and Pull Requests Liberally: When applicable,     include links to the related issues and pull requests, which     provides additional context.</li> </ol>"},{"location":"tutorials/naming-conventions/#vs-code-extensions-for-commit-messages","title":"VS Code Extensions for Commit Messages","text":"<p>To support our commit message standards, we recommend using the following Visual Studio Code extensions:</p> <ul> <li> <p>Gitmoji for VS Code: An extension that allows you to include emoji   in your commit messages, which can help to quickly identify the   purpose or nature of a change. It's available on the VS Code   Marketplace:   Gitmoji.</p> </li> <li> <p>Commit Message   Editor:   This extension helps to enforce the structure of a commit message,   ensuring that each part of our commit message standard is followed. It   offers prompts and snippets to guide you in creating a well-formatted   message.</p> </li> </ul>"},{"location":"tutorials/naming-conventions/#reference-for-best-practices","title":"Reference for Best Practices","text":"<p>For an in-depth understanding of best practices for git commit messages, refer to the following article: Git Commit Message Best Practices. This article provides a comprehensive guide and rationale behind the composition of effective commit messages.</p>"},{"location":"tutorials/naming-conventions/#conclusion_2","title":"Conclusion","text":"<p>By utilizing the outlined commit message structure, type guidelines, and recommended VS Code extensions, our team can ensure a uniform and informative history in our repository. Commit messages become more than a formality; they transform into a rich log that conveys the what, why, and how of our development process, enhancing clarity and streamlining collaboration.</p>"},{"location":"tutorials/naming-conventions/#naming-conventions-for-data-folders-in-ml-projects","title":"Naming Conventions for Data Folders in ML Projects","text":""},{"location":"tutorials/naming-conventions/#overview_2","title":"Overview","text":"<p>In Machine Learning (ML) projects, organizing and managing data efficiently is crucial. Adopting a clear and consistent naming convention for data folders not only facilitates better data management but also enhances collaboration and project understanding. Below are guidelines for naming data folders in ML projects.</p>"},{"location":"tutorials/naming-conventions/#folder-structure-and-naming-convention","title":"Folder Structure and Naming Convention","text":"<p>The data folder structure should be organized into distinct categories, each serving a specific purpose in the data processing pipeline. Here\u2019s the recommended structure:</p> <pre><code>data\n\u251c\u2500\u2500 raw                   # Original, immutable data dump\n\u251c\u2500\u2500 external              # Data from third-party sources\n\u251c\u2500\u2500 interim               # Intermediate data, partially processed\n\u251c\u2500\u2500 processed             # Fully processed data, ready for analysis\n\u2514\u2500\u2500 features              # Engineered features ready for model training\n</code></pre>"},{"location":"tutorials/naming-conventions/#guidelines-for-naming","title":"Guidelines for Naming","text":"<ol> <li>Descriptive and Clear:</li> <li> <p>Folder names should be self-explanatory, indicating clearly what      type of data they contain.</p> </li> <li> <p>Consistent Format:</p> </li> <li> <p>Use a consistent naming format across all folders. The recommended      format is all lowercase with words separated by hyphens for      readability.</p> </li> <li> <p>Standard Categories:</p> </li> <li> <p>Stick to standard naming categories (<code>raw</code>, <code>external</code>, <code>interim</code>,      <code>processed</code>, <code>features</code>) as they are widely recognized in data      science and ML communities.</p> </li> <li> <p>Avoid Overly Specific Names:</p> </li> <li>While being descriptive, avoid overly specific names which might      become irrelevant as the project evolves. The name should be broad      enough to encompass various data types that might fall into that      category.</li> </ol>"},{"location":"tutorials/naming-conventions/#explanation-of-categories","title":"Explanation of Categories","text":"<ul> <li>Raw: </li> <li> <p>Contains the original datasets. Data in this folder is immutable      and serves as a backup for all processing steps.</p> </li> <li> <p>External: </p> </li> <li> <p>For data sourced from outside the original dataset. This includes      third-party data, external APIs, or any supplementary data.</p> </li> <li> <p>Interim: </p> </li> <li> <p>Holds data that is in the process of being cleaned or transformed.      Useful for datasets that are not final but are needed in      intermediate stages.</p> </li> <li> <p>Processed: </p> </li> <li> <p>Contains the final version of the dataset, which is ready for      analysis or modeling. Data in this folder is cleaned, formatted,      and pre-processed.</p> </li> <li> <p>Features: </p> </li> <li>Dedicated to storing feature sets that are used in machine learning      models. This includes transformed data, new features, and selected      features.</li> </ul>"},{"location":"tutorials/naming-conventions/#conclusion_3","title":"Conclusion","text":"<p>Adhering to this naming convention for data folders will ensure a well-organized and manageable data structure in your ML projects. It facilitates easy access, understanding, and efficient data handling, crucial for the success of any ML project.</p>"},{"location":"tutorials/naming-conventions/#data-files-naming-conventions-in-ml-projects","title":"Data Files Naming Conventions in ML Projects","text":""},{"location":"tutorials/naming-conventions/#overview_3","title":"Overview","text":"<p>Proper naming conventions for data files are essential in Machine Learning (ML) projects to ensure easy identification, management, and tracking of datasets. This guide provides a structured approach to naming data files, particularly when handling multiple versions, subsets, or types of data.</p>"},{"location":"tutorials/naming-conventions/#naming-convention-structure_2","title":"Naming Convention Structure","text":"<p>Data file names should follow this format: <pre><code>&lt;dataset_name&gt;_&lt;version&gt;_&lt;creation_date&gt;_&lt;description&gt;.&lt;extension&gt;\n</code></pre></p>"},{"location":"tutorials/naming-conventions/#components_3","title":"Components","text":"<ul> <li>Dataset Name: A concise identifier for the dataset.</li> <li>Version: Version number or identifier of the dataset.</li> <li>Creation Date: Date when the dataset was created or last modified,   in the format <code>YYYYMMDD</code>.</li> <li>Description: A brief, clear description of the dataset or its   specific subset.</li> <li>Extension: The appropriate file extension (e.g., <code>.csv</code>, <code>.xlsx</code>,   <code>.json</code>).</li> </ul>"},{"location":"tutorials/naming-conventions/#guidelines_2","title":"Guidelines","text":"<ol> <li>Clarity and Descriptiveness:</li> <li> <p>Ensure the name is descriptive enough to give an immediate      understanding of the dataset\u2019s content and scope.</p> </li> <li> <p>Consistency:</p> </li> <li> <p>Maintain consistency in the naming convention across all data      files. This includes consistent use of underscores, date formats,      and versioning systems.</p> </li> <li> <p>Versioning:</p> </li> <li> <p>Use a logical versioning system, like semantic versioning (e.g.,      <code>v1.0</code>, <code>v1.1</code>, <code>v2.0</code>) or sequential numbering (<code>01</code>, <code>02</code>, etc.).</p> </li> <li> <p>Date Format:</p> </li> <li> <p>Stick to a standard date format (<code>YYYYMMDD</code>). This avoids ambiguity      and makes it easy to sort files chronologically.</p> </li> <li> <p>Concise Descriptions:</p> </li> <li> <p>Keep the description part brief yet informative. Avoid overly long      names but provide enough context to distinguish the dataset.</p> </li> <li> <p>File Extensions:</p> </li> <li>Use appropriate file extensions to indicate the file type, which      helps in quickly identifying the software or tools needed to open      them.</li> </ol>"},{"location":"tutorials/naming-conventions/#examples_2","title":"Examples","text":"<ul> <li><code>customer_data_v1.0_20240101_initial.csv</code></li> <li><code>sales_report_v2.2_20240305_updated.xlsx</code></li> <li><code>image_dataset_v1.0_20240220_raw.json</code></li> </ul>"},{"location":"tutorials/naming-conventions/#conclusion_4","title":"Conclusion","text":"<p>Adhering to these naming conventions for data files in ML projects will significantly enhance data manageability. It ensures ease of access, effective version control, and clear understanding, facilitating efficient data analysis and collaboration within the team.</p>"},{"location":"tutorials/naming-conventions/#model-persistence-file-naming-conventions","title":"Model Persistence File Naming Conventions","text":"<p>Model Persistence File</p> <p><code>&lt;project_name&gt;_&lt;model_version&gt;_&lt;model_type&gt;_&lt;timestamp&gt;.pkl</code></p> <p>When persisting machine learning models, adopting a consistent naming convention for binary files is crucial. This ensures easy identification and management of different model versions across various projects. The recommended format now includes the project name, model version, model type, and a timestamp.</p> Field Definition <code>project_name</code> The name of the project the model is associated with <code>model_version</code> The version of the model, following semantic versioning (MAJOR.MINOR.PATCH) <code>model_type</code> Type or name of the model (e.g., linearReg, neuralNet) <code>timestamp</code> Date when the model was persisted (YYYYMMDD format)"},{"location":"tutorials/naming-conventions/#examples_3","title":"Examples","text":"<ul> <li><code>service_sage_v1.2.0_linearReg_20240123.pkl</code> - Indicates a linear   regression model from the Service Sage project, version 1.2.0, updated   on January 23, 2024.</li> <li><code>one_assist_v3.0.1_neuralNet_20240215.pkl</code> - Represents a neural   network model for One Assist, version 3.0.1, updated on February 15,   2024.</li> </ul>"},{"location":"tutorials/naming-conventions/#versioning-scheme","title":"Versioning Scheme","text":"<ul> <li>MAJOR: Incremented for incompatible API changes.</li> <li>MINOR: Incremented for adding functionality in a   backward-compatible manner.</li> <li>PATCH: Incremented for backward-compatible bug fixes.</li> </ul>"},{"location":"tutorials/naming-conventions/#metadata-storage","title":"Metadata Storage","text":"<ul> <li>Alongside each model file, store a corresponding JSON file containing   model metadata (e.g.,   <code>service_sage_v1.2.0_linearReg_20240123_metadata.json</code>). This should   include information about training data, model parameters, performance   metrics, etc.</li> </ul>"},{"location":"tutorials/naming-conventions/#documentation-and-registry","title":"Documentation and Registry","text":"<ul> <li>Maintain a Makefile to automate the documentation or registry   generation process. This file should include commands to create and   update a comprehensive list of all models, their versions,   descriptions, and changelogs for team collaboration and future   reference.</li> </ul>"},{"location":"tutorials/naming-conventions/#automate-metadata-creation-using-json-files","title":"Automate Metadata Creation using <code>JSON</code> files","text":"<p>Below is an example of a Python script that trains a simple linear regression model using Scikit-Learn, then saves the model along with a JSON file containing metadata. The metadata includes information such as the model's parameters, performance metrics, and training data characteristics.</p> <pre><code>import json\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.datasets import load_boston\nimport joblib\n\n# Load sample data\ndata = load_boston()\nX = data.data\ny = data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train a linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predict using the model\ny_pred = model.predict(X_test)\n\n# Calculate performance metrics\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\n# Save the model\nmodel_filename = 'linear_regression_model.pkl'\njoblib.dump(model, model_filename)\n\n# Metadata\nmetadata = {\n    'model_name': 'Linear Regression',\n    'timestamp': '20240123',\n    'model_parameters': model.get_params(),\n    'performance_metrics': {\n        'mean_squared_error': mse,\n        'r2_score': r2\n    },\n    'data_description': 'Boston housing dataset',\n    'feature_names': data.feature_names.tolist(),\n    'target_name': 'Housing Price'\n}\n\n# Save metadata to a JSON file\nmetadata_filename = 'service_sage_v1.2.0_linearReg_20240123_metadata.json'\nwith open(metadata_filename, 'w') as f:\n    json.dump(metadata, f, indent=4)\n\nprint(f\"Model and metadata saved as {model_filename} and {metadata_filename} respectively.\")\n</code></pre> <p>In this script:</p> <ul> <li>The Boston housing dataset is used for demonstration purposes.</li> <li>A linear regression model is trained on the dataset.</li> <li>The model is then used to make predictions on the test set.</li> <li>Performance metrics like Mean Squared Error (MSE) and R-squared (R2)   are calculated.</li> <li>The model is saved as a pickle file using <code>joblib</code>.</li> <li>Metadata including model parameters, performance metrics, and data   description are saved in a JSON file.</li> </ul> <p>This script provides a basic framework. You might need to adjust it according to the specifics of your dataset, model, and required metadata details.</p> <p>Assuming the script has been run as provided, the output of the JSON file (<code>service_sage_v1.2.0_linearReg_20240123_metadata.json</code>) would hypothetically look like this:</p> <pre><code>{\n    \"model_name\": \"Linear Regression\",\n    \"timestamp\": \"20240123\",\n    \"model_parameters\": {\n        \"copy_X\": true,\n        \"fit_intercept\": true,\n        \"n_jobs\": null,\n        \"normalize\": false\n    },\n    \"performance_metrics\": {\n        \"mean_squared_error\": 24.291119474973684,\n        \"r2_score\": 0.6687594935356314\n    },\n    \"data_description\": \"Boston housing dataset\",\n    \"feature_names\": [\n        \"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"\n    ],\n    \"target_name\": \"Housing Price\"\n}\n</code></pre> <p>In this hypothetical example:</p> <ul> <li>The <code>model_parameters</code> section includes the parameters used for the   linear regression model. These values are defaults from Scikit-Learn's   <code>LinearRegression</code>.</li> <li>The <code>performance_metrics</code> show the Mean Squared Error (MSE) and   R-squared (R2) values calculated from the model's predictions on the   test dataset. The actual numbers (24.291119474973684 for MSE and   0.6687594935356314 for R2) are hypothetical and would vary based on   the training and test data splits, as well as any changes in model   parameters or the underlying data.</li> <li>The <code>data_description</code>, <code>feature_names</code>, and <code>target_name</code> provide   context about the dataset used for training the model.</li> </ul>"},{"location":"tutorials/naming-conventions/#using-markdown-files-for-model-documentation","title":"Using Markdown files for Model Documentation","text":"<p>Here's a Markdown file (<code>service_sage_v1.2.0_linearReg_20240123_documentation.md</code>) that documents the process and output of the Python script for training a linear regression model and generating its metadata file. This documentation can be included in your project repository to explain the model and metadata creation process.</p> <pre><code># Linear Regression Model Documentation\n\n## Overview\nThis document describes the process of training a Linear Regression model using the Boston housing dataset. It also details the generation of a metadata file that accompanies the model, capturing essential information about the model parameters, performance metrics, and data characteristics.\n\n## Model Training\nThe model is trained using the Boston housing dataset from Scikit-Learn. We perform a basic train-test split, train a Linear Regression model using the training data, and then evaluate its performance on the test data.\n\n### Data\n- **Dataset Used**: Boston Housing Dataset\n- **Features**: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT\n- **Target**: Housing Price\n\n## Model Parameters\nThe model is trained with the following parameters:\n- **fit_intercept**: true\n- **normalize**: false\n- **copy_X**: true\n- **n_jobs**: null\n\nThese parameters are the default settings for Scikit-Learn's LinearRegression model.\n\n## Performance Metrics\nThe model's performance is evaluated using the following metrics:\n- **Mean Squared Error (MSE)**\n- **R-squared (R2)**\n\nThe values for these metrics are computed based on the model's predictions on the test set.\n\n## Metadata File\nAlongside the model, a metadata file (`service_sage_v1.2.0_linearReg_20240123_metadata.json`) is generated. This JSON file includes:\n- **Model Name**: Linear Regression\n- **Timestamp**: Date when the model is trained and metadata is generated.\n- **Model Parameters**: A list of parameters used to train the model.\n- **Performance Metrics**: MSE and R2 values calculated from the test set.\n- **Data Description**: Brief description of the dataset used.\n- **Feature Names**: List of feature names from the dataset.\n- **Target Name**: Name of the target variable.\n\n## File Generation\nThe model is saved as a pickle file (`service_sage_v1.2.0_linearReg_20240123.pkl`), and the metadata is stored in a JSON file (`service_sage_v1.2.0_linearReg_20240123_metadata.json`). These files provide a snapshot of the model at the time of training, along with relevant information about its performance and configuration.\n</code></pre> <p>This Markdown file offers a concise yet comprehensive overview of the model training and metadata generation process, suitable for inclusion in project documentation or a repository readme. It explains the dataset, model parameters, performance metrics, and the structure of the generated metadata file.</p>"},{"location":"tutorials/naming-conventions/#directory-structure","title":"Directory Structure","text":"<p>Store related files in the same directory. For better organization, especially for projects with multiple models, use subdirectories:</p> <pre><code>/models\n    /service_sage_v1.2.0_linearReg_20240123\n        service_sage_v1.2.0_linearReg_20240123.pkl\n        service_sage_v1.2.0_linearReg_20240123_metadata.json\n        service_sage_v1.2.0_linearReg_20240123.pkl_documentation.md\n</code></pre>"},{"location":"tutorials/naming-conventions/#notebook-and-script-naming-conventions-in-ml-projects","title":"Notebook and Script Naming Conventions in ML Projects","text":""},{"location":"tutorials/naming-conventions/#overview_4","title":"Overview","text":"<p>Properly naming Jupyter notebooks and scripts is essential for quick identification, efficient management, and collaborative ease in machine learning projects. A systematic naming convention helps in understanding the file's purpose at a glance and tracking its evolution over time.</p>"},{"location":"tutorials/naming-conventions/#naming-convention-structure_3","title":"Naming Convention Structure","text":"<p>Names for notebooks and scripts should follow this format: <pre><code>&lt;type&gt;_&lt;topic&gt;_&lt;version&gt;_&lt;YYYYMMDD&gt;.&lt;extension&gt;\n</code></pre></p>"},{"location":"tutorials/naming-conventions/#components_4","title":"Components:","text":"<ul> <li>Type: A short identifier indicating the nature of the work (e.g.,   <code>eda</code> for exploratory data analysis, <code>preprocess</code> for data   preprocessing, <code>model</code> for model training).</li> <li>Topic: A concise descriptor of the notebook's or script's main   focus.</li> <li>Version: An optional version number or identifier, especially   useful if the notebook or script undergoes significant iterative   updates.</li> <li>Date: The creation or last modified date in <code>YYYYMMDD</code> format.</li> <li>Extension: The file extension, like <code>.ipynb</code> for Jupyter   notebooks, <code>.py</code> for Python scripts.</li> </ul>"},{"location":"tutorials/naming-conventions/#guidelines_3","title":"Guidelines:","text":"<ol> <li>Descriptive and Purposeful:</li> <li>Start with a type that categorizes the file based on its primary      purpose in the ML workflow.</li> <li> <p>The topic should be sufficiently descriptive to convey the specific      focus or task of the notebook/script.</p> </li> <li> <p>Versioning:</p> </li> <li> <p>Include a version number if the file is part of an iterative      process, such as <code>v1</code>, <code>v2</code>, or more detailed semantic versioning      like <code>1.0</code>, <code>1.1</code>.</p> </li> <li> <p>Date Stamp:</p> </li> <li> <p>Adding the date (in <code>YYYYMMDD</code> format) helps in identifying the      most recent version or understanding the timeline of development.</p> </li> <li> <p>Consistency:</p> </li> <li> <p>Maintain a consistent naming convention across all notebooks and      scripts for ease of organization and retrieval.</p> </li> <li> <p>Clarity and Brevity:</p> </li> <li>Ensure the name is clear yet concise. Avoid overly long names but      provide enough information to understand the file's content and      purpose.</li> </ol>"},{"location":"tutorials/naming-conventions/#examples_4","title":"Examples:","text":"<ul> <li><code>eda_customer_segmentation_v1_20240101.ipynb</code></li> <li><code>preprocess_data_cleaning_v2_20240215.py</code></li> <li><code>model_train_regression_20240310.ipynb</code></li> </ul>"},{"location":"tutorials/naming-conventions/#conclusion_5","title":"Conclusion","text":"<p>This naming convention for Jupyter notebooks and scripts will foster a more organized and manageable ML project environment. It aids in quickly locating specific files, understanding their purpose, and tracking their evolution over time.</p>"},{"location":"tutorials/naming-conventions/#enforcing-naming-conventions-using-github-actions","title":"Enforcing Naming Conventions Using GitHub Actions","text":""},{"location":"tutorials/naming-conventions/#overview_5","title":"Overview","text":"<p>In machine learning projects, maintaining consistent naming conventions across various assets like Jupyter notebooks, scripts, model files, data files, and Git branches is essential for organization and clarity. To enforce these conventions, we can utilize GitHub Actions, an automation tool that integrates seamlessly with GitHub repositories.</p>"},{"location":"tutorials/naming-conventions/#github-actions-for-naming-convention-checks","title":"GitHub Actions for Naming Convention Checks","text":"<p>GitHub Actions can be configured to automatically run checks on the names of files and branches whenever a new commit is pushed or a pull request is made. This process helps ensure that all team members adhere to the established naming conventions.</p>"},{"location":"tutorials/naming-conventions/#setup-and-configuration","title":"Setup and Configuration","text":"<ol> <li>Creating the Workflow File:</li> <li>A YAML file is created in the <code>.github/workflows/</code> directory of the      repository, named <code>check_naming_convention.yml</code>.</li> <li> <p>This file defines the GitHub Action workflow, specifying when the      checks should run (e.g., on push and pull requests) and what script      to execute.</p> </li> <li> <p>Writing the Validation Script:</p> </li> <li>A Python script, <code>validate_naming.py</code>, is placed in the      <code>.github/scripts/</code> directory.</li> <li>This script contains functions to validate the naming conventions      of Jupyter notebooks, Python scripts, model persistence files, data      files, and Git branches.</li> <li> <p>The script uses regular expressions to match filenames and branch      names against the specified patterns.</p> </li> <li> <p>Workflow Execution:</p> </li> <li>The workflow runs the Python script whenever code is pushed to the      repository or a pull request is made.</li> <li>The script checks each file and branch name involved in the      push/pull request against the defined naming conventions.</li> <li>If a naming convention violation is detected, the script exits with      a non-zero status, causing the GitHub Action to fail. This failure      is visible in the pull request or push, alerting the contributor to      the naming issue.</li> </ol>"},{"location":"tutorials/naming-conventions/#example-workflow-file","title":"Example Workflow File","text":"<pre><code>name: Check Naming Convention\n\non: [push, pull_request]\n\njobs:\n  naming-convention-check:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n\n    - name: Validate Naming Convention\n      run: python .github/scripts/validate_naming.py\n      env:\n        GITHUB_REF: ${{ github.ref }}\n</code></pre>"},{"location":"tutorials/naming-conventions/#example-python-script-structure","title":"Example Python Script Structure","text":"<pre><code>\"\"\"\nThis module `validate_naming.py` is part of a GitHub Actions workflow\ndesigned to enforce naming conventions across various assets in a\nmachine learning project. It includes functions to validate the naming\nof Jupyter notebooks, Python scripts, model persistence files, data\nfiles, and Git branches.\n\nThe module contains the following key functions: -\n`validate_ml_file_naming`: Checks naming conventions for Jupyter\nnotebooks and Python scripts.  - `validate_model_file_naming`: Ensures\nmodel file names adhere to the specified naming standards.  -\n`validate_data_file_naming`: Validates data file names against the\nproject's naming conventions.  - `validate_git_branch_naming`: Checks if\nthe Git branch names follow the predefined naming patterns.\n\nThe script is executed as part of a GitHub Actions workflow and is\nessential for maintaining consistency, readability, and organization of\nfiles in the repository.\n\nNote: - This script is meant to be run as a GitHub Action and relies on\nenvironmental variables provided by GitHub workflows.  - Regular\nexpressions are used for pattern matching to validate the naming\nconventions.\n\"\"\"\n\nimport os\nimport re\nimport sys\n\n\ndef validate_ml_file_naming(filename):\n    \"\"\"\n    Check if the filename adheres to the ML project's naming convention.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the filename matches the naming convention, False\n        otherwise.\n\n    Notes\n    -----\n    The naming convention expected:\n    '&lt;type&gt;_&lt;topic&gt;_&lt;version&gt;_&lt;YYYYMMDD&gt;.&lt;extension&gt;' Where: - &lt;type&gt; is\n    a flexible, user-defined category (e.g., 'eda', 'preprocess').  -\n    &lt;topic&gt; is a concise descriptor of the main focus.  - &lt;version&gt; is\n    an optional version identifier.  - &lt;YYYYMMDD&gt; is the creation or\n    last modified date.  - &lt;extension&gt; is either '.ipynb' for Jupyter\n    notebooks or '.py' for Python scripts.\n    \"\"\"\n    # Regex pattern with a more flexible 'type' component\n    pattern = re.compile(r\"^\\w+_\\w+(_v\\d+)?_\\d{8}\\.[ipynb|py]$\")\n    return pattern.match(filename)\n\n\ndef validate_model_file_naming(filename):\n    \"\"\"\n    Check if the filename adheres to the model persistence file naming\n    convention.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the model file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the filename matches the naming convention, False\n        otherwise.\n\n    Notes\n    -----\n    The naming convention for model persistence files is:\n    '&lt;project_name&gt;_&lt;model_version&gt;_&lt;model_type&gt;_&lt;timestamp&gt;.pkl' This\n    includes: - `project_name`: Name of the project.  - `model_version`:\n    Version of the model, following semantic versioning.  -\n    `model_type`: Type or name of the model.  - `timestamp`: Date when\n    the model was saved (YYYYMMDD format).\n    \"\"\"\n    pattern = re.compile(r\"^\\w+_(v\\d+\\.\\d+\\.\\d+)_\\w+_\\d{8}\\.pkl$\")\n    return pattern.match(filename)\n\n\ndef validate_data_file_naming(filename):\n    \"\"\"\n    Check if the filename adheres to the data file naming convention.\n\n    Parameters\n    ----------\n    filename : str\n        The name of the data file to be checked.\n\n    Returns\n    -------\n    bool\n        True if the filename matches the naming convention, False\n        otherwise.\n\n    Notes\n    -----\n    The naming convention for data files is:\n    '&lt;dataset_name&gt;_&lt;version&gt;_&lt;creation_date&gt;_&lt;description&gt;.&lt;extension&gt;'\n    This includes: - `dataset_name`: Identifier for the dataset.  -\n    `version`: Version of the dataset, following semantic versioning or\n    numbering.  - `creation_date`: Date of creation or last modification\n    (YYYYMMDD format).  - `description`: Brief description of the\n    dataset or subset.  - `extension`: File extension indicating the\n    file type (e.g., 'csv', 'xlsx', 'json').\n    \"\"\"\n    pattern = re.compile(r\"^\\w+_(v\\d+\\.\\d+\\.\\d+|\\d+)_\\d{8}_\\w+\\.\\w+$\")\n    return pattern.match(filename)\n\n\ndef validate_git_branch_naming():\n    \"\"\"\n    Check if the Git branch name adheres to the naming convention.\n    \"\"\"\n    branch_ref = os.getenv(\"GITHUB_REF\")\n    if not branch_ref:\n        print(\"No branch reference found.\")\n        return True  # Might not be a branch push, so don't fail the check.\n\n    # Extract the branch name from refs/heads/your-branch-name\n    branch_name = branch_ref.split(\"/\")[-1]\n    pattern = re.compile(\n        r\"^(feature|bugfix|data|experiment|model|docs|refactor|test|chore)/[\\w-]+(_\\d+)?$\"\n    )\n    return pattern.match(branch_name)\n\n\ndef main():\n    \"\"\"\n    The main function to validate naming conventions in a machine\n    learning project.\n\n    This function executes a series of naming convention checks on\n    various file types, including model files, Jupyter notebooks, Python\n    scripts, and data files. It also validates Git branch names. If any\n    file or branch name does not adhere to the predefined naming\n    conventions, the function flags an error.\n\n    The checks are performed as follows: - For Git branch names, using\n    the `validate_git_branch_naming` function.  - For model files\n    (*.pkl), using the `validate_model_file_naming` function.  - For\n    Jupyter notebooks and Python scripts (*.ipynb, *.py), using the\n      `validate_ml_file_naming` function.\n    - For data files (*.csv, *.xlsx, *.json), using the\n      `validate_data_file_naming` function.\n\n    If any naming convention violations are found, the script exits with\n    a status code of 1, indicating an error. This is used in the context\n    of GitHub Actions workflows to flag naming convention violations in\n    pull requests or pushes.\n    \"\"\"\n    error = False\n    if not validate_git_branch_naming():\n        print(f\"Invalid Git branch naming convention.\")\n        error = True\n\n    for root, _, files in os.walk(\".\"):\n        for file in files:\n            if file.endswith(\".pkl\") and not validate_model_file_naming(file):\n                print(f\"Invalid model file naming convention: {file}\")\n                error = True\n            elif file.endswith((\".ipynb\", \".py\")) and not \\\n                validate_ml_file_naming(file):\n                print(f\"Invalid notebook/script naming convention: {file}\")\n                error = True\n            elif file.endswith((\".csv\", \".xlsx\", \".json\")) and \\\n                 not validate_data_file_naming(file):\n                print(f\"Invalid data file naming convention: {file}\")\n                error = True\n\n\n    if error:\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"tutorials/naming-conventions/#conclusion_6","title":"Conclusion","text":"<p>Implementing this GitHub Actions workflow ensures that all team members adhere to the agreed-upon naming conventions, thereby maintaining consistency and organization across the project's assets. This automated check acts as a first line of defense, preventing naming irregularities from being merged into the main codebase.</p>"},{"location":"tutorials/poetry/","title":"Managing Dependencies with Poetry in Your Project","text":"<p>Poetry is a powerful package management tool that simplifies dependency management in Python projects. With the Cookiecutter template, you already have a pre-configured setup that makes it easy to manage your project's dependencies. This guide will walk you through how to effectively use Poetry in various scenarios, such as adding, updating, and removing dependencies.</p>"},{"location":"tutorials/poetry/#adding-new-dependencies","title":"Adding New Dependencies","text":"<p>Note</p> <p>The Cookiecutter template comes with a set of pre-configured dependencies specified in the <code>pyproject.toml</code> file. However, you may need to add additional packages as your project grows.</p>"},{"location":"tutorials/poetry/#adding-standard-dependencies","title":"Adding Standard Dependencies","text":"<p>To add a new package to your project, use the <code>poetry add</code> command:</p> <pre><code>poetry add &lt;package-name&gt;\n</code></pre> <p>Adding NumPy as a Dependency</p> <p>To add <code>numpy</code> to your project, run the following command:</p> <pre><code>poetry add numpy\n</code></pre>"},{"location":"tutorials/poetry/#adding-development-dependencies","title":"Adding Development Dependencies","text":"<p>For packages needed only during development, you can add them as development dependencies:</p> <pre><code>poetry add --dev &lt;package-name&gt;\n</code></pre> <p>Adding Pytest as a Development Dependency</p> <p>To add <code>pytest</code> as a development dependency, run:</p> <pre><code>poetry add --dev pytest\n</code></pre>"},{"location":"tutorials/poetry/#updating-existing-dependencies","title":"Updating Existing Dependencies","text":"<p>Keeping Dependencies Up-to-date</p> <p>It's a good practice to keep your dependencies updated to benefit from bug fixes, performance improvements, and new features.</p>"},{"location":"tutorials/poetry/#updating-a-specific-dependency","title":"Updating a Specific Dependency","text":"<p>To update a specific package, use the <code>poetry update</code> command:</p> <pre><code>poetry update &lt;package-name&gt;\n</code></pre> <p>Updating NumPy</p> <p>To update <code>numpy</code> to the latest version, run:</p> <pre><code>poetry update numpy\n</code></pre>"},{"location":"tutorials/poetry/#updating-all-dependencies","title":"Updating All Dependencies","text":"<p>To update all dependencies to their latest versions:</p> <pre><code>poetry update\n</code></pre>"},{"location":"tutorials/poetry/#removing-unneeded-dependencies","title":"Removing Unneeded Dependencies","text":"<p>Clean Up Unnecessary Packages</p> <p>Removing unused dependencies can help keep your project clean and reduce potential security risks.</p> <p>To remove a package, use the <code>poetry remove</code> command:</p> <pre><code>poetry remove &lt;package-name&gt;\n</code></pre> <p>Removing NumPy</p> <p>To remove <code>numpy</code> from your project, run:</p> <pre><code>poetry remove numpy\n</code></pre>"},{"location":"tutorials/poetry/#listing-project-dependencies","title":"Listing Project Dependencies","text":"<p>Know What You're Using</p> <p>It's useful to periodically review the list of installed packages and their versions.</p> <p>To list all installed packages and their versions, run:</p> <pre><code>poetry show\n</code></pre>"},{"location":"tutorials/poetry/#locking-dependencies","title":"Locking Dependencies","text":"<p>Note</p> <p>Ensuring that everyone working on the project uses the same versions of dependencies is crucial for maintaining a consistent development environment. This is where the <code>poetry.lock</code> file comes into play.</p>"},{"location":"tutorials/poetry/#why-use-poetrylock","title":"Why Use <code>poetry.lock</code>?","text":"<p>The <code>poetry.lock</code> file is generated when you first run <code>poetry install</code> or <code>poetry update</code>. It contains the exact versions of each dependency that your project relies on. By committing this file to your version control system (e.g., GitHub), you ensure that all team members and deployment environments use the same versions of dependencies, thereby reducing \"it works on my machine\" issues.</p>"},{"location":"tutorials/poetry/#how-to-lock-dependencies","title":"How to Lock Dependencies","text":"<p>To generate or update the <code>poetry.lock</code> file, you can run:</p> <pre><code>poetry lock\n</code></pre> <p>This will read your <code>pyproject.toml</code> file, resolve dependencies, and generate a new <code>poetry.lock</code> file with the exact package versions.</p>"},{"location":"tutorials/poetry/#committing-to-version-control","title":"Committing to Version Control","text":"<p>To ensure that everyone is on the same page, both <code>pyproject.toml</code> and <code>poetry.lock</code> should be committed to your version control system.</p> <p>What to Commit?</p> <ul> <li> <p>Commit <code>pyproject.toml</code>: This file contains the list of dependencies your project needs, without specifying the exact versions. It provides the flexibility to update packages within defined version ranges.</p> </li> <li> <p>Commit <code>poetry.lock</code>: This file locks the versions of all dependencies, ensuring that everyone uses the same versions. It should be committed to version control to maintain consistency across all environments.</p> </li> </ul> <p>By committing both files, you strike a balance between flexibility and consistency. The <code>pyproject.toml</code> allows for updates within specified ranges, while the <code>poetry.lock</code> ensures that these updates are consistent across all environments.</p> <p>Never Edit <code>poetry.lock</code> Manually</p> <p>The <code>poetry.lock</code> file is auto-generated and should never be edited manually. Always use Poetry commands to update it.</p>"},{"location":"tutorials/poetry/#conclusion","title":"Conclusion","text":"<p>Locking dependencies is an essential practice in collaborative development. It ensures that all contributors and environments are aligned, reducing potential bugs and inconsistencies. With Poetry and the Cookiecutter template, managing these dependencies becomes a straightforward task.</p>"},{"location":"tutorials/poetry/#troubleshooting-and-tips","title":"Troubleshooting and Tips","text":""},{"location":"tutorials/poetry/#resolving-dependency-conflicts","title":"Resolving Dependency Conflicts","text":"<p>If you encounter dependency conflicts, you can use the <code>poetry update --dry-run</code> command to simulate the update process:</p> <pre><code>poetry update --dry-run\n</code></pre>"},{"location":"tutorials/poetry/#debugging-environment-issues","title":"Debugging Environment Issues","text":"<p>If you suspect environment-related issues, you can check which Python interpreter Poetry is using:</p> <pre><code>poetry env info\n</code></pre>"},{"location":"tutorials/poetry/#conclusion_1","title":"Conclusion","text":"<p>Poetry simplifies dependency management, making it easier to maintain a clean and efficient project. With the Cookiecutter template, you have a head start in managing your project's dependencies effectively. Whether you're adding new packages, updating existing ones, or cleaning up unnecessary dependencies, Poetry has got you covered.</p>"},{"location":"tutorials/poetry/#further-reading-and-resources","title":"Further Reading and Resources","text":"<p>To deepen your understanding of Poetry and its capabilities, you may find the following resources helpful:</p> <ul> <li>Real Python's Guide on Dependency Management with Poetry</li> <li>Official Poetry Documentation</li> </ul> <p>These references provide comprehensive tutorials and documentation that can help you become more proficient in managing Python dependencies with Poetry. Whether you're a beginner or an experienced developer, these resources offer valuable insights into best practices and advanced features.</p>"},{"location":"tutorials/ruff-linter/","title":"Ruff: Python Linting for Clean and Compliant Code","text":"<p>ChatGPT Prompt for Google-Style Docstrings</p> <p>A fast and easy way to create Google-style docstrings is by using ChatGPT.  You can provide a simple prompt to request the creation of the docstring  for any Python function.</p> <p>Ruff is a high-performance linter for Python that swiftly evaluates and enhances the quality of your code. With an emphasis on speed and practicality, Ruff assists developers in maintaining consistent coding styles and standards within Python scripts and Jupyter notebooks alike.</p>"},{"location":"tutorials/ruff-linter/#example-prompt-for-a-function","title":"Example Prompt for a Function","text":"<p>Here\u2019s an example prompt you could use with ChatGPT to request a Google-style  docstring following the settings in your <code>.vscode/settings.json</code> file:</p> <p>Prompt:</p> <pre><code>Create a Google-style docstring for the following Python function. \nLimit comments to 72 characters per line and code lines to 79 characters.\n\nFunction:\ndef add_numbers(a, b):\n    result = a + b\n    return result\n\nContext (optional): This function adds two numbers together.\n</code></pre>"},{"location":"tutorials/ruff-linter/#generated-docstring","title":"Generated Docstring","text":"<p>Using the prompt above, ChatGPT will generate a docstring similar to this:</p> <pre><code>def add_numbers(a, b):\n    \"\"\"\n    Adds two numbers together.\n\n    Args:\n        a (int): The first number.\n        b (int): The second number.\n\n    Returns:\n        int: The sum of the two numbers.\n\n    Raises:\n        TypeError: If either 'a' or 'b' is not an integer.\n    \"\"\"\n    result = a + b\n    return result\n</code></pre>"},{"location":"tutorials/ruff-linter/#example-prompt-for-a-class","title":"Example Prompt for a Class","text":"<p>Here\u2019s an example prompt you could use with ChatGPT to request Google-style  docstrings for an entire class, including the module-level docstring,  class-level docstring, and docstrings for each method:</p> <p>Prompt:</p> <pre><code>Create Google-style docstrings for the following Python class. \nLimit comments to 72 characters per line and code lines to 79 characters.\nInclude the module-level docstring at the very top of the script before the imports,\nthe class-level docstring, and the docstrings for each method.\n\nClass:\nclass TextProcessor:\n    def __init__(self, language):\n        if language not in ['en', 'es', 'fr']:\n            raise ValueError(f\"Unsupported language: {language}\")\n        self.language = language\n\n    def clean_text(self, text):\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def tokenize(self, text):\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def analyze_sentiment(self, text):\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\nContext (optional): This class is used for text processing in NLP tasks.\nIt includes methods for cleaning text, tokenizing sentences, and analyzing sentiment.\n</code></pre>"},{"location":"tutorials/ruff-linter/#generated-docstring_1","title":"Generated Docstring","text":"<p>ChatGPT might respond with:</p> <pre><code>\"\"\"\nThis module provides text processing utilities for NLP projects.\n\nThe utilities include functions for text cleaning, tokenization, and \nsentiment analysis.\n\"\"\"\n\nclass TextProcessor:\n    \"\"\"\n    A class used to perform text processing for NLP tasks.\n\n    This class includes methods for cleaning text, tokenizing sentences,\n    and calculating sentiment scores.\n\n    Attributes:\n        language (str): The language of the text to be processed.\n    \"\"\"\n\n    def __init__(self, language):\n        \"\"\"\n        Initializes the TextProcessor with a specified language.\n\n        Args:\n            language (str): The language of the text to be processed.\n\n        Raises:\n            ValueError: If the provided language is not supported.\n        \"\"\"\n        if language not in ['en', 'es', 'fr']:\n            raise ValueError(f\"Unsupported language: {language}\")\n        self.language = language\n\n    def clean_text(self, text):\n        \"\"\"\n        Cleans the input text by removing special characters and extra spaces.\n\n        Args:\n            text (str): The text to be cleaned.\n\n        Returns:\n            str: The cleaned text.\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def tokenize(self, text):\n        \"\"\"\n        Tokenizes the input text into a list of words.\n\n        Args:\n            text (str): The text to be tokenized.\n\n        Returns:\n            list: A list of words (tokens).\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n\n    def analyze_sentiment(self, text):\n        \"\"\"\n        Analyzes the sentiment of the input text.\n\n        Args:\n            text (str): The text to be analyzed.\n\n        Returns:\n            float: The sentiment score of the text.\n\n        Raises:\n            TypeError: If the input text is not a string.\n        \"\"\"\n        if not isinstance(text, str):\n            raise TypeError(\"Input text must be a string\")\n        # Implementation goes here\n        pass\n</code></pre>"},{"location":"tutorials/ruff-linter/#running-ruff","title":"Running Ruff","text":"<p>To check your code with Ruff, run the following command in your terminal:</p> <pre><code>ruff check --preview --fix project_directory/utils/advanced_parser.py\n</code></pre>"},{"location":"tutorials/ruff-linter/#getting-started-with-ruff-check","title":"Getting Started with <code>ruff check</code>","text":"<p>The <code>ruff check</code> command forms the backbone of Ruff, providing a quick and thorough analysis of your Python files to identify and flag potential issues.</p>"},{"location":"tutorials/ruff-linter/#standard-linting-procedure","title":"Standard Linting Procedure","text":"<p>To initiate linting, utilize the following command pattern:</p> <pre><code>ruff check path/to/file_or_directory\n</code></pre> <p>For example, to lint a service component within your application, you might use:</p> <pre><code>ruff check project_directory/services/\n</code></pre>"},{"location":"tutorials/ruff-linter/#automated-error-correction-with-fix","title":"Automated Error Correction with <code>--fix</code>","text":"<p>Ruff's auto-fix capability streamlines the error correction process by automatically resolving numerous common linting issues:</p> <pre><code>ruff check --fix path/to/file_or_directory\n</code></pre> <p>For instance, to apply auto-fixes to an entire application directory:</p> <pre><code>ruff check --fix project_directory/\n</code></pre>"},{"location":"tutorials/ruff-linter/#utilizing-nursery-rules-with-preview","title":"Utilizing Nursery Rules with <code>--preview</code>","text":"<p>To explore beyond standard linting rules and include experimental ones, use the <code>--preview</code> flag:</p> <pre><code>ruff check --preview --fix path/to/file_or_directory\n</code></pre> <p>This is particularly useful when linting individual files that may leverage cutting-edge Python features:</p> <pre><code>ruff check --preview --fix project_directory/utils/advanced_parser.py\n</code></pre>"},{"location":"tutorials/ruff-linter/#understanding-ruffs-rules","title":"Understanding Ruff's Rules","text":"<p>An extensive explanation of Ruff's rules, both standard and nursery, is available at the Ruff Rules Documentation.</p>"},{"location":"tutorials/ruff-linter/#example-linting-a-python-script","title":"Example: Linting a Python Script","text":"<p>Let's say you have a Python script at <code>project_directory/services/parser.py</code>. To lint this script, apply standard and nursery rules, and fix issues:</p> <pre><code>ruff check --preview --fix project_directory/services/parser.py\n</code></pre>"},{"location":"tutorials/ruff-linter/#example-working-with-jupyter-notebooks","title":"Example: Working with Jupyter Notebooks","text":"<p>For a Jupyter notebook, <code>analysis.ipynb</code>, located in the same directory:</p> <pre><code>ruff check --preview --fix project_directory/notebooks/analysis.ipynb\n</code></pre> <p>Ruff will lint the notebook and provide feedback or fixes in a format compatible with Jupyter's code cells.</p>"},{"location":"tutorials/ruff-linter/#real-world-example-and-terminal-output","title":"Real-World Example and Terminal Output","text":"<p>Here's a real-world example showcasing the execution of Ruff and the corresponding terminal output:</p> <pre><code># Non-compliant code with E201\ndef my_function(): # Violates \"undocumented-public-function\"\n    my_list = [1, 2, 3, 4 ] # Violates \"whitespace-after-open-bracket\" and \"whitespace-before-close-bracket\"\n    long_string = \"This is a very long string...\" # Violates \"line-too-long\"\n</code></pre> <p>Assuming you're working on <code>project_directory/models/data_model.py</code> with some issues, you run:</p> <pre><code>ruff check --preview --fix project_directory/models/data_model.py\n</code></pre> <p>The terminal output might look like this:</p> <pre><code>project_directory/models/data_model.py:1:1: D100 Missing docstring in public module\nproject_directory/models/data_model.py:5:10: E231 missing whitespace after ','\nproject_directory/models/data_model.py:7:1: E302 expected 2 blank lines, found 1\nFound 5 errors (3 fixed, 2 remaining).\n</code></pre> <p>Each line of the output directs you to the specific issue within your code, providing guidance on the nature of the problem and its location.</p> <p>Quick Code Navigation</p> <p>In integrated development environments like VS Code, you can use the command-click (or control-click on some systems) feature on the path in the terminal output to jump directly to the problematic line, facilitating quick fixes.</p> <p>Handling Nursery Rule Warnings</p> <p>If you encounter a warning like:</p> <pre><code>warning: Selection of nursery rule `E201` without the `--preview` flag is deprecated.\n</code></pre> <p>This indicates that your <code>pyproject.toml</code> includes nursery rules, which are experimental and require the <code>--preview</code> flag during execution. In our configuration, where we utilize a set of 50 rules that encompasses several nursery rules, it's essential to always use the <code>--preview</code> flag. This ensures that Ruff considers all specified rules, including the experimental ones, to maintain the integrity of your code linting strategy.</p> <p>Always run Ruff with the <code>--preview</code> flag to avoid deprecation warnings and adhere to the complete set of rules defined for your project:</p> <pre><code>ruff check --preview path/to/file_or_directory\n</code></pre> <p>By doing so, you'll leverage the full capabilities of Ruff, keeping your codebase up to date with both standard and progressive linting practices.</p>"},{"location":"tutorials/ruff-linter/#advanced-integration-of-ruff-in-python-development","title":"Advanced Integration of Ruff in Python Development","text":"<p>Continuing from the foundational <code>ruff check</code> usage, let's delve into how Ruff can be integrated into more advanced development workflows. This includes setting up pre-commit hooks, utilizing GitHub Actions for continuous integration, and customizing Ruff to adhere to a specific set of rules defined in a project's <code>pyproject.toml</code>.</p>"},{"location":"tutorials/ruff-linter/#pre-commit-hooks-for-ruff","title":"Pre-commit Hooks for Ruff","text":"<p>Pre-commit hooks are a powerful way to ensure that code is automatically linted before it's committed to your repository, helping to maintain code quality and consistency.</p>"},{"location":"tutorials/ruff-linter/#setting-up-pre-commit-hooks","title":"Setting Up Pre-commit Hooks","text":"<p>To utilize Ruff as a pre-commit hook, you'll need to add a configuration to your <code>.pre-commit-config.yaml</code> file:</p> <pre><code>repos:\n-   repo: https://github.com/charliermarsh/ruff\n    rev: ''  # Use the tag for the desired release\n    hooks:\n    - id: ruff\n</code></pre> <p>This configuration will instruct the pre-commit tool to run Ruff against staged files whenever <code>git commit</code> is executed.</p>"},{"location":"tutorials/ruff-linter/#example-pre-commit-hook-in-action","title":"Example: Pre-commit Hook in Action","text":"<p>When you attempt to commit, the hook will run, and you might see output like this if issues are found:</p> <pre><code>pre-commit running: ruff\nproject_directory/services/parser.py:10:1: E302 expected 2 blank lines, found 1\n\nFailed. Ruff found linting errors. Review the issues and commit again.\n</code></pre>"},{"location":"tutorials/ruff-linter/#github-actions-for-continuous-linting","title":"GitHub Actions for Continuous Linting","text":"<p>GitHub Actions can be set up to run Ruff on every push or pull request to ensure that all contributions meet your coding standards.</p>"},{"location":"tutorials/ruff-linter/#github-actions-integration-with-ruff","title":"GitHub Actions Integration with Ruff","text":"<p>To ensure the codebase maintains high-quality standards and adheres to our defined linting rules, we use GitHub Actions to automate the linting process. This section provides an up-to-date guide on how Ruff is integrated within our CI/CD pipeline.</p>"},{"location":"tutorials/ruff-linter/#github-actions-workflow-for-ruff-linting","title":"GitHub Actions Workflow for Ruff Linting","text":"<p>Our GitHub Actions workflow is configured to run Ruff on every push and pull request. The action checks the code against a set of 50 rules, including several nursery rules, ensuring all contributions are up to the linting standards before merging into the main branch.</p>"},{"location":"tutorials/ruff-linter/#workflow-setup","title":"Workflow Setup","text":"<p>The workflow is defined in the <code>.github/workflows/ruff.yaml</code> file and consists of the following steps:</p> <ol> <li>Checkout Code: Retrieves the code from the current repository    branch that triggered the action.</li> <li>Set up Python: Prepares the Python environment using the version    specified, ensuring compatibility with Ruff.</li> <li>Install Ruff: Installs Ruff, making it available for linting.</li> <li>Check Code with Ruff: Runs Ruff against the specified directories    (<code>src/</code> and <code>notebooks/</code>) using the <code>--preview</code> flag to include    nursery rules from our <code>pyproject.toml</code>.</li> <li>Fail if Linting Errors Are Detected: If Ruff detects any linting    errors, the action outputs an error message and fails, prompting the    user to lint their code locally and resolve the issues before pushing    to the repository.</li> </ol>"},{"location":"tutorials/ruff-linter/#ruff-linting-action-definition","title":"Ruff Linting Action Definition","text":"<p>Here is the code snippet for the GitHub Actions workflow:</p> <pre><code>name: Ruff Linter\n\non: [push, pull_request]\n\njobs:\n  lint:\n    runs-on: ubuntu-latest\n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v2\n\n    - name: Set up Python\n      uses: actions/setup-python@v2\n      with:\n        python-version: 3.9\n\n    - name: Install Ruff\n      run: pip install ruff\n\n    - name: Check code with Ruff\n      run: ruff check --preview src/ notebooks/\n\n\n    - name: Fail if linting errors are detected\n      run: |\n        if ruff check --preview src/ notebooks/\n          --quiet | grep -q 'error'; then\n          echo \"Linting errors detected. Please lint your code locally with Ruff and resolve all issues before pushing.\"\n          exit 1\n        else\n          echo \"No linting errors detected.\"\n        fi\n</code></pre> <p>With this setup, we do not automatically fix any issues with Ruff in the GitHub Actions workflow. Instead, we enforce that the code must be linted locally by the developer. This approach ensures that all team members are actively involved in maintaining the code quality and are aware of the coding standards enforced by our Ruff configuration.</p> <p>Branch Protection with Ruff</p> <p>To ensure code quality, we need to protect the main branch with branch protection rules. This setup requires that all code must pass Ruff linting checks before being merged into the main branch. While you can push code to your feature branches even if it fails linting, merging into main is blocked until all linting errors are resolved. This approach mimics pre-commit hooks, maintaining a clean and compliant main branch. Developers should run Ruff locally and fix issues before pushing code for review.</p>"},{"location":"tutorials/ruff-linter/#example-github-action-log-for-linting-errors","title":"Example: GitHub Action Log for Linting Errors","text":"<p>When a developer pushes code or creates a pull request, the GitHub Action is triggered to ensure the code meets our linting standards. If Ruff detects linting issues, the action will fail, and the output in the GitHub Actions log will resemble the following:</p> <pre><code>Run ruff check --preview src/ notebooks/\nLinting errors detected. Please lint your code locally with Ruff and resolve all issues before pushing.\n\nproject_directory/models/data_model.py:1:1: D100 Missing docstring in public module\nproject_directory/services/parser.py:5:10: E231 missing whitespace after ','\nproject_directory/utils/calculations.py:7:1: E302 expected 2 blank lines, found 1\n...\n\nError: Process completed with exit code 1.\n</code></pre> <p>This log provides detailed feedback about the linting errors found. The developer is expected to address these issues by running Ruff locally with the <code>--preview</code> flag, ensuring that the code is compliant with our established set of rules, including nursery rules, before attempting to push again.</p>"},{"location":"tutorials/ruff-linter/#customizing-ruff-with-pyprojecttoml","title":"Customizing Ruff with <code>pyproject.toml</code>","text":"<p>Ruff allows for extensive customization of its linting rules through a <code>pyproject.toml</code> file. This enables you to specify which rules to enable, disable, or configure further.</p>"},{"location":"tutorials/ruff-linter/#commentary-on-pyprojecttoml-configuration","title":"Commentary on <code>pyproject.toml</code> Configuration","text":"<pre><code># pyproject.toml\n[tool.ruff]\n# We're using a curated set of 50 rules, which you can review and adjust as needed\ninclude = [ ... ]\nexclude = [ ... ]\n</code></pre> <p>The <code>include</code> and <code>exclude</code> keys allow you to precisely control which rules Ruff will apply or ignore. This ensures that the linter is aligned with your project's standards and practices.</p>"},{"location":"tutorials/ruff-linter/#conclusion","title":"Conclusion","text":"<p>By integrating Ruff with pre-commit hooks and GitHub Actions, and customizing its behavior with <code>pyproject.toml</code>, you can automate and fine-tune the linting process to fit your project's needs. This ensures high-quality, clean code that adheres to your defined standards, ultimately leading to a more robust and maintainable codebase.</p> <p>Remember, the set of rules within <code>pyproject.toml</code> is at the core of Ruff's operation within your project, and you should review and adapt these rules to align with your coding principles.</p>"},{"location":"tutorials/ruff-linter/#further-resources","title":"Further Resources","text":"<p>For more detailed information on using Ruff, customizing rules, and integrating with your development environment, please refer to the following:</p> <ul> <li>Ruff Official Documentation</li> <li>Ruff Rules Documentation</li> </ul> <p>Leverage these resources to fully harness the capabilities of Ruff for your Python projects.</p> <p>Selected rules:</p> Code Rule Description Documentation Link F401 Unused imports F401 F402 Import shadowed by loop var F402 F403 <code>from module import *</code> used F403 F405 Name may be undefined, or defined from star imports F405 F601 Dictionary key literal repeated F601 F602 Dictionary key variable repeated F602 F621 Too many expressions in star-unpacking assignment F621 F631 Assert test is a non-empty tuple F631 F632 Use <code>==</code> to compare constant literals F632 F701 <code>break</code> outside loop F701 F702 <code>continue</code> not properly in loop F702 F704 Yield statement outside of a function F704 F706 <code>return</code> statement outside of a function/method F706 F707 <code>except</code> block not the last exception handler F707 F722 Syntax error in forward annotation F722 F811 Redefinition of unused variable from line F811 F821 Undefined name F821 F841 Local variable is assigned to but never used F841 E101 Indentation contains mixed spaces and tabs E101 E111 Indentation is not a multiple of four E111 E112 Expected an indented block E112 E113 Unexpected indentation E113 E114 Indentation is not a multiple of four (comment) E114 E115 Expected an indented block (comment) E115 E116 Unexpected indentation (comment) E116 E117 Over-indented (comment) E117 E201 Whitespace after '(' or '[' E201 E202 Whitespace before ')' or ']' E202 E203 Whitespace before ':' E203 E211 Whitespace before '(' or '[' E211 E225 Missing whitespace around operator E225 E231 Missing whitespace after ',', ';', or ':' E231 E251 Unexpected spaces around keyword / parameter equals E251 E261 At least two spaces before inline comment E261 E262 Inline comment should start with '# ' E262 E265 Block comment should start with '# ' E265 E266 Too many leading '#' for block comment E266 E271 Multiple spaces after keyword E271 E272 Multiple spaces before keyword E272 E273 Tab after keyword E273 E274 Tab before keyword E274 E275 Missing whitespace after keyword E275 E401 Multiple imports on one line E401 E402 Module level import not at top of file E402 E501 Line too long E501 E711 Comparison to None should be 'expr is None' E711 E712 Comparison to True should be 'if cond is True:' or 'if cond:' E712 E713 Test for membership should be 'not in' E713 E714 Test for object identity should be 'is not' E714 E721 Do not compare types, use 'isinstance()' E721 E722 Do not use bare 'except' E722 E731 Do not assign a lambda expression, use a def E731 I001 Import block is un-sorted or un-formatted I001 I002 Missing required import I002 N801 Class name should use CapWords convention N801 N802 Function name should be lowercase N802 N803 Argument name should be lowercase N803 N804 First argument of a class method should be named 'cls' N804 N805 First argument of a method should be named 'self' N805 N806 Variable in function should be lowercase N806 D100 Missing docstring in public module D100 D101 Missing docstring in public class D101 D102 Missing docstring in public method D102 D103 Missing docstring in public function D103 D104 Missing docstring in public package D104 D105 Missing docstring in magic method D105 D106 Missing docstring in public nested class D106 D107 Missing docstring in init D107"},{"location":"tutorials/toc-tutorials/","title":"Tutorials","text":"<p>About This Section</p> <p>The Tutorials section provides a comprehensive learning path for various tools and methodologies integral to software development and data science, with a special emphasis on Python environments. The tutorials are meticulously curated to impart hands-on knowledge that will bolster your ability to improve code quality, streamline dependency management, document projects efficiently, and master configuration management, among other key skills. They cater to both the initiation of new projects and the enhancement of established workflows, offering a wealth of insights for developers at every level of expertise.</p>"},{"location":"tutorials/toc-tutorials/#purpose","title":"Purpose","text":"<ul> <li> Focused Guidance: Each tutorial zeroes in on particular development challenges, offering pragmatic solutions and tips.</li> <li> Broad Spectrum of Topics: The range spans from foundational setup to sophisticated features and tools.</li> <li> Inclusive Learning: Tailored for novices and seasoned developers alike, the content is structured to be accessible and informative for all.</li> </ul>"},{"location":"tutorials/toc-tutorials/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Formatting Your Code with Black</li> <li>Accelerating Linting with Ruff</li> <li>Poetry: Mastering Dependency Management</li> <li>Crafting Documentation with MkDocs</li> <li>Configuration Management via Hydra</li> <li>Crafting an Effective .gitignore File</li> <li>Data Versioning with DVC</li> <li>Local Data Versioning with DVC</li> <li>Leveraging Cloud Storage with DVC</li> <li>Collaborating with DVC</li> <li>Markdown Documentation for ML Models</li> <li>Introduction to Naming Conventions for AI Project Assets</li> <li>Windows OS: Setting Up Your Development Environment</li> <li>Data Management in ML Projects</li> <li>Using Code Tags</li> <li>Using TODO Tree with Code Tags</li> <li>Python OOP for Machine Learning Projects</li> <li>Sharing VS Code Settings for Python Projects</li> </ul> <p>Dive into each tutorial for detailed, step-by-step guidance that you can follow along with ease. Start exploring the topics that interest you the most.</p>"},{"location":"tutorials/vscode-settings/","title":"Sharing VS Code Settings for Python Projects","text":""},{"location":"tutorials/vscode-settings/#introduction","title":"Introduction","text":"<p>Setting norms for developer tasks on a team project is crucial for maintaining code consistency and readability. Visual Studio Code (VS Code) allows us to manage distinct user and workspace settings. This flexibility ensures that while each developer can maintain their custom settings, the team adheres to the shared settings that ensure consistency across the project, especially when working with Python.</p>"},{"location":"tutorials/vscode-settings/#how-we-share-vs-code-settings-across-python-projects","title":"How We Share VS Code Settings Across Python Projects","text":""},{"location":"tutorials/vscode-settings/#workspace-settings","title":"Workspace Settings","text":""},{"location":"tutorials/vscode-settings/#remote-development-with-vs-code","title":"Remote Development with VS Code","text":"<p>Info</p> <p>When developing on remote servers, such as AWS, you will encounter a \"Remote\" configuration in your VS Code settings, similar to the workspace settings you use locally. This \"Remote\" configuration serves the same purpose, allowing you to enforce consistent coding standards and best practices across your team. Despite the different context, the functionality remains identical, ensuring a seamless and efficient development experience regardless of your code's location.</p> <p>Workspace settings in VS Code are stored in a <code>.vscode/settings.json</code> file within the project directory. These settings are shared among all team members and include configurations essential for maintaining coding standards and best practices in Python. Workspace settings override user settings.</p>"},{"location":"tutorials/vscode-settings/#example-of-vscodesettingsjson","title":"Example of <code>.vscode/settings.json</code>","text":"<pre><code>{\n  \"python.pythonPath\": \"venv/bin/python\",\n  \"python.linting.enabled\": true,\n  \"python.linting.pylintEnabled\": true,\n  \"python.linting.flake8Enabled\": true,\n  \"python.formatting.provider\": \"black\",\n  \"editor.formatOnSave\": true,\n  \"files.exclude\": {\n    \"**/__pycache__\": true,\n    \"**/*.pyc\": true\n  }\n}\n</code></pre> <p>In the example above, the settings ensure: - The correct Python interpreter is used. - Linting is enabled using Pylint and Flake8. - Code is formatted using Black on save. - Certain files and directories (like <code>__pycache__</code> and <code>.pyc</code> files)   are excluded from the file explorer.</p>"},{"location":"tutorials/vscode-settings/#location-and-management","title":"Location and Management","text":"<ul> <li>The workspace settings file is located under the <code>.vscode</code> folder in   your project\u2019s root directory.</li> <li>You can edit the settings directly via the Preferences: Open Workspace   Settings (JSON) command in the Command Palette.</li> </ul>"},{"location":"tutorials/vscode-settings/#user-settings","title":"User Settings","text":"<p>User settings in VS Code are specific to an individual developer's environment and are stored outside the project directory. These settings allow each developer to customize their workspace without affecting the shared project settings.</p>"},{"location":"tutorials/vscode-settings/#how-to-access-user-settings","title":"How to Access User Settings","text":"<ol> <li>Open VS Code.</li> <li>Navigate to <code>File</code> &gt; <code>Preferences</code> &gt; <code>Settings</code> (or use the shortcut    <code>Ctrl + ,</code> on Windows/Linux or <code>Cmd + ,</code> on Mac).</li> <li>In the Settings tab, ensure you are in the <code>User</code> settings mode.</li> </ol>"},{"location":"tutorials/vscode-settings/#example-of-customizing-user-settings","title":"Example of Customizing User Settings","text":"<pre><code>{\n  \"workbench.colorTheme\": \"Visual Studio Dark\",\n  \"editor.fontSize\": 14,\n  \"editor.fontFamily\": \"Fira Code, monospace\"\n}\n</code></pre> <p>In the example above, the settings customize:</p> <ul> <li>The color theme of the editor.</li> <li>The font size and font family for the editor.</li> </ul>"},{"location":"tutorials/vscode-settings/#settings-precedence","title":"Settings Precedence","text":"<p>VS Code allows configurations to be overridden at multiple levels. The settings precedence determines which configuration will ultimately apply when there are conflicts. Here is the order of precedence:</p> <ol> <li>Default settings: The default, unconfigured setting values.</li> <li>User settings: Apply globally to all VS Code instances.</li> <li>Remote settings: Apply to a remote machine opened by a user.</li> <li>Workspace settings: Apply to the open folder or workspace.</li> <li>Workspace Folder settings: Apply to a specific folder of a    multi-root workspace.</li> <li>Language-specific default settings: Default values specific to a    language, contributed by extensions.</li> <li>Language-specific user settings: User settings, but specific to a    language.</li> <li>Language-specific remote settings: Remote settings, but specific    to a language.</li> <li>Language-specific workspace settings: Workspace settings, but    specific to a language.</li> <li>Language-specific workspace folder settings: Workspace folder     settings, but specific to a language.</li> <li>Policy settings: Set by the system administrator and always     override other setting values.</li> </ol> <p>From this list, it is clear that workspace settings override user settings.</p>"},{"location":"tutorials/vscode-settings/#practical-example","title":"Practical Example","text":"<p>If a setting is configured at multiple levels, the most specific scope will apply. For instance:</p> <ul> <li>User settings (<code>settings.json</code> in the user profile):</li> </ul> <pre><code>{\n  \"editor.formatOnSave\": false,\n  \"editor.fontSize\": 14\n}\n</code></pre> <ul> <li>Workspace settings (<code>.vscode/settings.json</code> in the project):</li> </ul> <pre><code>{\n  \"editor.formatOnSave\": true,\n  \"editor.fontSize\": 16\n}\n</code></pre> <p>In this scenario:</p> <ul> <li>The editor.formatOnSave setting will be <code>true</code> for the workspace,   overriding the user's global setting of <code>false</code>.</li> <li>The editor.fontSize will be <code>16</code> for the workspace, overriding the   user's global setting of <code>14</code>.</li> </ul>"},{"location":"tutorials/vscode-settings/#implications-for-our-project","title":"Implications for Our Project","text":"<p>Given the settings precedence, our project uses the <code>.vscode/settings.json</code> file to enforce consistent coding standards. This file falls under workspace settings, which override user settings.</p>"},{"location":"tutorials/vscode-settings/#steps-to-follow","title":"Steps to Follow","text":"<ol> <li>Clone the Repository: Ensure you clone the repository that    includes the <code>.vscode/settings.json</code> file.</li> </ol> <pre><code>git clone https://github.com/your-repo.git\n</code></pre> <ol> <li> <p>Do Not Ignore <code>.vscode/settings.json</code>: Make sure the    <code>.vscode/settings.json</code> file is not included in the <code>.gitignore</code> and    is shared across the team via GitHub.</p> </li> <li> <p>Customize User Settings: Personalize your VS Code experience by    adding your custom settings to the user settings file without    altering the workspace settings.</p> </li> </ol>"},{"location":"tutorials/vscode-settings/#conclusion","title":"Conclusion","text":"<p>Using distinct user and workspace settings in VS Code allows us to balance personal preferences with team-wide coding standards. By following this approach, we maintain code consistency and readability across the project while providing the flexibility for individual customization.</p> <p>This approach helps maintain code consistency and readability across the team, ensuring that everyone adheres to the same standards while allowing for personal customization where it doesn't impact the project's coding practices.</p>"},{"location":"tutorials/vscode-settings/#references","title":"References","text":"<ul> <li>Sharing VS Code Extensions Across   Teams</li> <li>VS Code Workspace Settings   Guide</li> <li>VS Code Documentation:   Settings</li> <li>Video: Sharing VS Code   Settings</li> </ul>"},{"location":"tutorials/windows-os-setup/","title":"Setting Up a Python Environment on Windows OS","text":"<p>Welcome to the setup guide for creating a Python environment on a Windows OS machine without admin rights and behind a firewall. This guide will walk you through all the prerequisites and steps required to set up a Python development environment, ensuring you can work seamlessly even with limited permissions on your Windows machine.</p> <p>In the following sections, you will find instructions on installing essential tools like Visual Studio Code, Anaconda, and Git Bash without requiring administrative privileges. We'll also provide guidance on configuring your environment behind a firewall, ensuring a smooth setup process for your project.</p> <p>Follow these steps carefully to get your Python environment up and running, and you'll be well-equipped to dive into your project. Let's get started!</p>"},{"location":"tutorials/windows-os-setup/#instructions","title":"Instructions","text":"<p>Note</p> <p>Before you can get started with this project, you'll need to have the following software installed on your Windows machine.</p>"},{"location":"tutorials/windows-os-setup/#prerequisites","title":"Prerequisites","text":"<p>Visual Studio Code (VS Code):</p> <ul> <li>Download and install VS Code by following the instructions on the   official website: VS Code   Download</li> <li>During the installation process, choose an installation location   within a directory where you have write permissions, typically   within your user directory, <code>C:/Users/[Username]/</code>. You should not   encounter any admin rights issues during the installation of VS   Code.</li> </ul> <p>Anaconda Distribution:</p> <ul> <li>Download the Anaconda Distribution for Windows from the official   website: Anaconda   Download</li> <li>Follow these steps to install Anaconda without admin rights:<ul> <li>During the Anaconda installation process, when you reach the   \"Advanced Options\" section, select the \"Install for me only\"   option.</li> <li>Choose a directory location where you have write permissions   (e.g., your user directory) for the installation.</li> <li>Complete the installation process.</li> </ul> </li> </ul> <p>Git Bash:</p> <ul> <li>Download and install Git Bash, which provides a Unix-like   command-line environment for Windows: Git Bash   Download</li> <li>During the installation, choose an installation location within a   directory where you have write permissions (e.g., your user   directory). You should not encounter any admin rights issues when   installing Git Bash.</li> </ul> <p>Once you have installed these prerequisites, you'll be ready to set up and run this project on your Windows machine, even without admin rights.</p>"},{"location":"tutorials/windows-os-setup/#install-recommended-vs-code-extensions","title":"Install Recommended VS Code Extensions","text":"<p>To ensure a consistent development environment and take advantage of tools and configurations tailored for this project, it's advisable to install the extensions specified in the <code>.vscode/extensions.json</code> file.</p> <p>Follow the steps below:</p> <ol> <li>Start by opening the project in Visual Studio Code.</li> <li>Access the Extensions panel:<ul> <li>Click the square icon on the sidebar</li> <li>Or press <code>Ctrl+Shift+X</code></li> </ul> </li> <li>Once in the Extensions panel:<ul> <li>Type <code>@recommended</code> in the search bar. This filters and displays   the list of recommended extensions for the project.</li> </ul> </li> <li>Install the extensions: For every listed recommended extension,    press the <code>Install</code> button to incorporate it into your VS Code.</li> </ol>"},{"location":"tutorials/windows-os-setup/#enable-the-usage-of-conda-commands-from-the-git-bash-terminal-in-vs-code","title":"Enable the Usage of Conda Commands from the Git Bash Terminal in VS Code","text":"<p>Follow the steps below to get started:</p>"},{"location":"tutorials/windows-os-setup/#1-identify-anaconda-installation-path","title":"1. Identify Anaconda Installation Path","text":"<ul> <li>Open the <code>Anaconda Prompt</code> from the Start menu.</li> <li>Type the command below and press <code>Enter</code>:</li> </ul> Anaconda Prompt<pre><code>conda info\n</code></pre> <ul> <li>Look for the line starting with <code>base environment :</code>. This displays   the path to your Anaconda installation.</li> </ul>"},{"location":"tutorials/windows-os-setup/#2-activate-conda-in-git-bash","title":"2. Activate Conda in Git Bash","text":"<ul> <li>Start <code>VS Code</code>.</li> <li>Open a new terminal, making sure it's <code>Git Bash</code>.</li> <li>Run the following command, substituting the path with your specific   Anaconda installation path:</li> </ul> Bash<pre><code>. C:/Users/[Username]/Anaconda3/etc/profile.d/conda.sh\n</code></pre>"},{"location":"tutorials/windows-os-setup/#3-automate-conda-activation","title":"3. Automate Conda Activation","text":"<p>To ensure Conda activates automatically every time you launch the integrated Git Bash terminal in VS Code:</p>"},{"location":"tutorials/windows-os-setup/#determine-bashrc-and-bash_profile-location","title":"Determine <code>.bashrc</code> and <code>.bash_profile</code> location","text":"<p>In the integrated Git Bash terminal of VS Code, run:</p> Bash<pre><code>echo $HOME\n</code></pre> <p>Note</p> <p><code>.bashrc</code> and <code>.bash_profile</code> might be hidden. Turn on the 'view hidden files' option in File Explorer.</p>"},{"location":"tutorials/windows-os-setup/#if-these-files-are-absent-in-your-home-directory","title":"If these files are absent in your home directory:","text":"<p>Info</p> <p>In Windows OS, the home directory for a user is typically located at <code>C:/Users/[Username]/</code>, where <code>[Username]</code> is the name of the user account. When using tools that are Unix-based or Unix-like (e.g., Git Bash), the concept of the \"home\" directory often maps to this path.</p> <p>When you're in Git Bash, for instance, referencing <code>~</code> (the tilde symbol) will typically point to this directory. So if you run a command like <code>cd ~</code> in Git Bash, it would take you to <code>C:/Users/[Username]/</code>.</p>"},{"location":"tutorials/windows-os-setup/#a-open-the-integrated-git-bash-terminal-in-vs-code","title":"a. Open the integrated Git Bash terminal in VS Code","text":"<p>You can do this by selecting the Git Bash option from the terminal dropdown in VS Code or by setting it as the default terminal.</p>"},{"location":"tutorials/windows-os-setup/#b-create-the-bashrc-file","title":"b. Create the <code>.bashrc</code> File","text":"<p>Enter this command to generate a <code>.bashrc</code> file in your home directory:</p> Bash<pre><code>touch ~/.bashrc\n</code></pre> <p>This <code>touch</code> command produces an empty <code>.bashrc</code> file if it's missing.</p>"},{"location":"tutorials/windows-os-setup/#c-create-the-bash_profile-file","title":"c. Create the <code>.bash_profile</code> File","text":"<p>To create the <code>.bash_profile</code> file, use:</p> Bash<pre><code>touch ~/.bash_profile\n</code></pre>"},{"location":"tutorials/windows-os-setup/#d-edit-the-bash_profile-file","title":"d. Edit the <code>.bash_profile</code> File","text":"<p>To edit the file using VS Code's built-in editor, run:</p> Bash<pre><code>code ~/.bash_profile\n</code></pre> <p>Once opened in VS Code, incorporate these lines:</p> Bash<pre><code>if [ -f ~/.bashrc ]; then\n  source ~/.bashrc\nfi\n</code></pre> <p>Save and close the file.</p>"},{"location":"tutorials/windows-os-setup/#e-edit-the-bashrc-file","title":"e. Edit the <code>.bashrc</code> File","text":"<p>To edit the <code>.bashrc</code> file in VS Code, use:</p> Bash<pre><code>code ~/.bashrc\n</code></pre> <p>Then, append:</p> Bash<pre><code>. C:/Users/[Username]/Anaconda3/etc/profile.d/conda.sh\n</code></pre> <p>Remember to replace <code>[Username]</code> with your actual username. Save and close the file once done.</p>"},{"location":"tutorials/windows-os-setup/#f-apply-the-changes","title":"f. Apply the Changes","text":"<p>Apply your changes by either restarting the integrated Git Bash terminal in VS Code or by sourcing the <code>.bash_profile</code> file:</p> Bash<pre><code>source ~/.bash_profile\n</code></pre> <p>Every time you initiate a new Bash session within VS Code, the <code>.bashrc</code> file gets sourced automatically due to the <code>.bash_profile</code> configuration. This setup loads the Conda configuration in <code>.bashrc</code>, enabling the use of Conda commands.</p>"},{"location":"tutorials/windows-os-setup/#4-verify-conda-activation","title":"4. Verify Conda Activation","text":"<ul> <li>Shut and reopen the Git Bash terminal in <code>VS Code</code>.</li> <li>Enter <code>conda activate</code> and hit <code>Enter</code>. If <code>(base)</code> appears in the   terminal, the base Conda environment is active.</li> </ul> <p>With these steps complete, you can now utilize Conda commands from the Git Bash terminal in VS Code on Windows. Whenever you launch a new Git Bash terminal in VS Code, Conda activates automatically, streamlining your Conda environment and package management tasks.</p>"},{"location":"tutorials/windows-os-setup/#install-linux-tools-on-git-bash","title":"Install Linux Tools on Git Bash","text":"<p>When you lack admin rights on a Windows machine, system-wide software installations become tricky. Yet, for your local environment, like Git Bash, there's a way. For instance, to install the <code>make</code> tool, follow the steps below.</p>"},{"location":"tutorials/windows-os-setup/#what-is-make","title":"What is <code>make</code>?","text":"<p><code>make</code> is a utility controlling the creation of executables and other non-source program files.</p>"},{"location":"tutorials/windows-os-setup/#steps-to-add-make-to-git-bash","title":"Steps to Add <code>make</code> to Git Bash","text":""},{"location":"tutorials/windows-os-setup/#1-download-the-tool","title":"1. Download the Tool","text":"<p>Head over to ezwinports and get <code>make-4.4.1-without-guile-w32-bin.zip</code>.</p> <p>Reference</p> <p>Reference: Check this article for a more comprehensive setup guide.</p>"},{"location":"tutorials/windows-os-setup/#2-extract-the-download","title":"2. Extract the Download","text":"<p>Unpack the contents to, say, <code>C:\\Users\\[Username]\\Tools\\make-4.4.1-without-guile-w32-bin</code>.</p>"},{"location":"tutorials/windows-os-setup/#3-update-the-path-in-git-bash","title":"3. Update the <code>$PATH</code> in Git Bash","text":"<p>Run the following commands:</p> Bash<pre><code>echo 'export PATH=$PATH:C:\\\\Users\\\\[Username]\\\\Tools\\\\make-4.4.1-without-guile-w32-bin\\\\bin' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre> <p>This is how your <code>.bashrc</code> profile should now appear:</p> Bash<pre><code>. C:/Users/[Username]/Anaconda3/etc/profile.d/conda.sh\nexport PATH=$PATH:C:\\\\Users\\\\[Username]\\\\Tools\\\\make-4.4.1-without-guile-w32-bin\\\\bin\n</code></pre>"},{"location":"tutorials/windows-os-setup/#4-verify-installation","title":"4. Verify Installation","text":"<p>At this point, you should be able to run <code>make</code> commands from Git Bash.</p> <p>Note</p> <p>Tools added this way are localized to your Git Bash environment for your user profile. They won't be accessible in other command-line interfaces unless their paths receive similar adjustments.</p> <p>Some tools might come with dependencies. Make it a habit to peruse documentation or README files to guarantee all requisite components are in place.</p>"},{"location":"tutorials/windows-os-setup/#install-poetry-for-dependency-management","title":"Install Poetry for Dependency Management","text":"<p>Here are the steps to install Poetry in the NYL Windows machine without admin access and firewall.</p>"},{"location":"tutorials/windows-os-setup/#disconnect-vpn","title":"Disconnect VPN","text":"<p>Disconnect your VPN is it is connected.</p>"},{"location":"tutorials/windows-os-setup/#update-conda","title":"Update Conda","text":"<p>Open the Anaconda PowerShell Prompt and update your Conda installation:</p> Anaconda Prompt<pre><code>conda update conda\n</code></pre>"},{"location":"tutorials/windows-os-setup/#install-poetry-for-package-management","title":"Install Poetry for Package Management","text":"<p>Following the Poetry's docs, let's install Poetry with <code>pipx</code>. First install <code>pipx</code>:</p> Anaconda Prompt<pre><code>pip install pipx\n</code></pre> <p>Install Poetry using <code>pipx</code>:</p> Anaconda Prompt<pre><code>pipx install poetry\n</code></pre> <p>Output:</p> Anaconda Prompt<pre><code>installed package poetry 1.6.1, installed using Python 3.9.18\n\nThese apps are now globally available\n\n- poetry.exe\n\n\u26a0\ufe0f Note: 'C:\\\\Users\\\\[Username]\\\\.local\\\\bin' is not on your PATH environment variable. These apps will not be globally accessible until your PATH is\nupdated. Run pipx ensurepath to automatically add it, or manually modify your PATH in your shell's config file (i.e. ~/.bashrc).\n\ndone! \u2728 \ud83c\udf1f \u2728\n</code></pre> <p>It looks like Poetry was installed successfully using Pipx, but the <code>.local/bin</code>folder where the poetry executable is installed is not in your PATH environment variable.</p> <p>Since you don't have admin access to modify the PATH globally, you need to update it just for your user account.</p> <p>Since we are using <code>.bashrc</code> and <code>.bash_profile</code> files for our shell configuration on Windows, here is how you can automatically add Poetry's path to our <code>PATH</code>. Add the following line to your <code>.bash_profile</code>:</p> Bash<pre><code>export PATH=$PATH:C:\\\\Users\\\\[Username]\\\\.local\\\\bin\n</code></pre> <p>Your <code>.bashrc</code> must looks like this:</p> Bash<pre><code># Add Anaconda to PATH\n# Sourcing this script initializes Conda and adds it to the PATH\n# This gives access to Conda, Python and any packages installed in the default Conda env\n# This give access to Conda command from Git Bash integrated in VS Code\n. C:/Users/[Username]/Anaconda3/etc/profile.d/conda.sh\n# Add Make to PATH\n# This adds the Make (GNU make) bin folder containing make.exe to the PATH\n# Now Make can be run from any directory in the Git Bash terminal integrated in VS Code\nexport PATH=$PATH:C:\\\\Users\\\\[Username]\\\\Tools\\\\make-4.4.1-without-guile-w32-bin\\\\bin\n# Add Poetry to PATH\n# Poetry is used for Python dependency and package management\n# It was installed using Pipx into the ~/.local/bin folder\n# Adding this folder to PATH makes the poetry command globally available\n# Now Make can be run from any directory in the Git Bash terminal integrated in VS Code\nexport PATH=$PATH:C:\\\\Users\\\\[Username]\\\\.local\\\\bin\n# General notes:\n# - Use single quotes for Windows paths\n# - Escape spaces and special characters in paths\n# - Export PATH additions to make commands globally available\n# - Source scripts like conda.sh to initialize environments\n# - Restart shell or run source ~/.bashrc after making changes\n</code></pre> <p>Now, restart your Git Bash terminal integrated in VS Code or run <code>source ~/.bashrc</code> and validate the Poetry command is available:</p> Bash<pre><code>$ poetry --version\nPoetry (version 1.6.1)\n(base) \n</code></pre> <p>Notice that the Conda <code>base</code> environment must be activated to validate that <code>conda</code>, <code>make</code> and, <code>poetry</code> commands are available on Git Bash:</p> Bash<pre><code>[Username]@IAG-MAGUI1-KQ10 MINGW64 ~/OneDrive - New York Life/Documents/Projects/service-sage-rag\n$ conda activate\n(base) \n[Username]@IAG-MAGUI1-KQ10 MINGW64 ~/OneDrive - New York Life/Documents/Projects/service-sage-rag\n$ conda --version\nconda 23.9.0\n(base) \n[Username]@IAG-MAGUI1-KQ10 MINGW64 ~/OneDrive - New York Life/Documents/Projects/service-sage-rag\n$ make --version\nGNU Make 4.4.1\nBuilt for Windows32\nCopyright (C) 1988-2023 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;\nThis is free software: you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n(base) \n[Username]@IAG-MAGUI1-KQ10 MINGW64 ~/OneDrive - New York Life/Documents/Projects/service-sage-rag\n$ poetry --version\nPoetry (version 1.6.1)\n(base) \n</code></pre>"}]}